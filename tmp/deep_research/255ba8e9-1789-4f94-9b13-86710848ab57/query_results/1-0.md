Query: early research combining reinforcement learning and language models
Extracted page content:
 Title: The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement
Learning and Large Language Models URL Source:
https://arxiv.org/html/2402.01874v1 Markdown Content: Moschoula Pternea
Microsoft mpternea@microsoft.com \\\AndPrerna Singh Microsoft
prernasingh@microsoft.com \\\AndAbir Chakraborty Microsoft
abir.chakraborty@microsoft.com \\\AndYagna Oruganti Microsoft
yaorugan@microsoft.com \\\AndMirco Milletari Microsoft mimillet@microsoft.com
\\\AndSayli Bapat Microsoft saylibapat@microsoft.com \\\AndKebei Jiang
Microsoft kebei.jiang@microsoft.com ###### Abstract In this work, we review
research studies that combine Reinforcement Learning (RL) and Large Language
Models (LLMs), two areas that owe their momentum to the development of deep
neural networks. We propose a novel taxonomy of three main classes based on
the way that the two model types interact with each other. The first class,
RL4LLM, includes studies where RL is leveraged to improve the performance of
LLMs on tasks related to Natural Language Processing. RL4LLM is divided into
two sub-categories depending on whether RL is used to directly fine-tune an
existing LLM or to improve the prompt of the LLM. In the second class, LLM4RL,
an LLM assists the training of an RL model that performs a task that is not
inherently related to natural language. We further break down LLM4RL based on
the component of the RL training framework that the LLM assists or replaces,
namely reward shaping, goal generation, and policy function. Finally, in the
third class, RL+LLM, an LLM and an RL agent are embedded in a common planning
framework without either of them contributing to training or fine-tuning of
the other. We further branch this class to distinguish between studies with
and without natural language feedback. We use this taxonomy to explore the
motivations behind the synergy of LLMs and RL and explain the reasons for its
success, while pinpointing potential shortcomings and areas where further
research is needed, as well as alternative methodologies that serve the same
goal. \\\nameMoschoula Pternea \\\emailmpternea@microsoft.com \\\namePrerna
Singh \\\emailprernasingh@microsoft.com \\\nameAbir Chakraborty
\\\emailabir.chakraborty@microsoft.com \\\nameYagna Oruganti
\\\emailyaorugan@microsoft.com \\\nameMirco Milletari
\\\emailmimillet@microsoft.com \\\nameSayli Bapat
\\\emailsaylibapat@microsoft.com \\\nameKebei Jiang
\\\emailkebei.jiang@microsoft.com \\\addrOne Microsoft Way, Redmond, WA 98052
1 Introduction -------------- Reinforcement Learning (RL) and Large Language
Models (LLMs) are experiencing tremendous progress over recent years, with the
common factor behind the growth of both Artificial Intelligence domains being
the development of Deep Neural Networks (DNNs). The foundations of Markov
Decision Processes (MDPs), which are at the core of every RL model, can
practically be traced back to the mid-20th century
\\[[12](https://arxiv.org/html/2402.01874v1#bib.bib12)\\], when they
originated in the field of stochastic control
\\[[132](https://arxiv.org/html/2402.01874v1#bib.bib132)\\] with the goal to
model sequential decision making in uncertain environments. Reinforcement
Learning proposed a formal framework for approaching sequential decision
making problems by adapting concepts from behavioral psychology, where an
agent can learn by interacting with their environment and utilizing their past
experience \\[[118](https://arxiv.org/html/2402.01874v1#bib.bib118),
[44](https://arxiv.org/html/2402.01874v1#bib.bib44)\\]. However, it was the
development of Deep Reinforcement Learning
\\[[44](https://arxiv.org/html/2402.01874v1#bib.bib44)\\] that addressed the
key challenges of traditional value and policy function approximations by
tackling the curse of dimensionality through efficient state representation,
better generalization, and sample efficiency. As a result, Deep RL algorithms
have become increasingly popular over recent years, with applications in
control systems, robotics, autonomous vehicles, healthcare, finance, to name
only a few. Similarly, Natural Language Processing (NLP) problems, like speech
recognition, natural language understanding, machine translation, text
summarization, etc., have long been successfully solved by machine learning
algorithms, ranging from Na√Øve Bayes and Maximum Entropy Models to Decision
Trees and Random Forests.
\\[[18](https://arxiv.org/html/2402.01874v1#bib.bib18)\\] attributed the
impressive development of NLP methods to three overlapping curves ‚Äì
Syntactics, Semantics, and Pragmatics ‚Äì and foresaw the eventual evolution of
NLP research to natural language understanding. Deep learning revolutionized
NLP tasks with various Neural Network architectures, such as Recurrent Neural
Networks (RNNs), Long Short-Term Memory (LSTM), Convolutional Neural Networks
(CNN) and, more recently, Transformers
\\[[123](https://arxiv.org/html/2402.01874v1#bib.bib123)\\]. Eventually, Deep
Neural Networks are opening new avenues in the field of Natural Language
Processing with the development of LLMs, which are language models trained on
massive amounts of text using specialized hardware, like GPU and TPUs, to
perform complicated NLP tasks. Apart from owing their growth to the
development of Deep Neural networks, LLMs and RL are intertwined from a
theoretical and practical perspective because they can both be formulated and
approached as sequential modeling problems: LLMs generate text in a sequential
decision-making framework, selecting the most likely next word or phrase. As
noted by \\[[105](https://arxiv.org/html/2402.01874v1#bib.bib105)\\], ‚Äúif we
view text generation as a sequential decision-making problem, reinforcement
learning (RL) appears to be a natural conceptual framework‚Äù. On the other
side, RL deals inherently with control problems, where an agent must select
the most appropriate action, interact with its environment, observe the result
of its action, and continue in a loop of state-observation-action-reward for a
possibly infinite horizon. Motivated by the impressive prominence of
Reinforcement Learning and Large Language Models, along with the impressive
range of practical applications they both present, we perform a comprehensive
literature review of studies that embed Large Language Models and
Reinforcement Learning Agents in a common computational framework. More
specifically, we are proposing a novel taxonomy to classify those studies
based on the way that the LLM and the RL agent interact in the framework. With
this taxonomy as the backbone of our review, we break down the state-of-art
frameworks into their fundamental components and present details to describe
the ways in which the two model types collaborate in each study. In parallel,
we explain the key motivational factors and the reasons behind the success of
this collaboration. We also review the potential limitations of this synergy
and present alternative state-of-art methods that, while not parts of this
taxonomy, have been developed with the intent to address the same issues as
the studies that we are focusing on. This thorough categorization will help
researchers obtain a better understanding of the dynamics of this synergy,
explain trends and opportunities in the intersection of RL and LLMs, and serve
as a starting point for the development of novel AI frameworks that combine
the best of both these worlds. The rest of this paper is structured as
follows: in section [2](https://arxiv.org/html/2402.01874v1#S2 "2 Background,
State-of-Art, Scope, and Contributions ‚Ä£ The RL/LLM Taxonomy Tree: Reviewing
Synergies Between Reinforcement Learning and Large Language Models"), we
provide the fundamental terms and concepts around Reinforcement Learning,
transformers, and LLMs, to facilitate the reader in understanding the material
that follows, and outline the scope and contributions of this study. In
section [3](https://arxiv.org/html/2402.01874v1#S3 "3 The RL/LLM Taxonomy Tree
‚Ä£ The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning
and Large Language Models"), we provide an overview of the proposed taxonomy.
Sections [4](https://arxiv.org/html/2402.01874v1#S4 "4 RL4LLM: Using
Reinforcement Learning to Enhance Large Language Models ‚Ä£ The RL/LLM Taxonomy
Tree: Reviewing Synergies Between Reinforcement Learning and Large Language
Models"), [5](https://arxiv.org/html/2402.01874v1#S5 "5 LLM4RL: Enhancing
Reinforcement Learning Agents through Large Language Models ‚Ä£ The RL/LLM
Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning and Large
Language Models") and [6](https://arxiv.org/html/2402.01874v1#S6 "6 RL+LLM:
Combining Independently Trained Models for Planning ‚Ä£ The RL/LLM Taxonomy
Tree: Reviewing Synergies Between Reinforcement Learning and Large Language
Models") are dedicated to the main classes of our proposed taxonomy,
corresponding to RL4LLM, LLM4RL, and RL+LLM, respectively. In section
[7](https://arxiv.org/html/2402.01874v1#S7 "7 Discussion ‚Ä£ The RL/LLM Taxonomy
Tree: Reviewing Synergies Between Reinforcement Learning and Large Language
Models"), we discuss the emerging patterns of this taxonomy and the reasons
behind the success of this synergy, as well as shortcomings and alternative
methods to achieve the same goals. Finally, in section
[8](https://arxiv.org/html/2402.01874v1#S8 "8 Conclusions and Future Work ‚Ä£
The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning
and Large Language Models") we summarize our findings and conclusions and
propose paths for future research. 2 Background, State-of-Art, Scope, and
Contributions ---------------------------------------------------- ### 2.1
Overview of RL and LLMs Reinforcement learning encompasses a range of
algorithms created to tackle problems that require a series of decisions and
it differs significantly from both supervised and unsupervised learning
methods: it requires the learning system, also referred to as an agent, to
independently determine the best sequence of actions to achieve its goal
through interaction with its environment. Reinforcement methods are primarily
divided into three categories: dynamic programming, Monte Carlo methods, and
temporal difference methods. All these methods present the decision-making
issue as a Markov decision process (MDP), a mathematical approach to solving
sequential decision-making problems that involves a state set SùëÜSitalic\\_S,
an action set Aùê¥Aitalic\\_A, a transition function TùëáTitalic\\_T, and a reward
function RùëÖRitalic\\_R. The goal of an MDP (S,A,T,R)ùëÜùê¥ùëáùëÖ(S,A,T,R)( italic\\_S
, italic\\_A , italic\\_T , italic\\_R ) is to determine an optimal policy
function œÄùúã\\\piitalic\\_œÄ, which outlines the agent‚Äôs behavior at any given
time. Essentially, a policy maps the set of states SùëÜSitalic\\_S perceived
from the environment to a set of actions Aùê¥Aitalic\\_A that should be
performed in those states. The objective of the agent is to maximize a
cumulative reward r‚ààRùëüùëÖr\\\in Ritalic\\_r ‚àà italic\\_R by selecting the
actions to perform in each state sùë†sitalic\\_s. When in state sùë†sitalic\\_s,
the agent performs action aùëéaitalic\\_a, receives reward rùëüritalic\\_r from
its environment, and then moves to the next state
s‚Ä≤superscriptùë†‚Ä≤s^{\\\prime}italic\\_s start\\_POSTSUPERSCRIPT ‚Ä≤
end\\_POSTSUPERSCRIPT. Each step can therefore be represented as a transition
tuple (s,a,r,s‚Ä≤)ùë†ùëéùëüsuperscriptùë†‚Ä≤(s,a,r,s^{\\\prime})( italic\\_s , italic\\_a
, italic\\_r , italic\\_s start\\_POSTSUPERSCRIPT ‚Ä≤ end\\_POSTSUPERSCRIPT ).
The process to estimate the policy œÄùúã\\\piitalic\\_œÄ depends on the algorithm
in use and the specifics of the problem. In certain instances, especially when
the state and action space are tractable, the policy can be stored as a lookup
table, while in others, a function approximator (such a neural network) is
used. Within the realm of Natural Language Processing (NLP), Large Language
Models (LLMs) have become a ubiquitous component. The primary role of a
language model is to establish a probability distribution across word
sequences, a process achieved by the application of the chain rule of
probability to dissect the joint probability of a word sequence into
conditional probabilities. Language models may be unidirectional, forecasting
future words based on past ones as seen in n-gram models, or bidirectional,
making predictions for a word based on both antecedent and subsequent words as
exemplified by Transformer models. Owing to advancements in deep learning,
neural language models have seen a surge in popularity. An LLM makes use of a
specific kind of neural network known as a transformer
\\[[123](https://arxiv.org/html/2402.01874v1#bib.bib123)\\], which uses a
mechanism called attention to weigh the influence of different words when
producing an output. The term ‚Äúlarge‚Äù in this context signifies the
substantial number of parameters these models hold. LLMs are capable of
responding to queries, authoring essays, summarizing texts, translating
languages, and even generating poetry. Some of the most popular LLMs include
BERT \\[[37](https://arxiv.org/html/2402.01874v1#bib.bib37)\\], GPT
\\[[16](https://arxiv.org/html/2402.01874v1#bib.bib16)\\], PaLM
\\[[32](https://arxiv.org/html/2402.01874v1#bib.bib32)\\], and LaMDA
\\[[119](https://arxiv.org/html/2402.01874v1#bib.bib119)\\]. ### 2.2 State-of-
Art Review Studies As rapidly advancing fields, with a wide range of
applications, both Reinforcement Learning and Natural Language Processing have
been the focus of numerous studies that aim to synthesize and evaluate state-
of-art research in each area. Since it first emerged, RL has been of
particular interest to researchers in computer science, robotics, and control
and, as a result, numerous surveys on RL have been published, ranging from
general overview of RL \\[[63](https://arxiv.org/html/2402.01874v1#bib.bib63),
[5](https://arxiv.org/html/2402.01874v1#bib.bib5)\\] to comprehensive reviews
that focus on a particular technique (Offline RL,
\\[[98](https://arxiv.org/html/2402.01874v1#bib.bib98)\\]; Meta-Reinforcement
Learning, \\[[11](https://arxiv.org/html/2402.01874v1#bib.bib11)\\]; RL on
graphs, \\[[84](https://arxiv.org/html/2402.01874v1#bib.bib84)\\];
Evolutionary RL, \\[[7](https://arxiv.org/html/2402.01874v1#bib.bib7)\\];
Hierarchical RL, \\[[94](https://arxiv.org/html/2402.01874v1#bib.bib94)\\];
Multi-Agent Deep RL, \\[[51](https://arxiv.org/html/2402.01874v1#bib.bib51),
[40](https://arxiv.org/html/2402.01874v1#bib.bib40)\\], application
(healthcare, \\[[140](https://arxiv.org/html/2402.01874v1#bib.bib140)\\];
robotics, \\[[48](https://arxiv.org/html/2402.01874v1#bib.bib48)\\];
combinatorial optimization,
\\[[76](https://arxiv.org/html/2402.01874v1#bib.bib76)\\]; generative AI,
\\[[20](https://arxiv.org/html/2402.01874v1#bib.bib20)\\], learning
assumptions (dynamically varying environment,
\\[[91](https://arxiv.org/html/2402.01874v1#bib.bib91)\\]. Owing to the rapid
emergence of LLMs, we are also beginning to witness review papers dedicated to
this topic, like the comprehensive review on RLHF by
\\[[116](https://arxiv.org/html/2402.01874v1#bib.bib116)\\]. A similar trend
can be observed in Natural Language Processing, with numerous examples of
survey studies providing an overall study of the concepts and methods in the
field \\[[120](https://arxiv.org/html/2402.01874v1#bib.bib120)\\],
particularly since the introduction of deep learning for NLP
\\[[89](https://arxiv.org/html/2402.01874v1#bib.bib89),
[120](https://arxiv.org/html/2402.01874v1#bib.bib120),
[31](https://arxiv.org/html/2402.01874v1#bib.bib31)\\]. Similarly to the case
of RL, literature review studies specialize by application (e.g., healthcare,
\\[[133](https://arxiv.org/html/2402.01874v1#bib.bib133)\\]; fake news
detection, \\[[88](https://arxiv.org/html/2402.01874v1#bib.bib88)\\];
bioinformatics, \\[[143](https://arxiv.org/html/2402.01874v1#bib.bib143)\\] or
focus on particular methods (pretrained models,
\\[[100](https://arxiv.org/html/2402.01874v1#bib.bib100)\\], graphs,
\\[[82](https://arxiv.org/html/2402.01874v1#bib.bib82)\\], etc. Not
surprisingly, LLMs themselves, which lie at the intersection of Natural
Language Processing and Reinforcement Learning, have attracted the attention
of researchers worldwide, resulting already in an impressive wealth of
comprehensive literature review publications, ranging from general reviews
\\[[148](https://arxiv.org/html/2402.01874v1#bib.bib148),
[137](https://arxiv.org/html/2402.01874v1#bib.bib137),
[79](https://arxiv.org/html/2402.01874v1#bib.bib79),
[139](https://arxiv.org/html/2402.01874v1#bib.bib139),
[74](https://arxiv.org/html/2402.01874v1#bib.bib74)\\] to surveys focusing on
different aspects of LLMs, like evaluation
\\[[53](https://arxiv.org/html/2402.01874v1#bib.bib53),
[23](https://arxiv.org/html/2402.01874v1#bib.bib23)\\], alignment with humans
\\[[111](https://arxiv.org/html/2402.01874v1#bib.bib111),
[128](https://arxiv.org/html/2402.01874v1#bib.bib128)\\], explainability
\\[[147](https://arxiv.org/html/2402.01874v1#bib.bib147)\\], Responsible AI
considerations \\[[47](https://arxiv.org/html/2402.01874v1#bib.bib47)\\], a
knowledge acquisition and updating
\\[[19](https://arxiv.org/html/2402.01874v1#bib.bib19),
[126](https://arxiv.org/html/2402.01874v1#bib.bib126),
[92](https://arxiv.org/html/2402.01874v1#bib.bib92)\\] as well as using LLMs
for specific applications like information retrieval
\\[[151](https://arxiv.org/html/2402.01874v1#bib.bib151)\\], natural language
understanding \\[[39](https://arxiv.org/html/2402.01874v1#bib.bib39)\\],
instruction tuning
\\[[144](https://arxiv.org/html/2402.01874v1#bib.bib144)\\], software
engineering \\[[124](https://arxiv.org/html/2402.01874v1#bib.bib124),
[43](https://arxiv.org/html/2402.01874v1#bib.bib43)\\], recommendation systems
\\[[134](https://arxiv.org/html/2402.01874v1#bib.bib134),
[70](https://arxiv.org/html/2402.01874v1#bib.bib70),
[72](https://arxiv.org/html/2402.01874v1#bib.bib72)\\], opinion prediction
\\[[65](https://arxiv.org/html/2402.01874v1#bib.bib65)\\], and other
applications. As will be explained in more detail in subsection
[2.3](https://arxiv.org/html/2402.01874v1#S2.SS3 "2.3 Scope of This Study ‚Ä£ 2
Background, State-of-Art, Scope, and Contributions ‚Ä£ The RL/LLM Taxonomy Tree:
Reviewing Synergies Between Reinforcement Learning and Large Language
Models"), this survey examines Reinforcement Learning and Large Language
Models from a completely different angle compared to the aforementioned review
papers, since it focuses exclusively on studies where RL and LLMs are both
indispensable components of the same computational framework. ### 2.3 Scope of
This Study As explained in section [1](https://arxiv.org/html/2402.01874v1#S1
"1 Introduction ‚Ä£ The RL/LLM Taxonomy Tree: Reviewing Synergies Between
Reinforcement Learning and Large Language Models"), we are presenting a survey
on studies that combine Reinforcement Learning and Large Language Models in a
common modeling framework and we are proposing a mew taxonomy to classify
them. The taxonomy is visualized as the RL/LLM Taxonomy Tree (Fig.
[1](https://arxiv.org/html/2402.01874v1#S3.F1 "Figure 1 ‚Ä£ 3 The RL/LLM
Taxonomy Tree ‚Ä£ The RL/LLM Taxonomy Tree: Reviewing Synergies Between
Reinforcement Learning and Large Language Models")), which maps each study to
a tree node, according to the details of the synergy between the two models.
Although Reinforcement Learning ‚Äì in its RLHF form ‚Äì is an essential component
of any Large Language Model, our review is only concerned with studies that
involve already trained LLMs, which are then either improved and fine-tuned
with RL - beyond RLHF, that was used to train the original model - or combined
with some RL agent to perform a downstream task. Studies where Reinforcement
Learning is limited to training the original LLM are beyond the scope of our
taxonomy. In addition, literature exhibits state-of-art survey papers that
focus on the use of LLMs for tasks that are not related to natural language,
including the augmentation of LLMs with reasoning and other skills
\\[[58](https://arxiv.org/html/2402.01874v1#bib.bib58),
[78](https://arxiv.org/html/2402.01874v1#bib.bib78),
[130](https://arxiv.org/html/2402.01874v1#bib.bib130)\\], multimodal LLMs
\\[[127](https://arxiv.org/html/2402.01874v1#bib.bib127)\\], and autonomous
agents \\[[125](https://arxiv.org/html/2402.01874v1#bib.bib125),
[73](https://arxiv.org/html/2402.01874v1#bib.bib73)\\]. While in this survey
we are also reviewing studies where LLMs are used to perform general tasks
(sections [4](https://arxiv.org/html/2402.01874v1#S4 "4 RL4LLM: Using
Reinforcement Learning to Enhance Large Language Models ‚Ä£ The RL/LLM Taxonomy
Tree: Reviewing Synergies Between Reinforcement Learning and Large Language
Models") and [6](https://arxiv.org/html/2402.01874v1#S6 "6 RL+LLM: Combining
Independently Trained Models for Planning ‚Ä£ The RL/LLM Taxonomy Tree:
Reviewing Synergies Between Reinforcement Learning and Large Language
Models"), we are exclusively focusing on those where the RL agent, rather that
an LLM, is performing a downstream task, and the LLM assists the framework
either at training (LLM4RL class, section
[5](https://arxiv.org/html/2402.01874v1#S5 "5 LLM4RL: Enhancing Reinforcement
Learning Agents through Large Language Models ‚Ä£ The RL/LLM Taxonomy Tree:
Reviewing Synergies Between Reinforcement Learning and Large Language Models")
or at inference (RL+LLM class, section
[6](https://arxiv.org/html/2402.01874v1#S6 "6 RL+LLM: Combining Independently
Trained Models for Planning ‚Ä£ The RL/LLM Taxonomy Tree: Reviewing Synergies
Between Reinforcement Learning and Large Language Models")). Therefore,
contrary to \\[[73](https://arxiv.org/html/2402.01874v1#bib.bib73)\\] and
\\[[125](https://arxiv.org/html/2402.01874v1#bib.bib125)\\], we are not
concerned with evaluating the performance of the LLMs as autonomous agents.
For the sake of completeness, we still discuss the use of LLMs to tasks that
are not related to Natural Language, along with Multimodel LLMs in section
[7](https://arxiv.org/html/2402.01874v1#S7 "7 Discussion ‚Ä£ The RL/LLM Taxonomy
Tree: Reviewing Synergies Between Reinforcement Learning and Large Language
Models"). Finally, the use of pretrained language models to aid RL agents
through reward design
\\[[21](https://arxiv.org/html/2402.01874v1#bib.bib21)\\], policy priors
\\[[30](https://arxiv.org/html/2402.01874v1#bib.bib30)\\] or policy transfer
\\[[62](https://arxiv.org/html/2402.01874v1#bib.bib62)\\] preceded the
development of LLMs. While we refer to those studies for the sake of
completeness, our taxonomy and subsequent analysis only captures those studies
where the language model used is an LLM. ### 2.4 Contributions of This Study
To the best of our knowledge, this is the first study that attempts a thorough
review of the state-of-art research on the intersection of Large Language
Models and Reinforcement Learning. To this direction, we have identified 24
publications that combine LLMs and RL and fall within the scope of this review
as described in subsection [2.3](https://arxiv.org/html/2402.01874v1#S2.SS3
"2.3 Scope of This Study ‚Ä£ 2 Background, State-of-Art, Scope, and
Contributions ‚Ä£ The RL/LLM Taxonomy Tree: Reviewing Synergies Between
Reinforcement Learning and Large Language Models"). Our goal is to examine how
the two distinct models are embedded in a common framework to achieve a
specific task. A set of common patterns emerged from the review of those
studies, which helped us categorize the studies according to the way that the
two models collaborate, as well as the nature of the end task to be achieved.
Therefore, we are proposing a novel, systematic taxonomy of those studies that
helps researchers understand the scope and applications of the various
synergies between RL and LLMs. First, we follow the proposed taxonomy to
individually present the key features, goals, and highlights of each study
that we have reviewed. Then, we shift our focus to obtaining a global
perspective on the collective goals of each study category and explain their
strengths and potential shortcomings. In summary, the contribution of our work
is threefold: 1. 1.We collect, review, and analyze state-of-art studies which
combine Reinforcement Learning and Large Language Models in the same
framework. 2. 2.We propose a novel taxonomy to explain the synergy between RL
and LLMs. In particular, we visualize the classification of the RL/LLM studies
using the RL/LLM Taxonomy Tree, which includes three main classes which
collectively capture any problem that utilizes both RL and LLMs. The criterion
for generating the three classes is whether RL is utilized to improve the
performance of an LLM (class 1 ‚Äì RL4LLM), or an LLM is used to train an RL
agent to perform a non-NLP task (class 2 ‚Äì LLM4RL), or whether the two models
are trained independently and then embedded in a common framework to achieve a
planning task (class 3 ‚Äì RL+LLM). The order in which we present the individual
studies (contribution 1) is based on this taxonomy. 3. 3.We utilize our
findings from the taxonomy to discuss the applications of this synergy,
explain the reasons for its success, identify strengths and potential
weaknesses, and investigative alternative ways towards achieving the same
tasks. 3 The RL/LLM Taxonomy Tree -------------------------- Even though LLMs
are an emerging field, there exists a substantial body of literature dedicated
to their intersection with Reinforcement Learning. We can readily discern a
top-level classification based on the interplay between the two models - RL
agent and LLM ‚Äì as the key classification criterion. We have therefore
identified the following classes of studies, which constitute the core classes
of our taxonomy: ![Image 1: Refer to
caption](https://arxiv.org/html/2402.01874v1/extracted/5386064/Supp-
Figures/tree.png) Figure 1: The RL/LLM Taxonomy Tree. 1. 1.RL4LLM. These
studies use RL to improve the performance of the LLM in an NLP task. 2.
2.LLM4RL. These studies use an LLM to supplement the training of an RL model
that performs a general task that is not inherently related to natural
language. 3. 3.RL+LLM. These studies combine RL models with LLM models to plan
over a set of skills, without using either model to train or fine-tune the
other. The frameworks belonging to RL4LLM class start from a trained LLM and
subsequently utilize RL to modify it, with the goal to improve its performance
on specific tasks or align it to user intent and ethical AI standards. On the
contrary, studies in LLM4RL category utilize the LLM as a component of an RL
training framework with the goal of helping an RL agent perform a specific
task. Finally, RL+LLM involves the two models as independent components of a
common framework, without either of them directly participating in the
training or tuning of the other in any way. Interestingly, the way that the RL
agent and the LLM types interact in each case is directly tied to the goal of
the synergy, which helps us identify a mapping between the structure of each
framework and its end goal. More specifically, RL4LLM studies aim to improve
the performance of the LLM in a downstream task that is related to natural
language processing, such as text summarization, question-answering, or
conversation. In turn, the goal of LLM4RL frameworks is to improve the
training efficiency or performance of a control task that would still rely on
RL in the absence of the LLM and is therefore generally not related to NLP.
Finally, studies of RL+LLM generally use the LLM to plan over individual
skills that have been learned through RL. Studies within each top-level class
of the RL/LLM Taxonomy Tree exhibit significant variety in the way that the RL
agent and the LLM interact in each framework. thus requiring further
refinement of the taxonomy. Specifically, studies within RL4LLM can be broken
down into the following subcategories: * ‚Ä¢RL4LLM-Fine-tuning: Encompasses
studies where RL is used to perform model fine-tuning, which involves tweaking
the model parameters, until the model achieves the desired performance. This
subclass can be further refined according to the presence or absence of human
feedback. * ‚Ä¢RL4LLM-Prompt Engineering: Includes studies where RL is used to
iteratively update the prompt of the LLM, until the model achieves the desired
performance. Similarly, LLM4RL can be further divided according to the
component of the RL framework that is assisted, replaced, or represented by
the LLM, namely: * ‚Ä¢LLM4RL-Reward: Includes studies where the LLM is used to
design the reward function of the RL agent. * ‚Ä¢LLM4RL-Goal: includes studies
where the LLM is utilized for goal setting, which applies to goal-conditioned
RL settings. * ‚Ä¢LLM4RL-Policy: includes studies where the LLM represents the
policy function to be learned, or directly assists its training or
pretraining. Finally, RL+LLM class is branched into two subclasses: * ‚Ä¢RL+LLM-
No Language Feedback: studies where the prompt of the LLM is updated during
the planning process. * ‚Ä¢RL+LLM-With Language Feedback: studies where the
prompt of the LLM stays fixed throughout the planning process. The RL/LLM
Taxonomy Tree is visualized in Fig.
[1](https://arxiv.org/html/2402.01874v1#S3.F1 "Figure 1 ‚Ä£ 3 The RL/LLM
Taxonomy Tree ‚Ä£ The RL/LLM Taxonomy Tree: Reviewing Synergies Between
Reinforcement Learning and Large Language Models"), while Table
[1](https://arxiv.org/html/2402.01874v1#S3.T1 "Table 1 ‚Ä£ 3 The RL/LLM Taxonomy
Tree ‚Ä£ The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement
Learning and Large Language Models") maps the research studies to the
particular subclasses they belong to, corresponding to the leaf nodes of the
RL/LLM Taxonomy Tree. To the best of our knowledge, the classification
proposed by the RL/LLM Taxonomy Tree is exhaustive and captures all state-of-
art studies that fall within the scope of our taxonomy
([2.3](https://arxiv.org/html/2402.01874v1#S2.SS3 "2.3 Scope of This Study ‚Ä£ 2
Background, State-of-Art, Scope, and Contributions ‚Ä£ The RL/LLM Taxonomy Tree:
Reviewing Synergies Between Reinforcement Learning and Large Language
Models")) as of today. We have so far identified 24 studies that fall under
the scope of this review and can direcly be mapped to RL/LLM Taxonomy Tree
leaves. Therefore, it has the potential to serve as a reference and mapping
tool for researchers and practitioners of Artificial Intelligence. In
addition, as researchers continue developing novel ways to combine
Reinforcement Learning with Large Language Models, the tree can be potentially
expanded with new nodes. By bridging the gap between RL and LLMs, this
taxonomy can be a valuable resource for researchers who are experienced in one
of the two domains and are looking to venture into the other, and vice versa,
as well as for anyone who wishes to explore whether a framework combining RL
and LLMs is a promising solution for the problem they are seeking to address.
More importantly, the taxonomy guides researchers as they are shaping the
requirements of their application, whether those are related to model
performance, training efficiency, or responsible AI considerations. Table 1:
Mapping Studies to RL/LLM Taxonomy Tree Leaves 4 RL4LLM: Using Reinforcement
Learning to Enhance Large Language Models
----------------------------------------------------------------------- As
explained in section [2](https://arxiv.org/html/2402.01874v1#S2 "2 Background,
State-of-Art, Scope, and Contributions ‚Ä£ The RL/LLM Taxonomy Tree: Reviewing
Synergies Between Reinforcement Learning and Large Language Models"),
Reinforcement Learning with Human Feedback is an integral part for the
training of Large Language Models. Nevertheless, there is a wealth of studies
where the synergy between RL and LLM extends beyond training of the LLM.
Therefore, RL4LLM class includes a significant body of work where
Reinforcement Learning is used to further refine an already trained LLM and
improve its performance on NLP-related tasks. This improvement in performance
is measured according to the goal of each research, with the most common goals
being the following: * ‚Ä¢Improved performance in downstream NLP tasks
\\[[36](https://arxiv.org/html/2402.01874v1#bib.bib36),
[105](https://arxiv.org/html/2402.01874v1#bib.bib105),
[49](https://arxiv.org/html/2402.01874v1#bib.bib49),
[145](https://arxiv.org/html/2402.01874v1#bib.bib145),
[115](https://arxiv.org/html/2402.01874v1#bib.bib115)\\]. * ‚Ä¢Alignment with
intent, values, and goals of user
\\[[105](https://arxiv.org/html/2402.01874v1#bib.bib105),
[90](https://arxiv.org/html/2402.01874v1#bib.bib90)\\]. * ‚Ä¢Alignment with
Responsible AI considerations
\\[[96](https://arxiv.org/html/2402.01874v1#bib.bib96),
[8](https://arxiv.org/html/2402.01874v1#bib.bib8),
[9](https://arxiv.org/html/2402.01874v1#bib.bib9)\\]. * ‚Ä¢Reduction of data and
resource requirements
\\[[145](https://arxiv.org/html/2402.01874v1#bib.bib145),
[115](https://arxiv.org/html/2402.01874v1#bib.bib115)\\]. RL4LLM studies can
be further divided into two major sub-categories: a) Studies that use
knowledge from LLMs to build an RL model to fine-tune an LLM, or part of it,
to perform a downstream NLP task and b) studies using RL to design prompts to
query LLMs. A summary of the natural langauge application of each framework is
shown in Table [3](https://arxiv.org/html/2402.01874v1#S4.T3 "Table 3 ‚Ä£ 4.2
RL4LLM-Prompt ‚Ä£ 4 RL4LLM: Using Reinforcement Learning to Enhance Large
Language Models ‚Ä£ The RL/LLM Taxonomy Tree: Reviewing Synergies Between
Reinforcement Learning and Large Language Models"). ### 4.1 RL4LLM-Fine-Tuning
This class includes studies where RL is used for directly fine-tuning an
existing LLM to make it more aligned with specific goals by updating an
enormous set of LLM parameters. The presence of human feedback for fine-tuning
serves as the criterion for further branching the RL4LLM\\-Fine-tuning node of
our taxonomy tree, resulting in two new subclasses: RL4LLM - Fine-tuning with
human feedback ([4.1.1](https://arxiv.org/html/2402.01874v1#S4.SS1.SSS1 "4.1.1
With human feedback ‚Ä£ 4.1 RL4LLM-Fine-Tuning ‚Ä£ 4 RL4LLM: Using Reinforcement
Learning to Enhance Large Language Models ‚Ä£ The RL/LLM Taxonomy Tree:
Reviewing Synergies Between Reinforcement Learning and Large Language Models")
and RL4LLM - Fine-tuning without human feedback
([4.1.2](https://arxiv.org/html/2402.01874v1#S4.SS1.SSS2 "4.1.2 Without human
feedback ‚Ä£ 4.1 RL4LLM-Fine-Tuning ‚Ä£ 4 RL4LLM: Using Reinforcement Learning to
Enhance Large Language Models ‚Ä£ The RL/LLM Taxonomy Tree: Reviewing Synergies
Between Reinforcement Learning and Large Language Models"). #### 4.1.1 With
human feedback Human input can be critical when assessing the quality of the
LLM output in terms of harmlessness. Preventing the generation of toxic and
harmful content has been the focus of multiple studies even before Large
Language Models. For example,
\\[[114](https://arxiv.org/html/2402.01874v1#bib.bib114)\\] trained an RL
agent to predict which summary of a given Reddit post is most likely to be
preferred by a human. The authors used a supervised learning model as a reward
function that selects a summary among multiple candidate summaries. The result
of the summary selection is then used to fine-tune the RL agent using PPO.
Authors found that optimizing the reward model resulted in better human-
preferred summaries in comparison to using typical NLP evaluation metrics like
ROUGE. In a similar manner, the idea of fine-tuning language models through RL
agents extended naturally to the realm of LLMs. Human feedback can generally
be embedded in the fine-tuning framework through the construction of the
training dataset for the policy and reward models: For training the policy
model, humans demonstrate the target behavior of the LLM, while for the reward
model, they rank the alternative outputs of the LLM based on how well they
align to the intent of the framework. For a study to be classified as RL4LLM-
Fine-Tuning with human feedback, it should include human feedback in the
training dataset of at least the initial policy model or the reward model;
else, it belongs to RL4LLM-Fine-tuning without human feedback. Ouyang et al.
\\[[90](https://arxiv.org/html/2402.01874v1#bib.bib90)\\] developed Instruct-
GPT, an LLM capable of capturing and following the intent of the user without
producing untruthful, toxic, or generally unhelpful content. Instruct-GPT
consists of three steps. The first includes the training of the policy model
as a supervised learning model. The training dataset is generated by
collecting demonstrations of the desired model behavior. To generate each new
data point, a prompt is sampled from a baseline prompt dataset and a human
‚Äúlabeler‚Äù demonstrates the desired behavior of the model. The dataset is then
used to fine-tune GPT-3
\\[[16](https://arxiv.org/html/2402.01874v1#bib.bib16)\\] with supervised
learning. The second step is the training of the reward model. Like the policy
model, the reward model is also a supervised learning model, but it is trained
on comparison data. To generate the training dataset of the reward model, a
prompt and a set of model outputs are sampled for each data point, and a human
‚Äúlabeler‚Äù assigns rankings to the outputs. Finally, the third step is GPT-3
fine-tuning, with the reward model embedded in an RL training framework.
Experimental evaluation in public NLP datasets showed that Instruct-GPT
demonstrates improved performance regarding truthfulness and harmlessness
compared to its baseline model, with only minimal performance degradation,
while also showing generalization capabilities to instructions outside the
distribution present in the fine-tuning dataset. Bai et al.
\\[[8](https://arxiv.org/html/2402.01874v1#bib.bib8)\\] also utilized
preference modeling and RLHF to train helpful, honest, and harmless AI
assistants. Like \\[[90](https://arxiv.org/html/2402.01874v1#bib.bib90)\\],
they trained an initial policy by fine-tuning a pretrained LLM. First, a HHH
(Helpful, Honest, and Harmless) Context-Distilled Language Model was used to
build a base dataset. This dataset was used to train a preference model to
generate, in turn, a new dataset, using rejection sampling. Finally, the
initial policy and the preference model were combined in an RLHF framework for
fine-tuning the AI agent. Extensive data collection was performed by crowd
workers, who were interacting with the models in open-ended conversations. The
human feedback data, along with the preference models and the resulting RL
policies, were updated on a weekly basis in an online manner to improve the
quality of both the datasets and the models themselves. As a result, the
authors achieved the desired alignment between the language model and human
preferences in almost all NLP evaluations, with friendly and deployable AI
assistants, that also presented improved AI capabilities in NLP tasks, such as
text summarization, even extending to specialized tasks, like python code
generation. Hu et al.
\\[[57](https://arxiv.org/html/2402.01874v1#bib.bib57)\\] propose an offline
RLHF framework to align LLMs to human intent. Rather than the typical PPO
architecture applied in RLHF settings, they fine-tune the LLM on pre-generated
samples in an offline manner, within a framework consisting of four steps:
First, the pre-trained language model is fine-tuned on human-labeled
instruction data using a supervised learning method, resulting in a new model
called SFT. Second, they train a human preference model (RM) to predict
rewards, using binary loss or ranking loss functions. Third, they build a
combined dataset consisting of both human-labeled data (used in training the
SFT model in the first step) as well as model-generated data (generated by the
SFT model using prompts from the user, the SFT dataset, and the RM dataset).
Finally, the SFT model is fine-tuned on this combined dataset using offline
RL. The authors implemented three offline RLHF algorithms, namely Maximum
Likelihood Estimation (MLE) with Filtering
\\[[112](https://arxiv.org/html/2402.01874v1#bib.bib112)\\], Reward-Weighted
Regression \\[[97](https://arxiv.org/html/2402.01874v1#bib.bib97)\\], and
Decision Transformer \\[[24](https://arxiv.org/html/2402.01874v1#bib.bib24)\\]
[5.3.3](https://arxiv.org/html/2402.01874v1#S5.SS3.SSS3 "5.3.3 LLM Being the
Policy ‚Ä£ 5.3 LLM4RL-Policy ‚Ä£ 5 LLM4RL: Enhancing Reinforcement Learning Agents
through Large Language Models ‚Ä£ The RL/LLM Taxonomy Tree: Reviewing Synergies
Between Reinforcement Learning and Large Language Models"), with specific data
pre-processing methods and training loss function for every algorithm choice.
The performance of the models was evaluated both by humans and by
GPT-4\\[[87](https://arxiv.org/html/2402.01874v1#bib.bib87)\\], with the
Decision Transformer architecture outperforming both MLE with Filtering and
Reward-Weighted Regression in terms of evaluation score. The Decision
Transformer-based offline method was also shown to obtain comparable results
to PPO, while also achieving faster convergence. #### 4.1.2 Without human
feedback This class of methods is primarily focused on the development of
responsible AI systems. Interestingly, the presence of a human in the loop is
not required for ensuring helpfulness and harmlessness of robotic assistants.
As a result, this subclass includes studies where human feedback is either
completely omitted or provided by a capable AI system. In a variation of
\\[[8](https://arxiv.org/html/2402.01874v1#bib.bib8)\\], Bai et al.
\\[[9](https://arxiv.org/html/2402.01874v1#bib.bib9)\\] proposed
‚ÄúConstitutional AI‚Äù, a framework to train AI assistants capable of handling
objectionable queries without being evasive by using AI Feedback. The AI model
is trained through self-improvement, while human feedback is restricted in
providing a list of rules and principles. Constitutional AI consists of two
phases, namely a supervised learning phase and a reinforcement learning phase.
In the supervised learning phase, an initial helpful-only LLM assistant
generates responses to red teaming prompts that are designed to typically
elicit harmful and toxic responses. This phase is the AI analogue of a human
demonstrating the desired behavior in
\\[[90](https://arxiv.org/html/2402.01874v1#bib.bib90)\\] and
\\[[57](https://arxiv.org/html/2402.01874v1#bib.bib57)\\], or the use of the
distilled model \\[[8](https://arxiv.org/html/2402.01874v1#bib.bib8)\\]. The
model is asked to evaluate the response it provided based on a constitutional
principle and then revise its response based on this critique. Responses are
repeatedly revised at each step of the process by randomly drawing principles
from the constitution. For the RL stage, a preference model is trained to act
as the reward function using a training dataset generated by the trained model
from the supervised learning stage: To generate a data point, the assistant is
prompted again with a harmful prompt and is asked to select the best response
from pair of responses, based on the constitutional principles. This process
produces an AI-generated preference dataset from harmlessness, which is
combined with a human feedback-generated dataset for helpfulness. The
preference model is then trained based on this combined dataset and is used to
fine-tune the supervised model from the first stage in an RL framework that
follows the general principles of RLHF - with the difference that the feedback
is provided by the AI, hence the term ‚ÄúRLAIF‚Äù. The resulting LLM responds to
harmful queries by explaining its objections to them. This study is an example
where alignment to human goals can be achieved with minimal human supervision.
More recently, Ramamurthy et al.
\\[[105](https://arxiv.org/html/2402.01874v1#bib.bib105)\\] examined whether
RL is the best choice for aligning pre-trained LLMs to human preferences,
compared to supervised learning techniques, given the challenges of training
instability that RL algorithms might suffer from, as well as the lack of open-
source libraries and benchmarks that are suitable for LLM fine-tuning. To
address those issues, the authors released RL4LM, an open-source library built
on HuggingFace, which enables generative models to be trained with a variety
of on-policy RL methods, such as PPO, TRPO, and A2C. The library provides a
variety of reward functions and evaluation metrics. In addition, the authors
composed GRUE (General Reinforced-language Understanding Evaluation)
benchmark, a set of seven language generation tasks which are supervised by
reward functions that quantify metrics of human preference. Finally, they
introduce NLPO (Natural Language Policy Optimization), an on-policy RL
algorithm (also available in RL4LM) that dynamically learns task-specific
constraints over the distribution of language to effectively reduce the
combinatorial action space in language generation. The authors provided
detailed results on a case-by-case basis to determine when RL is preferred
over supervised learning as well as when NLPO is preferred to PPO. However,
NLPO demonstrated overall greater stability and performance than other policy
gradient methods, such as PPO, while RL techniques were shown to generally
outperform their supervised learning counterparts in terms of aligning LMs to
human preferences. Ghalandari et al.
\\[[49](https://arxiv.org/html/2402.01874v1#bib.bib49)\\] used Reinforcement
Learning to fine-tune an LLM for sentence compression while addressing the
issue of inefficiency at inference time. In the specific task of this study,
the goal is to summarize a sentence by extracting a subset of its tokens in
their original order. Given a tokenized input sentence x, the output is a
binary vector indicating whether the corresponding input token is included in
the compressed sentence or not. To evaluate the output of the policy, a reward
is calculated as the average of three metrics: fluency (for grammatically
correct and well-written sentences), similarity to source (to preserve the
meaning of the original sentence, measured using bi-encoder similarity, cross-
encoder similarity, and cross-encoder NLI), and output length or compression
ratio (imposing soft length control using Gaussian reward functions). The
policy was initialized using DistilRoBERTa
\\[[109](https://arxiv.org/html/2402.01874v1#bib.bib109)\\], a six-layer a
transformer encoder model with a linear classification head, and the RL agent
was trained through a Policy Gradient method. The model was shown to
outperform unsupervised models (with no labelled examples) while also enabling
fast inference with one-step sequence labeling at test time and allowing for
configurable rewards to adapt to specific use cases. ### 4.2 RL4LLM-Prompt
Constructing a suitable prompt is the first step towards ensuring that an LLM
will generate the desired output in terms of relevance, format, and ethical
considerations. Indeed, careful prompt engineering is often sufficient for
aligning the output of an LLM to human preferences without fine-tuning the
weights of the LLM itself, a computationally intensive process which usually
requires extensive data collection. Prompting concatenates the inputs with an
additional piece of text that directs the LLM to produce the desired outputs.
Most studies focus on tuning soft prompts (e.g., embeddings), which are
difficult to interpret and non-transferable across different LLMs
\\[[36](https://arxiv.org/html/2402.01874v1#bib.bib36)\\]. On the other hand,
discrete prompts, which consist of concrete tokens from vocabulary, are hard
to optimize efficiently, due to their discrete nature and the difficulty of
efficient space exploration. To address this limitation, this class of studies
utilize RL for discrete prompt optimization, with the goal to enhance the
performance of the LLM on diverse tasks, often requiring just a few training
instances. Two recent studies in this class are TEMPERA and RLPROMPT, both of
which use Roberta-large as the background LLM. Proposed by Zhang et al.
\\[[145](https://arxiv.org/html/2402.01874v1#bib.bib145)\\], TEMPERA (TEst-
tiMe Prompt Editing using Reinforcement leArning) is a framework for
automating the design of optimal prompts at test time. Prompt optimization is
formulated as an RL problem with the goal to incorporate human knowledge and
thus design prompts that are interpretable and can be adapted to different
queries. The RL agent performs different editing techniques at test time to
construct query-dependent prompts efficiently. The action space allows the RL
agent to edit instructions, in-context examples and verbalizers as needed,
while the score differences between successive prompts before and after
editing are used as reward. Unlike previous methods, TEMPERA makes use of
prior human knowledge and provides interpretability; also, compared to
approaches like prompt tweaking, AutoPrompt, and RLPrompt, it also
significantly improves performance on tasks like sentiment analysis, subject
classification, natural language inference, etc. On the other hand, RLPROMPT
by Deng et al. \\[[36](https://arxiv.org/html/2402.01874v1#bib.bib36)\\] is an
optimization approach where a policy network is trained to generate desired
prompts. The experimental results show that the policy is transferable across
different LMs, which allows learning cheaply from smaller models and infer for
larger, more powerful models. The authors also noted that optimized prompts
were often grammatical ‚Äúgibberish‚Äù, which indicates that high-quality LM
prompting does not necessarily follow human language patterns. However,
RLPROMPT treats the LLM as a black box model with only access to the generated
output whereas TEMPERA assumes it to have access to the embedding vectors. The
policy models are similar with GPT encoder but the action space is very
different, since TEMPERA uses only discrete actions, whereas RLPROMPT treats
the entire vocabulary as possible actions. Finally, the performance of TEMPERA
was benchmarked for text classification tasks, whereas RLPROMPT was applied to
text generation. More recently, Sun
\\[[115](https://arxiv.org/html/2402.01874v1#bib.bib115)\\] proposed Prompt-
OIRL, a framework that uses offline reinforcement learning to achieve cost-
efficient and context-aware prompt design. The framework utilizes readily
available offline datasets generated through expert evaluation of previously
crafted prompts. First, the authors apply inverse-RL to learn a proxy reward
model that can perform query-dependent offline prompt evaluations. Then, they
use this model as an offline evaluator to perform query-dependent prompt
optimization. Contrary to
\\[[36](https://arxiv.org/html/2402.01874v1#bib.bib36)\\], who perform task-
agnostic prompt optimization, Prompt-OIRL performs query-dependent prompt
evaluation, similarly to
\\[[145](https://arxiv.org/html/2402.01874v1#bib.bib145)\\]. The dependence of
prompt evaluation on the query allows for context awareness, which helps the
prompt evaluator predict what prompting techniques (e.g., Chain of Thought)
are most likely to obtain correct answer given a specific prompt (e.g., an
arithmetic question). The design of the proxy reward function allows for
offline query-dependent evaluation thus achieving both context awareness and
lower costs, and the framework was evaluated across four LLMs and three
arithmetic datasets. A side-by-side comparison of the three methods is shown
in Table [2](https://arxiv.org/html/2402.01874v1#S4.T2 "Table 2 ‚Ä£ 4.2 RL4LLM-
Prompt ‚Ä£ 4 RL4LLM: Using Reinforcement Learning to Enhance Large Language
Models ‚Ä£ The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement
Learning and Large Language Models"). Table 2: RL4LLM-Prompt Studies Another
study where Reinforcement Learning with AI feedback is used to ensure
harmlessness of AI assistants ‚Äì this time by using RL for prompt design - is
the study of Perez et al.
\\[[96](https://arxiv.org/html/2402.01874v1#bib.bib96)\\], who used a Language
Model to generate test questions (‚Äúred teaming‚Äù) that aim to elicit harmful
responses from the target LM. Then, a classifier that is trained to detect
offensive content is used to evaluate the target LM‚Äôs replies to those
questions. Contrary to studies in previous sections, this study uses RL to
train the red-teaming LLM, instead of fine-tuning or prompting the target LLM.
More precisely, starting from a pretrained LM for red teaming, the authors
perform a first pass of fine-tuning using supervised learning. Then, they use
RL to train the red-teaming LLM in a synchronous advantage actor-critic (A2C)
framework with the objective of maximizing the expected harmfulness. The
reward function is a linear combination of the A2C loss and the K-L divergence
penalty between the target policy and the distribution of the initialization
over the next tokens. The authors performed red teaming for a variety of
harmful behaviors, including offensive language, data leakage, personal
contact information generation, and distributional bias of the output text.
LM-based red teaming was shown to be a promising tool for the timely
identification of potentially harmful behavior, with RL-based red teaming
being the most effective at eliciting offensive replies compared to the other
methods, which included zero-shot and stochastic few-shot generation, and
supervised learning. Table 3: RL4LLM Natural Language Processing Application 5
LLM4RL: Enhancing Reinforcement Learning Agents through Large Language Models
-------------------------------------------------------------------------------
The LLM4RL class covers studies where an LLM acts as a component of an RL
training pipeline. Contrary to RL4LLM studies (section
[4](https://arxiv.org/html/2402.01874v1#S4 "4 RL4LLM: Using Reinforcement
Learning to Enhance Large Language Models ‚Ä£ The RL/LLM Taxonomy Tree:
Reviewing Synergies Between Reinforcement Learning and Large Language
Models"), where the end goal is an NLP task, the RL agent in this category is
trained for tasks which are generally not related to natural language.
Overall, the motivation behind studies in the LLM4RL category is twofold, as
shown on Table [4](https://arxiv.org/html/2402.01874v1#S5.T4 "Table 4 ‚Ä£ 5.3.5
Using an adapter model ‚Ä£ 5.3 LLM4RL-Policy ‚Ä£ 5 LLM4RL: Enhancing Reinforcement
Learning Agents through Large Language Models ‚Ä£ The RL/LLM Taxonomy Tree:
Reviewing Synergies Between Reinforcement Learning and Large Language
Models"). 1. 1.Improved performance of the RL Agent: In LLM4RL frameworks,
improving the performance of the agent requires alignment with human intent or
feedback \\[[56](https://arxiv.org/html/2402.01874v1#bib.bib56),
[113](https://arxiv.org/html/2402.01874v1#bib.bib113),
[75](https://arxiv.org/html/2402.01874v1#bib.bib75),
[67](https://arxiv.org/html/2402.01874v1#bib.bib67),
[138](https://arxiv.org/html/2402.01874v1#bib.bib138)\\], grounding of the
agent to its environment
\\[[138](https://arxiv.org/html/2402.01874v1#bib.bib138),
[22](https://arxiv.org/html/2402.01874v1#bib.bib22)\\] or learning learning
complex tasks \\[[34](https://arxiv.org/html/2402.01874v1#bib.bib34),
[75](https://arxiv.org/html/2402.01874v1#bib.bib75)\\] 2. 2.Efficient training
of the RL Agent: Training an RL agent can be computationally intensive,
requiring not only significant computational resources, but also large amounts
of data. Even with those prerequisites available, RL training might still
suffer due to inefficient sampling, especially for complex, long-term tasks.
Therefore, LLM4RL frameworks also focus on improving training efficiency ‚Äì and
therefore ensuring satisfying execution of the target tasks at test time ‚Äì by
facilitating exploration
\\[[101](https://arxiv.org/html/2402.01874v1#bib.bib101),
[41](https://arxiv.org/html/2402.01874v1#bib.bib41)\\], policy transfer of
trained models \\[[106](https://arxiv.org/html/2402.01874v1#bib.bib106)\\] and
effective planning for reduced data requirements
\\[[34](https://arxiv.org/html/2402.01874v1#bib.bib34)\\]. The LLM replaces or
assists in different ways one of the fundamental components of the
reinforcement learning agent - namely, the reward function, the training goal,
or the policy function. Using the corresponding component in each case as a
criterion, we further break down the LLM4RL class in three sub-categories,
where the LLM is used for a) determining the reward function (LLM4RL-Reward),
b) expressing internal goals (LLM4RL-Goal), and c) pretraining, representing,
or updating the policy function (LLM4RL-Policy). ### 5.1 LLM4RL-Reward As
noted by \\[[118](https://arxiv.org/html/2402.01874v1#bib.bib118)\\], ‚Äúthe use
of a reward signal to formalize the idea of a goal is one of the most
distinctive features of reinforcement learning‚Äù. The reward signal received
through the interaction with the environment is critical for training an agent
to achieve the desired behavior. Until recently, the bulk of RL research
treated the reward function as given and focused on the training algorithms
themselves \\[[42](https://arxiv.org/html/2402.01874v1#bib.bib42)\\].
Designing the reward function of an RL training framework is straightforward
when direct knowledge of the problem is available, as when solving a well-
defined problem or earning a high score in a well-defined game. Common
examples in this category are Atari games, which are frequently utilized as
sandbox environments for testing various aspects of RL training, or games
where the agent receives a positive reward if they win, and negative reward
otherwise. However, there exists a significant number of applications where it
is difficult to directly translate the desired behavior into rewards signals,
especially for long and complex tasks or when the agent can discover
unexpected ways, and potentially dangerous, ways to generate reward from the
environment. When the agent must learn to perform a long and possibly complex
task, where designing a proper reward function not straightforward, and where
human feedback is critical, it is common to rely on expert demonstrations or
interactive modification of the reward signal. Expert demonstrations generally
utilize on a technique known as Inverse Reinforcement Learning to infer the
reward function by observing the desired behavior
\\[[2](https://arxiv.org/html/2402.01874v1#bib.bib2)\\], with a reward
designer observing the agent‚Äôs performance and tweaking the reward signal
during a trial-and-error process to asjust the agent‚Äôs behavior accordingly.
Motivated by the direct involvement of humans in this interactive loop,
combined with the ability of LLMs to learn in-context from few or even zero
examples \\[[16](https://arxiv.org/html/2402.01874v1#bib.bib16)\\],
researchers are exploring ways to bridge the gap between human preference and
agent behavior in cases where the explicit quantification of rewards is
difficult or time-consuming. In this context, the LLM is used either for
reward shaping, i.e., i.e., guiding the learning agent with additional rewards
to preserve policy optimality, or as a proxy reward function. Prior to the LLM
era, \\[[50](https://arxiv.org/html/2402.01874v1#bib.bib50)\\] used language
for reward shaping: They trained a model to predict if the actions in a
trajectory match some specific language description and used the output to
generate intermediate RL rewards. Similarly,
\\[[21](https://arxiv.org/html/2402.01874v1#bib.bib21)\\] extended the
underlying Markov Decision Process by including a natural language
instruction; the authors first generated instructions and obtained the
corresponding word embeddings using BERT, and then trained an alignment model
that maps action trajectories to their corresponding instructions. They found
that augmenting the default reward of an Atari environment with the language-
based reward significantly improves the performance of the agent. The first
study utilizing LLMs for RL agent reward design is the one by Kwon et al.
\\[[67](https://arxiv.org/html/2402.01874v1#bib.bib67)\\], who evaluated
whether LLMs can produce reward signals that are consistent with the user
behavior using GPT-3 \\[[17](https://arxiv.org/html/2402.01874v1#bib.bib17)\\]
as a proxy reward function in an RL framework. At the beginning of training,
the user specifies the desired behavior using a prompt with explanation and an
example of the desired behavior, while during training, the LLM evaluates the
agent‚Äôs behavior against the behavior described in the prompt and generates a
reward signal accordingly, which the RL agent uses to update its behavior. In
more detail, the proposed framework is as follows: The LLM is provided with a
task description, a user‚Äôs description of the objective, a string-formatted
episode outcome, and a question asking if the outcome satisfies the objective.
The LLM‚Äôs response to the question is used as reward signal for the agent.
Based on this reward signal, the agent updates their weights and generates a
new episode, the outcome of which is parsed back into a string, and the
episode continues. To evaluate their framework, the authors compared it to
three baseline cases: a) a few-shot baseline, where a supervised learning
model is trained to predict reward signals using the same examples given to
the LLM, b) a zero-shot baseline, where the LLM is prompted without the user‚Äôs
description of the objective, and c) a baseline where the agents are trained
with ground truth reward functions. Experimental results showed that the
proposed RL training framework - which is agnostic to the RL algorithm used -
can achieve user objective-aligned behavior, as measured both with automated
metrics and with human users. In addition, the performance of the agents is
shown to outperform agents trained with reward functions learned via
supervised learning, even when no examples were provided - as long as the
objective is well-defined - or when the tasks were complicated. Grounding a
robotic agent to the environment and achieving the desirable behavior as
directed through human feedback is the focus of TEXT2REWARD framework by Xie
et al. \\[[138](https://arxiv.org/html/2402.01874v1#bib.bib138)\\].
TEXT2REWARD allows for the generation and continuous improvement of python
code that expresses dense rewards functions for robotic agents performing
manipulation tasks. The framework is composed of three stages: Abstraction,
Instruction, and Feedback. In the Abstraction stage, an expert provides an
abstract representation of the robot‚Äôs environment using Python classes. In
the Instruction stage, a user provides a natural language description of the
goal to be achieved by the robot (e.g., ‚Äúpush the chair to the marked
position‚Äù). Finally, in the Feedback phase, the user summarizes their
preferences or the failure mode of the robot‚Äôs action. This summary is then
used to update the reward function and retrain the RL agent. The authors
evaluated their framework on two robotic manipulation benchmarks - MANISKILL2
\\[[52](https://arxiv.org/html/2402.01874v1#bib.bib52)\\] and METAWORLD
\\[[141](https://arxiv.org/html/2402.01874v1#bib.bib141)\\] - and two MUJOCO
locomotion environments
\\[[15](https://arxiv.org/html/2402.01874v1#bib.bib15)\\]. For manipulation
tasks, the experiments demonstrated comparable results to human oracle-
designed rewards in terms of performance and convergence speed, with the
performance improvement verified through few-shot examples. For locomotion
tasks, the agent was able to successfully learn six new behaviors (move
forward, front flip, back flip, etc.) with a rate of success ranging from 94%
to 100% for each task. Finally, error analysis revealed that the generated
code was correct at 90% of the time, with most common errors originating from
wrong use of class attributed (wrong use or hallucination of non-existent
attributes), syntax errors or shape mismatch, or wrong imports. The success of
TEXT2REWARD was largely owed to the use of human feedback to resolve ambiguity
through providing clear, language-based instructions to correct the behavior
of the robot. Aside from grounding and alignment to human preferences,
TEXT2REWARD has the advantage of generating of highly interpretable functions,
while requiring any data for reward training. The EUREKA framework by Ma et
al. \\[[75](https://arxiv.org/html/2402.01874v1#bib.bib75)\\] is another
example of reward design through direct python code generation. EUREKA
consists of three fundamental components: the use of environment as context,
evolutionary search, and reward reflection. The environment code, excluding
the part of it that defines the reward, is directly provided to the LLM, which
in turn extracts its semantics to compose a suitable reward function for the
target task. The LLM outputs the reward code, following some general
formatting instructions. Evolutionary computation is used to overcome sub-
optimal or non-executable code by sequentially generating improved reward
functions using the concepts of reward mutation, which modifies previous
solutions based on textual feedback, and random restarts, to escape local
optima. Finally, reward reflection acts as a supplement to the numeric value
of the reward expressed through the fitness function by explaining why a
candidate reward function works or does not work and assigning credit
accordingly. The framework follows a PPO architecture and is tested on a
variety of benchmarking environments (e.g., Cartpole and BallBalance), where
it achieves a higher success rate compared to human-specified rewards. The use
of evolutionary computation is shown to be necessary for continuous increase
in the success rate of the framework over time, while also allowing for the
generation of more diverse and often counter-intuitive rewards that outperform
human-designed rewards, particularly for difficult tasks. The authors also
implemented a curriculum learning
\\[[13](https://arxiv.org/html/2402.01874v1#bib.bib13)\\] approach to teach a
Shadow Hand to rotate a pen according to a set of pre-defined spinning
patterns, thus demonstrating the capability of the framework to execute
complex, low-level tasks. By successfully handling task complexity and
allowing for the discovery of unexpected high-performing policies, EUREKA
successfully deals with two key reasons that inhibit the translation of the
desired agent behavior to rewards that were identified in subsection
[5.1](https://arxiv.org/html/2402.01874v1#S5.SS1 "5.1 LLM4RL-Reward ‚Ä£ 5
LLM4RL: Enhancing Reinforcement Learning Agents through Large Language Models
‚Ä£ The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning
and Large Language Models"). Finally, similarly to the LLM4RL\\-Reward studies
discussed in [5.1](https://arxiv.org/html/2402.01874v1#S5.SS1 "5.1 LLM4RL-
Reward ‚Ä£ 5 LLM4RL: Enhancing Reinforcement Learning Agents through Large
Language Models ‚Ä£ The RL/LLM Taxonomy Tree: Reviewing Synergies Between
Reinforcement Learning and Large Language Models"), a key benefit of EUREKA is
the alignment of rewards to human preferences by incorporating human knowledge
about the state through appropriate initialization of the reward. Similarly,
Song et al. \\[[113](https://arxiv.org/html/2402.01874v1#bib.bib113)\\]
proposed a three-step, self-refined LLM framework for generating reward
functions to help robotic agents achieve specific goals. The first step is the
initial design of the reward function based on the natural language input
provided by the user. The input includes a description of the environment, a
description of the task in hand, broken down into goals, a description of the
observable state (such as position and velocity of robot), and a list of rules
that should be followed when designing the reward function (for example,
restricting the dependence of the reward exclusively on known quantities). In
the second step, the initial reward function is applied, the robot acts, and
its behavior is evaluated. In the evaluation step, the user collects their
observations around the training process and convergence, the objective
metrics, the success rate of the tasks, and makes an overall assessment
(‚Äúgood‚Äù or ‚Äúbad‚Äù) of the robotic agent‚Äôs performance. Finally, in the self-
refinement step, the feedback of the user is embedded in a feedback prompt,
which is then used by the LLM to generate the next reward signal. The authors
evaluated the performance of the framework for nine different continuous
control tasks across three different robotic system systems and achieved a
success rate between 93% and 100% for all tasks, outperforming the
corresponding manual reward setting in almost all of them. ### 5.2 LLM4RL-Goal
Goal setting is a key element in intrinsically motivated reinforcement
learning \\[[28](https://arxiv.org/html/2402.01874v1#bib.bib28)\\], which
addresses key challenges of the traditional Deep RL setting, including the
difficulty to abstract actions and to explore the environment
\\[[6](https://arxiv.org/html/2402.01874v1#bib.bib6)\\]. Contrary to
traditional RL, where the training of the agent relies exclusively on external
rewards from the environment, intrinsic RL builds on the psychology concept of
intrinsic motivation and developmental learning, which is inspired by babies
developing their skills by exploring the environment
\\[[6](https://arxiv.org/html/2402.01874v1#bib.bib6)\\]. In a similar manner,
Intrinsic Motivation in RL allows agents to learn reusable, general skills
that are applicable to various tasks over their lifetime. With right internal
goals, agents can autonomously explore open-ended environments and build
useful skills at a pretraining phase. Intrinsic RL is therefore particularly
useful when the design of a reward function is not straightforward. However,
the benefits of intrinsic RL are limited when the environment complexity and
size increases significantly, since there is no guarantee that the skills the
agent builds throughout its lifetime will be useful to perform any downstream
tasks at all. To address this shortcoming, Du et al.
\\[[41](https://arxiv.org/html/2402.01874v1#bib.bib41)\\] proposed a new,
intrinsically motivated RL method called ELLM (Exploring with LLMs). The
authors build on the observation that, when faced with new tasks, humans do
not uniformly (and therefore, blindly) explore the outcome spaces of their
actions, but rely on their physical and social common sense to prioritize the
exploration of plausibly useful behaviors, that have the reasonably highest
likelihood to succeed, such as using a key to open a door. ELLM leverages
knowledge from text corpora to capture useful semantic information and thus
enable structured exploration of task-agnostic (i.e., pretraining)
environments by allowing the agent to use this knowledge to reason about the
usefulness of new begaviors. In particular, the authors used a pre-trained
LLM, GPT-2 \\[[102](https://arxiv.org/html/2402.01874v1#bib.bib102)\\] to
suggest goals during exploration. In each step, the LLM is prompted with a
list of the agent‚Äôs available actions, along with a text description of the
current observation, and suggests a goal, which should be diverse, common-
sense sensitive, and context sensitive. After the acgent takes an action and
transitions in the environment, the goal-conditioned rewards are computed
based on the semantic similarity between the LLM-generated goal and the
description of the agent‚Äôs transition. The exploration of RL training is
therefore controlled and enhanced without the need for explicit human
intervention. ELLM was shown capable of producing context-sensitive, common-
sensical and diverse goals, boosting pretraining exploration performance, as
well as performance on downstream tasks. TaskExplore by Quartey et al.
\\[[101](https://arxiv.org/html/2402.01874v1#bib.bib101)\\] is another
framework that uses LLMs to facilitate RL training by generating intermediate
tasks. The goal of TaskExplore is to maximize the use of previous experiences
collected by a robotic agent during their training in a simulated home
environment by generating and learning useful auxiliary tasks while solving a
larger target task. The process starts by converting a task specification to a
graph using Linear Temporal Logic. Then, the graph is traversed to create an
abstract task template where object propositions are replaced with their
context-aware embedding representations, which have been generated by an LLM.
The abstract task template is subsequently used to generate auxiliary tasks
for each proposition node by swapping relevant objects from the environment
and selecting objects with the most similar embeddings with the template
embedding node under consideration. The training process consists of both
online RL (where a policy for the given task is learned by following an
epsilon-greedy behavioral policy) and offline RL (where all Q-value functions,
including those of subtasks, are updated). Interestingly, the authors found
that learning the auxiliary tasks does not adversely affect the performance of
learning the main task. In addition, the auxiliary tasks learned through
TaskExplore performed better compared to the same tasks learned through a
random behavior policy. Finally, the curriculum built on auxiliary tasks
developed through TaskExplore outperformed the corresponding curriculum
developed by randomly sampled tasks. ### 5.3 LLM4RL-Policy In this subclass, a
large language model directly assists the policy of an RL agent by generating
trajectories for pretraining
\\[[106](https://arxiv.org/html/2402.01874v1#bib.bib106)\\], creating a policy
prior \\[[56](https://arxiv.org/html/2402.01874v1#bib.bib56)\\], acting as a
planner in a Planner-Actor-Reporter scheme
\\[[34](https://arxiv.org/html/2402.01874v1#bib.bib34)\\], by directly
representing the policy
\\[[22](https://arxiv.org/html/2402.01874v1#bib.bib22)\\], or by combining an
adapter model to fine-tune the prompt of another LLM that generates
instructions \\[[146](https://arxiv.org/html/2402.01874v1#bib.bib146)\\]. ####
5.3.1 Trajectory Generation for Pretraining The power of Reinforcement
Learning largely lies in the ability of the agent to learn through interacting
with its environment. However, sometimes the interaction with the environment
is impractical, either because it is expensive (e.g., in robotics), or
dangerous (as in healthcare or autonomous vehicles)
\\[[68](https://arxiv.org/html/2402.01874v1#bib.bib68)\\]. However, as
explained in the LLM4RL-Reward subsection
([5.1](https://arxiv.org/html/2402.01874v1#S5.SS1 "5.1 LLM4RL-Reward ‚Ä£ 5
LLM4RL: Enhancing Reinforcement Learning Agents through Large Language Models
‚Ä£ The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning
and Large Language Models"), even when such interaction is possible and
assuming available training data and computational resources, training a model
from scratch might still face slow convergence. Therefore, pretraining with
previously collected data can benefit the agent, particularly in complex
domains where large datasets are needed for scalable generalization. In such
cases, Offline Reinforcement Learning is commonly used. Offline RL handles the
control problem as a sequence modeling problem
\\[[27](https://arxiv.org/html/2402.01874v1#bib.bib27),
[61](https://arxiv.org/html/2402.01874v1#bib.bib61),
[46](https://arxiv.org/html/2402.01874v1#bib.bib46)\\] and uses supervised
learning to fit a dataset consisting of state-action-reward trajectories. By
framing offline RL as a sequence modeling problem, Reid et al.
\\[[106](https://arxiv.org/html/2402.01874v1#bib.bib106)\\] investigate
whether LLMs can be successfully transferred to other domains when fine-tuned
on offline RL tasks that have no relation to language. The authors modeled
trajectories autoregressively, representing each trajectory tùë°titalic\\_t as a
sequence of the form
t\=(R^1,s1,Œ±1,R^2,s2,Œ±2,‚Ä¶,R^N,sN,Œ±N)ùë°subscript^ùëÖ1subscriptùë†1subscriptùõº1subscript^ùëÖ2subscriptùë†2subscriptùõº2‚Ä¶subscript^ùëÖùëÅsubscriptùë†ùëÅsubscriptùõºùëÅt=(\\\hat{R}\\_{1},\\\allowbreak
s\\_{1},\\\allowbreak\\\alpha\\_{1},\\\allowbreak\\\hat{R}\\_{2},%
s\\_{2},\\\alpha\\_{2},\\\allowbreak...,\\\hat{R}\\_{N},s\\_{N},\\\allowbreak\\\alpha\\_{N})italic\\_t
= ( over^ start\\_ARG italic\\_R end\\_ARG start\\_POSTSUBSCRIPT 1
end\\_POSTSUBSCRIPT , italic\\_s start\\_POSTSUBSCRIPT 1 end\\_POSTSUBSCRIPT ,
italic\\_Œ± start\\_POSTSUBSCRIPT 1 end\\_POSTSUBSCRIPT , over^ start\\_ARG
italic\\_R end\\_ARG start\\_POSTSUBSCRIPT 2 end\\_POSTSUBSCRIPT , italic\\_s
start\\_POSTSUBSCRIPT 2 end\\_POSTSUBSCRIPT , italic\\_Œ± start\\_POSTSUBSCRIPT
2 end\\_POSTSUBSCRIPT , ‚Ä¶ , over^ start\\_ARG italic\\_R end\\_ARG
start\\_POSTSUBSCRIPT italic\\_N end\\_POSTSUBSCRIPT , italic\\_s
start\\_POSTSUBSCRIPT italic\\_N end\\_POSTSUBSCRIPT , italic\\_Œ±
start\\_POSTSUBSCRIPT italic\\_N end\\_POSTSUBSCRIPT ), with
r^isubscript^ùëüùëñ\\\hat{r}\\_{i}over^ start\\_ARG italic\\_r end\\_ARG
start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT,
sisubscriptùë†ùëñs\\_{i}italic\\_s start\\_POSTSUBSCRIPT italic\\_i
end\\_POSTSUBSCRIPT, and aisubscriptùëéùëña\\_{i}italic\\_a start\\_POSTSUBSCRIPT
italic\\_i end\\_POSTSUBSCRIPT representing the state, action, and cumulative
reward, respectively, at time step tùë°titalic\\_t. The final objective of the
supervised learning was a weighted sum of three loss functions: The primary
loss function is a classic Mean Squared Error loss. The second loss function
is a cosine similarity loss that quantifies the similarity between language
representations and offline RL input representations, with the goal to make
the input language embeddings as similar as possible to their language
counterparts. Finally, the third loss function represents the negative
log=likelihood-based language modeling objective to allow for joint training
of language modeling and trajectory modeling. The authors used pre-trained
models GPT2-small and ChibiT, a model that was pretrained on Wikitext-103
dataset \\[[77](https://arxiv.org/html/2402.01874v1#bib.bib77)\\]. As shown by
experiments in four Atari tasks and three OpenAI Gym tasks, pretraining with
language datasets can successfully fine-tune offline RL tasks and, most
importantly, by achieving significant gains in terms of both convergence speed
and total reward received, compared to baseline offline RL including Decision
Transformer \\[[24](https://arxiv.org/html/2402.01874v1#bib.bib24)\\], CQL
\\[[66](https://arxiv.org/html/2402.01874v1#bib.bib66)\\], TD3+BC
\\[[45](https://arxiv.org/html/2402.01874v1#bib.bib45)\\], BRAC
\\[[135](https://arxiv.org/html/2402.01874v1#bib.bib135)\\], and AWR baselines
\\[[95](https://arxiv.org/html/2402.01874v1#bib.bib95)\\]. #### 5.3.2 Creating
a Policy Prior In subsection [5.1](https://arxiv.org/html/2402.01874v1#S5.SS1
"5.1 LLM4RL-Reward ‚Ä£ 5 LLM4RL: Enhancing Reinforcement Learning Agents through
Large Language Models ‚Ä£ The RL/LLM Taxonomy Tree: Reviewing Synergies Between
Reinforcement Learning and Large Language Models"), we reviewed studies where
human-agent alignment is achieved through reward design. An alternative way to
train RL agents that behave according to human preferences is the creation of
a policy prior that is aligned with human preferences and reasoning. Hu and
Sadigh \\[[56](https://arxiv.org/html/2402.01874v1#bib.bib56)\\] propose
Instruct-RL, a framework for human-AI coordination where the human uses high-
level natural language instructions to specify to the AI agent the type of
behavior they expect. The human natural language instruction is then passed to
pre-trained LLMs, which produce a prior policy. To construct the prior, the
LLM is supplied with the initial human instruction as well as a language
prompt, which provides a language description of the observations of the
environment while the policy is being followed. In practice, the LLM uses a
softmax function to predict the probability of possible actions given the
observed state and the human instruction. Finally, the policy prior is used as
a reference policy during the training of the RL agent to regularize the
training objective, with the regularization technique varying according to the
RL training algorithm, by augmenting the epsilon-greedy method in Q-learning
and adding a KL penalty to the training objective in PPO. The experimental
results based on both the performance of the algorithms in benchmarking tasks
(Hanabi and a negotiating game) and the human evaluation demonstrated that
instructRL successfully incorporated human preference to produce high-
performance policies, even when the prior policies were imperfect and
generated with simple prompts. However, the authors highlighted the need for
fine-tuning to improve test-time performance, since adding the LLM priors to
trained agents was shown to add no meaningful improvement to the policy. ####
5.3.3 LLM Being the Policy While LLMs possess general reasoning capabilities,
they are not trained to solve environment specific problems during their
training, and thus cannot influence or explore the specific environment where
a task needs to be accomplished. Carta et al.
\\[[22](https://arxiv.org/html/2402.01874v1#bib.bib22)\\] proposed a framework
to overcome the lack of alignment between the general statistical knowledge of
the LLM and the environment by functionally grounding LLMs and using them
directly as the policy to be updated. Using a simple grid-world based text
environment (Baby AI), the authors formulated a goal-augmented, partially
observable MDP where, given a prompt p, the LLM outputs a probability
distribution over the possible actions and an action is sampled according to
this distribution. The authors used Proximal Policy Optimization to train the
agent and used Flan-T5 780 M as the policy LLM and observed 80% success rate
after 250 K training steps, which is a significant improvement compared to
previous models. The authors also considered genitalization over new objects,
where a drop in performance was observed, with the model still outperforming
the benchmark. The biggest drop is observed when testing against
generalization to new tasks or a different language. #### 5.3.4 Planner
Dasgupta et al. \\[[34](https://arxiv.org/html/2402.01874v1#bib.bib34)\\]
combined the reasoning capabilities of the LLM with the specialization and the
knowledge of the environment of an RL agent in a Planner-Actor-Reporter
scheme. This study belongs to the RL + LLM cateogry, and will therefore be
analyzed in section [5](https://arxiv.org/html/2402.01874v1#S5 "5 LLM4RL:
Enhancing Reinforcement Learning Agents through Large Language Models ‚Ä£ The
RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning and
Large Language Models"). However, the authors also designed a variation of the
main framework where the Planner is embedded in the training loop of the
Reporter with the goal to increase its truthfulness, so that it reports
accurate information to the planner. The Reporter uses the reward received at
the end of the episode to update their reporting policy such that it
eventually learns to only report helpful, i.e. truthful and relevant,
information back to the planner. The reader shall refer to section
[6](https://arxiv.org/html/2402.01874v1#S6 "6 RL+LLM: Combining Independently
Trained Models for Planning ‚Ä£ The RL/LLM Taxonomy Tree: Reviewing Synergies
Between Reinforcement Learning and Large Language Models") for more details on
this publication. #### 5.3.5 Using an adapter model Zhang and Lu
\\[[146](https://arxiv.org/html/2402.01874v1#bib.bib146)\\] developed the
RLAdapter Framework, a complex system that, apart from the RL agent and the
LLM, includes additionally an Adapter model to improve the connection between
the RL agent and the LLM without requiring the costly and often impossible
fine-tuning of the base LLM. The Adapter itself is also an LLM - in
particular, a 4-bit quantized version of the LLaMA2-7B model, which is fine-
tuned with feedback from the RL agent and the LLM. Embedding the adapter model
in the RL training loop is shown to result in more meaningful guidance
overall, by enhancing both the LLM‚Äôs comprehension of downstream tasks as well
as the agent‚Äôs understanding capability and effective learning of difficult
tasks. The key metric that aids this closed-loop feedback is the understanding
score, which quantifies the semantic similarity between the agent‚Äôs recent
actions and the sub-goals suggested by the LLM as measured by the cosine
similarity between the embeddings of the LLM-provided sub-goals and the
episode trajectory. The prompt of the Adapter includes a description pf the
player‚Äôs observations, past actions, past sub-goals, and the understanding
score. The Adapter generates a prompt for the base LLM, containing a natural
language summary of the player‚Äôs past observations, actions, and
understanding. In turn, the base LLM generates updated instructions for the RL
agent, which, as usual, takes an action, receives the response from the
environment, and updates its policy. The understanding score is then
calculated and is used to fine-tune the adapter model, which then goes on to
generate a new prompt. The performance of RLAdapter with GPT-3.5
\\[[86](https://arxiv.org/html/2402.01874v1#bib.bib86)\\] as baseline was
compared to baseline models, including ELLM by
\\[[41](https://arxiv.org/html/2402.01874v1#bib.bib41)\\]. RLAdapter was shown
to outperform all baselines for 1 million steps, apart from ELLM; however, for
5 million steps, the performance of RLAdapter with GPT-4
\\[[87](https://arxiv.org/html/2402.01874v1#bib.bib87)\\] exceeded that of
ELLM as well, and RLAdapter with GPT-3.5
\\[[86](https://arxiv.org/html/2402.01874v1#bib.bib86)\\] matched SPRING by
\\[[136](https://arxiv.org/html/2402.01874v1#bib.bib136)\\] in terms of
performance. The main novelty of RLAdapter lies in fine-tuning the
lightweight, cheaper-to-update adapter model, while only updating the prompt
of the base LLM. Despite the LLM prompt being updated through this feedback
loop, we do not classify this study as RL4LLM, since RL is not used to improve
the performance of a downstream language task. Table 4: Breakdown Of LLM4RL
Synergy Goals Per Subclass Table 5: LLM4RL Environment and Applications Table
6: LLM4RL Base Algorithms and RL Architecture 6 RL+LLM: Combining
Independently Trained Models for Planning
------------------------------------------------------------- The last major
class of studies includes those where the RL agent and the LLM are fundamental
components of the same framework and where, contrary to the two previous
categories, they are independent from each other. In this class, an RL agent
is trained to learn specific skills, and the LLM leverages its knowledge of
the real world to determine ways to plan over those skills in order to
accomplish a task. This combination results in an agent that knows how to
perform long-horizon tasks in a specific environment. This category can be
further refined based on whether planning relies on conversational feedback or
not. In the first subcategory, RL+LLM-No Language Feedback, the LLM generates
a static skill graph but does not participate in the planning process after
that. In the second subcategory, (RL+LLM-With Language Feedback), the user
query or the LLM prompt is updated according to the results of the interaction
between the agent and the environment in each step. However, we should
highlight that RL+LLM-With Language Feedback studies where the prompt is
modified during planning are completely different from the RL4LLM-Prompt
studies that were analyzed in section
[4.2](https://arxiv.org/html/2402.01874v1#S4.SS2 "4.2 RL4LLM-Prompt ‚Ä£ 4
RL4LLM: Using Reinforcement Learning to Enhance Large Language Models ‚Ä£ The
RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning and
Large Language Models"): The goal of frameworks of the RL4LLM - Prompt
category is the improvement of the LLM itself, while the goal of frameworks in
the RL + LLM category is planning towards a downstream task that is not
directly related to the LLM - or natural language in general. ### 6.1 RL+LLM-
Without Language Feedback Aside from grounding robotic agents to their
environment, RL + LLM combinations have been shown to be beneficial for
learning multiple, long-horizon tasks in an open-ended environment, as in the
case of Yuan et al.
\\[[142](https://arxiv.org/html/2402.01874v1#bib.bib142)\\], who developed
Plan4MC, an framework for executing Minecraft tasks. As the authors explain,
exploration under long-horizon tasks owes its difficulty to the size,
complexity, and partial observability of open-ended environments. The most
common strategy so far in Reinforcement learning literature and practice has
been imitation learning, which relies on expert demonstrations or video
datasets, which are frequently difficult to obtain. However, even setting
aside this obstacle, training an RL agent in a large state space is inevitably
hard due to sample inefficiency, while skipping demonstrations is not a viable
choice, since it might allow the agent to learn only a very restricted set of
skills. As such, the main idea of
\\[[142](https://arxiv.org/html/2402.01874v1#bib.bib142)\\], it to break down
tasks into basic, short-horizon skills, learn those separately, and plan over
skills ‚Äì in other words, find the proper sequence of skills to be executed. In
this context, reinforcement learning is used to train the fundamental skills,
which are classified as ‚ÄúFind‚Äù, ‚ÄúManipulate‚Äù, and ‚ÄúCraft‚Äù, independently in
advance. It is worth nothing that each type of skill is trained with a
different algorithm. The authors prompt ChatGPT
\\[[85](https://arxiv.org/html/2402.01874v1#bib.bib85)\\] by providing the
context, along with an example of the output in the desired format, and
ChatGPT produces the skill graph in the desired format, where nodes represent
skills and arcs represent ‚Äúrequire‚Äù and ‚Äúconsume‚Äù relationships. During online
planning, the agent alternates between skill planning (i.e., identifying a
feasible plan to achieve the goal by performing depth-first search on the
skill graph) and skill execution (i.e., selecting policies to solve the
complicating skills, and reverting to skill planning if the execution of a
task fails). The experimental results confirmed the validity of Plan4MC, which
was shown to achieve higher success rate compared to other variations of the
algorithm, including those without task decomposition or without separate
learning of ‚ÄúFind‚Äù. In a separate set of experiments, ChatGPT was also used to
generate the plan, but this model variation was outperformed by the original
Plan4MC version. ### 6.2 RL+LLM-With Language Feedback In a common planning
scheme under this category, the LLMs generate instructions for tasks that the
agent has already learned through Reinforcement Learning, while the feedback
from the environment is used to update the instructions. The feedback from the
environment includes natural language (although not necessarily limited to it
- see \\[[60](https://arxiv.org/html/2402.01874v1#bib.bib60)\\]) and is used
to update the user query or prompt based on the results of the itneraction
between the agent and the environment in each step. ‚ÄúSayCan‚Äù by Ahn et al.
\\[[3](https://arxiv.org/html/2402.01874v1#bib.bib3)\\] involves a robotic
assistant that performs household tasks in a kitchen environment. The authors
rely on the observation that, the knowledge of LLMs about the world, while
vast, is not physically grounded to the environment that the robotic agent is
operating in. As a result, a fully trained robotic agent might not be able to
select the appropriate skills to accomplish a task. The role of reinforcement
learning in this case is to achieve grounding by helping the agent obtain
awareness of the scene. In this case, grounding is measured by calculating the
probability of successfully executing a task using a particular skill in a
given state. Both the LLM and RL directly participate in computing this
probability: the LLM is used to calculate the probability that each skill
contributes to completing the instruction, while the affordance function of
the RL agent provides the probability that each skill will be executed
successfully. The product of those two quantities is the probability that a
skill will successfully perform the instruction. Then, the most probable skill
is selected, its policy is executed, and the LLM query is amended to include
the language description of the skill. The plan is formulated as a dialogue
between the robot and the user, where the user provides a high-level
instruction and the robot responds by listing the skill sequence that is it
going to execute. SayCan was evaluated on 101 different tasks in a real
kitchen environment To improve the performance of the system, the LLM
undergoes prompt engineering to ensure that it produces skill recommendations
that are consistent with the user query. In fact, the authors found that the
performance of the LLM improved when a) the sequential steps were explicitly
numbered, b) the objects mentioned presented variation across the prompt
examples, c) careful and error-free phrasing of the names of skills and
objects. In a separate set of experiments, SayCan was integrated with Chain of
Thought \\[[131](https://arxiv.org/html/2402.01874v1#bib.bib131)\\], which was
shown to improve its performance at negotiation tasks. In addition, it was
able to successfully execute instructions provided in languages other than
English. Similarly to‚ÄúSayCan‚Äù, Huang et al.
\\[[60](https://arxiv.org/html/2402.01874v1#bib.bib60)\\]proposed Inner
Monologue, a framework for planning and interaction with robotic agents that
have been trained to execute a variety of skills. Like the previous study of
\\[[3](https://arxiv.org/html/2402.01874v1#bib.bib3)\\], LLMs help the agent
understand what the available skills are, how they affect the environment when
executed, and how the changes in the environment translate to feedback in
natural language. However, contrary to SayCan, Inner Monologue also provides
closed-loop feedback to the LLM predictions. Inner Monologue chains together
three components: a) the pre-trained language-conditioned robotic manipulation
skills, b) a set of perception models, like scene descriptors and success
detectors, and c) human feedback provided by a user that generates natural
language instructions towards the robot. The pretrained manipulation skills
are short-horizon skills accompanied by short language descriptions, and may
be trained through RL. The LLM plays the role of the Planner, whose goal is to
find a sequence of skills that achieve the goal expressed by the user. First,
the Planner receives the human instruction and breaks it down into a sequence
of steps. As it executes the generated plan, the Planner receives three types
of textual feedback from the environment: a) Success Detection, which answers
whether the low-level skill was successful, b) Passive Scene Description,
which is provided without explicitly querying the Planner and includes object
recognition feedback and task-progress scene descriptions, and c) Active Scene
Description, which is provided by a person or a pretrained model (like a
Visual Question Answering model) in response to explicit questions asked by
the LLM. As the robot interacts with its environment, the collected feedback
is continuously appended to the LLM prompt, thus forming an ‚Äúinner monologue‚Äù
that closes the loop from the environment to the agent and therefore enhancing
the planning capabilities of the LLM. Inner Monologue was tested on simulated
and real table-top manipulation environments, as well as a real kitchen mobile
manipulation environment. In the latter, pre-trained affordance functions are
used for action grounding and the results are compared to SayCan by
\\[[3](https://arxiv.org/html/2402.01874v1#bib.bib3)\\] under standard
conditions and under adversarial conditions, with added disturbances during
control policy executions that cause the plan to fail, In all cases, the
embodied feedback provided in the Inner Monologue framework was shown to
improve the success rate of the tasks compared to Inner its predecessor, while
under adversarial conditions it was only Inner Monologue that was able to
consistently complete the instructions successfully. In addition, Inner
Monologue was shown to possess significant reasoning capabilities, including
continuous adaptation to new instructions, self-proposing goals in cases of
infeasiblity, multi-lingual understanding, interactive scene understanding,
and robustness to disturbances in human instructions, like swapping the order
of feedback or typos. Three failure modes were also observed: False positive
and negative success detections, LLM Planning errors due to ignoring the
environment feedback, and control errors. Dasgupta et al.
\\[[34](https://arxiv.org/html/2402.01874v1#bib.bib34)\\] proposed a Planner-
Actor-Reporter scheme to take advantage of the reasoning capabilities of the
LLM and the specialized control skills of a trained RL agent: The LLM acts as
the Planner that receives a task description, performs logical reasoning, and
decomposes the task into a sequence of instructions that it passes to the
Actor, who executes the instructions, while the reporter provides feedback on
the action effects back to the Planner. The framework is implemented in a
partially observable 2-D grid-world
\\[[35](https://arxiv.org/html/2402.01874v1#bib.bib35)\\], with each object
possessing a unique combination of color, shape, and texture. Both the Actor
and the Reporter are RL agents, trained with VTrace loss. The Actor follows a
pre-trained policy that has been trained on simple tasks in the same
environment. To allow the agent to receive feedback both from environment
observations and through natural language, the policy uses two encoders: a
convolutional visual encoder for visual observations and an LSTM-based
language encoder for natural language instructions (e.g., ‚ÄúPick up X‚Äù), and
its action space includes movement within the grid, picking up objects, and
examining objects. The Reporter possesses a similar architecture with the
Planner, since it also includes encoders for vision and language, but it
additionally possesses a memory module and a policy head, which is a binary
classifier head that chooses one of two possible reports. The Reporter
observes the results of the Actor‚Äôs interaction with the environment and
communicates with the Planner to inform it of its next command. In every step,
the information generated by the Reporter is appended to the dialogue
transcript, which is then used as the updated prompt of the Planner, that
generates a new instruction for the next step. The authors argue that training
the Reporter to ignore noise and produce useful feedback for the planner is
more efficient compared to utilizing a large ‚Äì and therefore expensive ‚Äì
planner. From a robustness perspective, they showed that the framework
exhibited robustness for challenging tasks where the Planner needs to
explicitly request specific information to incorporate into its next set of
instructions, as well as for tasks performed in environments for which the LLM
lacks previous semantic experience and therefore needs to perform abstract
logical reasoning. They also showed that the Planner-Actor-Reporter scheme is
better at learning tasks that are difficult to learn using pure RL Baselines.
Table [7](https://arxiv.org/html/2402.01874v1#S6.T7 "Table 7 ‚Ä£ 6.2 RL+LLM-With
Language Feedback ‚Ä£ 6 RL+LLM: Combining Independently Trained Models for
Planning ‚Ä£ The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement
Learning and Large Language Models") summarizes the tasks and applications of
RL+LLM studies. Table 7: RL+LLM Environment and Applications 7 Discussion
------------ ### 7.1 Goals of Synergy and Reasons for Success So far, we have
built our taxonomy based on the structural breakdown of the methods analyzed
in the studies that combine Reinforcement Learning and Large Language Models,
by identifying the way that each of the two models is embedded in the
integrated framework where they coexist and potentially interact and
pinpointing specific components within this framework where the two model
types are interlocked. In this section, we provide a broader view of the
studies by examining the goal of this synergy, in line with the inherent
features of the two models that make the synergy successful. #### 7.1.1
RL4LLM: Responsible AI, Alignment with Human Preference, Performance
Improvement In the RL4LLM case, particular emphasis is placed on improving the
quality of the output of the NLP application that the LLM aims to complete,
the performance of the LLM at task execution, or both. Overall, the primary
quality considerations in the RL4LLM category are Responsible AI and Alignment
with human preferences and intent. Not surprisingly, given the generative
nature of LLMs, Responsible AI is a primary concern of researchers, who wish
to ensure the design of models that are not only helpful, but also harmless.
Research on mitigating potentially harmful output precedes the era of Large
Language Models, with ‚Äúharmfulness‚Äù manifesting itself in a variety of ways:
offensive responses \\[[96](https://arxiv.org/html/2402.01874v1#bib.bib96)\\]
(e.g., unkind, with offensive jokes, or references to morally questionable or
sexual desires), data leakage
\\[[96](https://arxiv.org/html/2402.01874v1#bib.bib96),
[9](https://arxiv.org/html/2402.01874v1#bib.bib9)\\] (i.e., the use of
confidential data, such as Social Security Numbers, for training the model,
which can then be inferred by an adversary), generated contact information
(such as phone numbers, home addresses, and e-mail addresses)
\\[[96](https://arxiv.org/html/2402.01874v1#bib.bib96)\\], distributional bias
- i.e., text that is negative more often for specific groups
\\[[96](https://arxiv.org/html/2402.01874v1#bib.bib96),
[8](https://arxiv.org/html/2402.01874v1#bib.bib8)\\], engagement to sensitive
questions \\[[8](https://arxiv.org/html/2402.01874v1#bib.bib8),
[9](https://arxiv.org/html/2402.01874v1#bib.bib9)\\]. Notably, all studies
which emphasize LLM harmlessness fall under the ‚Äúfine-tuning‚Äù umbrella, either
with or without human feedback, a fact that indicates that careful prompt
design is not always sufficient to guarantee the adherence of the output to
responsible AI principles. In fact, the variety of ways in which harmful
content can be generated can only be covered through extensive examples, in
which few-shot learning is not enough. Naturally, the construction of fine-
tuning datasets ‚Äì which embed the preference and ethical standards of humans ‚Äì
and the subsequent tuning of the model parameters through Reinforcement
Learning is a natural choice. Helpfulness, i.e., the alignment between the
goals and preferences of the user and the LLM output is another aspect of
output quality: For example, an LLM assistant that is tasked to generate
Python code is expected to produce clean, executable, and correct results.
#### 7.1.2 LLM4RL: Efficiency, Grounding, and Human Preferences Studies in
this class depart from the field of Natural Language Processing and extend to
applications where the use of a language model would seem irrelevant in the
pre-LLM era. A detailed review of the studies presented in section
[4](https://arxiv.org/html/2402.01874v1#S4 "4 RL4LLM: Using Reinforcement
Learning to Enhance Large Language Models ‚Ä£ The RL/LLM Taxonomy Tree:
Reviewing Synergies Between Reinforcement Learning and Large Language Models")
reveals that LLMs possess three particular features which make the
collaboration between them and RL agents successful: 1. 1.Ability for zero-
shot or few-shot learning: the ability of LLMs to learn through no examples or
few examples of desired behavior allows to align their output to human
feedback. This alignment is primarily utilized for RL agent reward design
([5.1](https://arxiv.org/html/2402.01874v1#S5.SS1 "5.1 LLM4RL-Reward ‚Ä£ 5
LLM4RL: Enhancing Reinforcement Learning Agents through Large Language Models
‚Ä£ The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning
and Large Language Models")) with the goal to generate appropriate reward
signals that successfully represent human preferences. 2. 2.Real-world
knowledge: LLMs possess vast ‚Äúknowledge‚Äù of the real world, which allows them
to explore new behaviors and generate training data. Both capabilities result
in time and cost-efficient RL training by a) helping the agent avoid expensive
exploration, particularly in open-ended environments, b) eliminating the need
for expensive data collection and c) reducing the need for from-scratch
training, since they allow policies to be transferred between agents. 3.
3.Reasoning capabilities: for applications involving robotic manipulation, the
robotic agent is the one possessing real-world knowledge about its
environment, while the LLM is used for grounding the actions of the agent to
the environment by ensuring those actions achieve the desired task. #### 7.1.3
RL+LLM: Planning The goal of all studies in the RL+LLM category is successful
planning and execution of relatively complex tasks. In all cases, the agent is
equipped with a set of skills that they have already learned through RL. The
LLM then helps the agent combine those tasks in order to execute longer-
horizon, complex tasks that generally require the execution of more than one
of those simple skills, in the correct order. Without the LLM, the agent would
have to learn the long-term skills from scratch. However, as we discussed in
section [6](https://arxiv.org/html/2402.01874v1#S6 "6 RL+LLM: Combining
Independently Trained Models for Planning ‚Ä£ The RL/LLM Taxonomy Tree:
Reviewing Synergies Between Reinforcement Learning and Large Language
Models"), training for long-term tasks, especially in complex and partially
observable environments, can be data intensive, suffer from sample
inefficiency, and eventually unsuccessful. Therefore, instead of explicitly
learning complex tasks, planning determines the appropriate sequence of basic
skills that have to be executed. LLMs are suitable for planners because they
can reason about possible skills execution sequences based on their knowledge
of the real world. ### 7.2 Shortcomings #### 7.2.1 LLM4RL While the available
results of the LLM4RL and RL+LLM synergy are impressive and more than
promising with regards to the potential for future development, we can
identify a set of shortcomings of this class of problems, which refer to two
key metrics: the applicability of each framework, and the scalability of the
process. ##### Applicability. Despite the wide applicability of RL agents in
domains like (see subsection [2.3](https://arxiv.org/html/2402.01874v1#S2.SS3
"2.3 Scope of This Study ‚Ä£ 2 Background, State-of-Art, Scope, and
Contributions ‚Ä£ The RL/LLM Taxonomy Tree: Reviewing Synergies Between
Reinforcement Learning and Large Language Models") for more details), our
review of the work on the LLM4RL and RL+LLM classes reveals that practically
all applications of the relevant studies are limited to either benchmarking
environments, games, or robotic environments (Tables
[5](https://arxiv.org/html/2402.01874v1#S5.T5 "Table 5 ‚Ä£ 5.3.5 Using an
adapter model ‚Ä£ 5.3 LLM4RL-Policy ‚Ä£ 5 LLM4RL: Enhancing Reinforcement Learning
Agents through Large Language Models ‚Ä£ The RL/LLM Taxonomy Tree: Reviewing
Synergies Between Reinforcement Learning and Large Language Models") and
[7](https://arxiv.org/html/2402.01874v1#S6.T7 "Table 7 ‚Ä£ 6.2 RL+LLM-With
Language Feedback ‚Ä£ 6 RL+LLM: Combining Independently Trained Models for
Planning ‚Ä£ The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement
Learning and Large Language Models")), a trend that might initially raise
questions about the applicability of the synergy to real-world scenarios
beyond household tasks or games. We can attribute this apparent limitation to
three reasons: First, the majority of studies presented in sections
[4](https://arxiv.org/html/2402.01874v1#S4 "4 RL4LLM: Using Reinforcement
Learning to Enhance Large Language Models ‚Ä£ The RL/LLM Taxonomy Tree:
Reviewing Synergies Between Reinforcement Learning and Large Language Models")
and [6](https://arxiv.org/html/2402.01874v1#S6 "6 RL+LLM: Combining
Independently Trained Models for Planning ‚Ä£ The RL/LLM Taxonomy Tree:
Reviewing Synergies Between Reinforcement Learning and Large Language Models")
focus on introducing novel concepts involving the use of LLMs for tasks that
have traditionally been performed otherwise. As proofs-of-concept, they are
therefore well-suited to benchmarking environments, like Atari games. Second,
before a combined modeling framework is deployed in the real-world, its
behavior must be extensively tested for safety, security, and responsible AI
considerations. The amplitude of research on Responsible AI on LLMs, both in
the RL4LLM domain and in aside from that, serves as a proof that these
considerations are taken seriously by the scientific community. Therefore, it
will likely not be long until the LLM4RL classes encompass practical real-
world applications of control systems in areas like healthcare and finance.
Third, the key strength of LLMs that enables this synergy, i.e., the ability
to convey human sense and preferences restricts, at the same time, the range
of applications that can be accommodated by LLM4RL frameworks. This limitation
applies to both the specification of goals or desired behavior through human
language, as well as on the representation of the state using natural language
\\[[41](https://arxiv.org/html/2402.01874v1#bib.bib41)\\]. Even within the
realm of the commonly used benchmarking environments, the applicability of
LLM4RL methods is often constrained by the limitations of the frameworks
themselves. For example, some frameworks, such as the GLAM method by
\\[[22](https://arxiv.org/html/2402.01874v1#bib.bib22)\\] are exclusively
limited to textual environments, while the ELLM method by
\\[[41](https://arxiv.org/html/2402.01874v1#bib.bib41)\\] assumes a natural
language textual representation of the agent‚Äôs state. Other methods (e.g.,
TEXT2REWARD by \\[[138](https://arxiv.org/html/2402.01874v1#bib.bib138)\\] are
capable of handling relatively simple tasks, but are yet to be tested for more
complex tasks. ##### Performance. Aside from applicability, performance is
another parameter that requires further evaluation in LLM4RL studies, with the
specific requirements varying among studies. For example,
\\[[56](https://arxiv.org/html/2402.01874v1#bib.bib56)\\] identify the need to
fine-tune InstructRL to improve its performance at test-time.In other cases,
the performance of the underlying LLM is shown to be sensitive to prompt
choice or even prone to errors despite well-formulated prompts (e.g.,
\\[[41](https://arxiv.org/html/2402.01874v1#bib.bib41)\\]. Certain language
model shortcomings had already been identified prior to the rapid expansion of
LLMs - for example, the policy transfer framework of
\\[[62](https://arxiv.org/html/2402.01874v1#bib.bib62)\\] was shown to
occasionally suffer from ‚Äúcatastrophic forgetting‚Äù, which significantly
reduced the benefits of the agent policy initialization. ##### Scalability.
Finally, the scalability of the solution as the state and action space of the
RL agents grows is a potential challenge. As pointed by
\\[[22](https://arxiv.org/html/2402.01874v1#bib.bib22)\\], scaling up can be
computationally inefficient and therefore constrain the application to a
single environment and relatively small LLMs. #### 7.2.2 RL+LLM The
combination of RL and LLM for planning long and complex tasks is showing
promising results in both studies included in the RL+LLM class. However, an
inevitable outcome of such a synergy is that the final model can eventually
only be as good as the individual skills for which the agent has been already
trained for, irrespective of the robustness of the skill plan generated by the
LLM. As pointed in the SayCan paper
\\[[3](https://arxiv.org/html/2402.01874v1#bib.bib3)\\], there is a chance
that the system cannot react in case the execution of the intermediate skills
fails. Similarly, low success rate of specific individual skills (‚ÄúFind-
Skills‚Äù) are the key limitations highlighted by
\\[[142](https://arxiv.org/html/2402.01874v1#bib.bib142)\\], therefore
hindering the end-to-end execution of the plan generated by the Plan4MC
method. ### 7.3 Alternatives Having reviewed the ways that RL and LLM
collaborate, along with the strengths and weaknesses of each framework, we are
now exploring the existence of LLM-based approaches designed to achieve the
same goals without involving RL agents. We investigate the following
questions: 1. 1.Is RL required for fine-tuning an LLM? 2. 2.Is RL required for
prompt optimization? 3. 3.Is RL required for an LLM to achieve a non-NLP-
related task? Interestingly, the answer to all the above questions is ‚Äúno‚Äù. In
the discussion that follows, we offer a brief review of state-of-art
frameworks that serve as counterexamples. These frameworks are out of the
scope of this taxonomy, since they do not rely on the synergy between an RL
agent and a LLM - aside, of course, from the use of RLHF for the initial
training of the LLM. #### 7.3.1 Fine-tuning an LLM without RL: Syndicom, Rain,
LIMA In section [4.1](https://arxiv.org/html/2402.01874v1#S4.SS1 "4.1 RL4LLM-
Fine-Tuning ‚Ä£ 4 RL4LLM: Using Reinforcement Learning to Enhance Large Language
Models ‚Ä£ The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement
Learning and Large Language Models"),we presented how RL is used to fine-tune
a trained LLM to improve its output for specific tasks. In this section, we
present recent studies that achieve fine-tuning without using RL ‚Äì instead,
they use either supervised training methods
\\[[149](https://arxiv.org/html/2402.01874v1#bib.bib149),
[107](https://arxiv.org/html/2402.01874v1#bib.bib107)\\] or self-evaluation
\\[[71](https://arxiv.org/html/2402.01874v1#bib.bib71)\\] using specially
crafted datasets. The LIMA (Less Is More for Alignment) model
\\[[149](https://arxiv.org/html/2402.01874v1#bib.bib149)\\] was fine-tuned
using supervised learning. The authors analyzed their Superficial Alignment
Hypothesis, according to which ‚Äúa model‚Äôs knowledge and capabilities are
learnt almost entirely during pretraining, while alignment teaches it which
subdistribution of formats should be used when interacting with users‚Äù. LIMA
creators fine-tune a LLaMa language model with 65 billion parameters using
standard supervised loss and a small dataset of 1000 prompt-response pairs.
The responses of LIMA outperform
GPT-4\\[[87](https://arxiv.org/html/2402.01874v1#bib.bib87)\\], Bard, and
DaVince003 models, based on human evaluation, and demonstrate ability to
handle complex queries and generalize well to previously unseen tasks. In the
SYNDICOM framework by
\\[[107](https://arxiv.org/html/2402.01874v1#bib.bib107)\\], the creators
fine-tuned a conversational agent to enhance its commonsense reasoning.
SYNDICOM consists of two components: First, a dataset containing valid and
invalid responses in dialogue contexts, with the invalid ones accompanied by
natural language feedback. The authors build a template by randomly sampling
from ATOMIC and use GPT-3 to convert the template to natural-sounding dialogue
and mark the invalid responses, while human feedback is provided by crowd
workers. The second key component of SYNDICOM is a training procedure of a
feedback model and a response generation model: First, the feedback model is
trained to predict the natural language feedback for invalid responses. Then,
the response generation model is trained based on the invalid response, the
predicted feedback, and the dialogue. The quality of SYNDICOM responses was
shown to outperform ChatGPT based on both ROUGE-1 score and human evaluation.
In a different study,
\\[[71](https://arxiv.org/html/2402.01874v1#bib.bib71)\\] proposed the RAIN
(Rewindable Auto-regressive Inference) method to produce LLM responses aligned
to human intent by fine-tuning through self-evaluation and rewind mechanisms.
RAIN is a self-alignment model, i.e., does not receive external supervision,
and rather allows LLMs to evaluate their output and use the evaluation results
to improve it. In a nutshell, RAIN searchers over token sets, with each token
set mapping to the node of a search tree. The search consists of an inner and
an outer loop. The inner loop, which updates token attributes, consists of a
forward search step from root to leaf through heuristic simulation and a
backward rewind step to the root. The outer loop adjusts the token set
probabilities and determines the next token set. RAIN was shown to outperform
LLaMA30B in terms of harmlessness and perform equally well in terms of
helpfulness and was also shown to outperform LLaMA-2-chat 13B in terms of
truthfulness. #### 7.3.2 Prompt Optimization Without RL: Learning-Based Prompt
Optimization In subsection [4.2](https://arxiv.org/html/2402.01874v1#S4.SS2
"4.2 RL4LLM-Prompt ‚Ä£ 4 RL4LLM: Using Reinforcement Learning to Enhance Large
Language Models ‚Ä£ The RL/LLM Taxonomy Tree: Reviewing Synergies Between
Reinforcement Learning and Large Language Models"), we reviewed studies where
RL is used for LLM prompt engineering. Nonetheless, RL is not the sole method
for conducting prompt engineering:
\\[[115](https://arxiv.org/html/2402.01874v1#bib.bib115)\\] summarized state-
of-art methods on learning-based prompt optimization, with examples where
prompt optimization is achieved through methods like Beam Search
\\[[99](https://arxiv.org/html/2402.01874v1#bib.bib99)\\] or Evolution
Strategy \\[[150](https://arxiv.org/html/2402.01874v1#bib.bib150)\\]. However,
every single of the RL4LLM-Prompt frameworks presetned in this study was able
to overcome traditional challenges that were primarily related to training
efficiency of supervised learning methods. RLPrompt
\\[[36](https://arxiv.org/html/2402.01874v1#bib.bib36)\\] combined multiple
desirable properties which previously had not been present collectively
present in any framework: it is automated, gradient-free (therefore
eliminating the need to access or compute gradients, which can be
computationally expensive), uses frozen LMs (thus not updating any LM
parameters), efficient (since it guides optimization through the RL reward
information), transferable between different langauge models (due to the use
of discrete prompts rather than embeddings), and capable of few-shot and zero-
shot learning (since the reward function eliminates the necessity for
supervised data). TEMPERA
\\[[145](https://arxiv.org/html/2402.01874v1#bib.bib145)\\] outperformed
RLPrompt in multiple tasks like fine-tuning, prompt tuning and discrete prompt
search. Finally, Prompt-OIRL was the first model to address the challenged of
inference time evaluation (through offline prompt evaluation) and expensive
online prompt optimization (through offline prompt optimization without access
to the target LLM). #### 7.3.3 LLMs for non-NLP tasks As established in
section [5](https://arxiv.org/html/2402.01874v1#S5 "5 LLM4RL: Enhancing
Reinforcement Learning Agents through Large Language Models ‚Ä£ The RL/LLM
Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning and Large
Language Models"), integrating a Large Language Model in a RL framework allows
us to utilize the vast knowledge and grounding capabilities of LLMs and
achieve a variety of control tasks that are not inherently related to natural
language, ranging from playing games to robotic manipulation. We also reviewed
studies where the output of LLMs together with learned robotic policies can be
used for planning or sequential decision-making tasks in the LLM+RL category.
Particularly in the realm of robotics, we showed
([5](https://arxiv.org/html/2402.01874v1#S5 "5 LLM4RL: Enhancing Reinforcement
Learning Agents through Large Language Models ‚Ä£ The RL/LLM Taxonomy Tree:
Reviewing Synergies Between Reinforcement Learning and Large Language Models")
and [6](https://arxiv.org/html/2402.01874v1#S6 "6 RL+LLM: Combining
Independently Trained Models for Planning ‚Ä£ The RL/LLM Taxonomy Tree:
Reviewing Synergies Between Reinforcement Learning and Large Language
Models")) that grounding the agent to the natural environment is a key
challenge that LLMs have successfully addressed. KOSMOS
\\[[59](https://arxiv.org/html/2402.01874v1#bib.bib59)\\] is a multimodal
Large Language Model that has been trained on web-scale multimodal corpora,
including text, image-caption pairs and documents with both images and text.
The goal of KOSMOS is to align perception with LLMs, practically allowing
models to see and talk. The key idea behind KOSMOS is directly analogous to
that of Large Language Models, since it is trained to predict the most likely
next token. However, it extends this principle beyond language, showing
successful performance on vision tasks as well. More specifically, the model
is capable of successfully executing dialogue tasks, visual explanation and
Question-Answering, number recognition, and image captioning. Similarly,
PaLM-E \\[[38](https://arxiv.org/html/2402.01874v1#bib.bib38)\\] is a general-
purpose multimodal language model for embodied reasoning, visual-language, and
language tasks. Inputs such as images and neural 3D representations are
embedded alongside text tokens and passed as input to the Transformer.
Incorporating continuous inputs from various sensor modalities of the embodied
agent can enable the multimodal language model itself to make grounded
inferences for sequential decision making in the real world. PaLM-E transfers
knowledge from visual-language domains into embodied reasoning, such as
sequential robotic planning and answering questions about the observable
world. This knowledge transfer leads to high data efficiency for robotics
tasks. PaLM-E operates on multimodal sequences of tokens with inputs such as
images and neural 3D representations alongside text tokens. The authors
demonstrate that a generalist multi-embodiment agent can be trained leveraging
transfer learning across modalities, by incorporating embodied data into the
training of a multimodal LLM. Like KOSMOS-1, PaLM-E can perform tasks such as
zero shot multimodal chain of thought, visually-conditioned jokes, zero-shot
multi-image relationships, spatial grounding, robot visual perception,
dialogue and planning etc. GPT-4V by OpenAI
\\[[1](https://arxiv.org/html/2402.01874v1#bib.bib1)\\] is a multimodal LLM
that has been trained to analyze and understand text and image input and
generate text outputs demonstrates impressive performance on various tasks,
such as exams, logic puzzles, as well as vision and language tasks. GPT-4V was
trained on a large-scale corpus of web data, including both positive and
negative examples (right and wrong solutions to problems, weak and strong
reasoning, self-contradictory and consistent statements) and of various
ideologies and ideas. Note that the model‚Äôs capabilities seem to come
primarily from the pre-training process. It is interesting to note that
multimodality is not necessary for an LLM to succesfully execute non-NLP
tasks. A typical example is the SPRING framework
\\[[136](https://arxiv.org/html/2402.01874v1#bib.bib136)\\], where an LLM
learns to play complex, open-world games like Crafter or Minecraft by reading
the Latex source code of the related academic papers. A directed acyclic graph
is constructed, with gameplay specific questions as nodes and dependencies
between questions as edges. The experiments demonstrated that the LLM show
that when using chain-of-thought prompting, LLMs can successfully execute
complex tasks, while SPRING‚Äôs zero-shot performance exceeded that of state-of-
art RL algorithms for 1 million training steps. 8 Conclusions and Future Work
----------------------------- In this work, we have proposed the RL/LLM
Taxonomy Tree, a comprehensive classification of state-of-art computational
frameworks that combine Reinforcement Learning Agents and Large Language
Models to achieve a target task. We have therefore identified three core
classes, namely RL4LLM, which use RL to improve the performance of an LLM;
LLM4RL, where an LLM assists the training of an RL agent; and RL+LLM, where an
RL agent and an LLM participate in a common framework for planning downstream
tasks. We have further divided each class into subcategories based on
observations that differentiate the studies that belong in each one. Since
each category corresponds to a distinct type of synergy between RL and LLMs,
we have explored the key motivations behind the development of the frameworks
included in each category and have explained which key strengths of RL and
LLMs are utilized each time. The adaptability of RL to NLP tasks thanks to
their sequential decision-making nature, as well as the reasoning capabilities
and vast knowledge about the real world that LLMs possess serve as testaments
for the success of the synergy, resulting in models that are aligned with
human intent and Responsible AI principles. In addition, by reviewing the
prolific literature on alternative methods, we acknowledge that, for most
applications, this synergy is not the only choice. Finally, since LLMs are a
relatively new area of Artificial Intelligence, there still exist potential
shortcomings; those primarily concern the applicability of LLM4RL and RL+LLM
frameworks, along with aspects like computational efficiency and scalability.
Nevertheless, the pace of research is so rapid that we can only anticipate
substantial improvements in those areas as well. This review is intended to
help researchers understand RL-LLM synergies and develop their own AI
frameworks. In the future, we will keep classifying new studies based on the
RL/LLM Taxonomy Tree and, if appropriate, expand it to capture novel
categories that, given the pace of ongoing research, will almost certainly
arise. Undoubtedly, the future holds boundless possibilities for RL-LLM
synergies in this regard. References ---------- * gpt \\[2023\\] Gpt-4
technical report. Technical report, OpenAI, 2023. URL
[https://openai.com/contributions/gpt-4v](https://openai.com/contributions/gpt-4v).
* Abbeel and Ng \\[2004\\] P. Abbeel and A. Y. Ng. Apprenticeship learning via
inverse reinforcement learning. In _Proceedings of the Twenty-First
International Conference on Machine Learning_, ICML ‚Äô04, page 1, New York, NY,
USA, 2004. Association for Computing Machinery. ISBN 1581138385. doi:
[10.1145/1015330.1015430](https://arxiv.org/html/2402.01874v1/10.1145/1015330.1015430).
URL
[https://doi.org/10.1145/1015330.1015430](https://doi.org/10.1145/1015330.1015430).
* Ahn et al. \\[2022\\] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes,
B. David, C. Finn, C. Fu, K. Gopalakrishnan, K. Hausman, A. Herzog, D. Ho, J.
Hsu, J. Ibarz, B. Ichter, A. Irpan, E. Jang, R. J. Ruano, K. Jeffrey, S.
Jesmonth, N. J. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, K.-H. Lee, S.
Levine, Y. Lu, L. Luu, C. Parada, P. Pastor, J. Quiambao, K. Rao, J.
Rettinghouse, D. Reyes, P. Sermanet, N. Sievers, C. Tan, A. Toshev, V.
Vanhoucke, F. Xia, T. Xiao, P. Xu, S. Xu, M. Yan, and A. Zeng. Do as i can,
not as i say: Grounding language in robotic affordances, 2022. * AnyRobotics
\\[2023\\] AnyRobotics. Anymal, 2023. URL
[https://www.anybotics.com/robotics/anymal/](https://www.anybotics.com/robotics/anymal/).
* Arulkumaran et al. \\[2017\\] K. Arulkumaran, M. P. Deisenroth, M. Brundage,
and A. A. Bharath. A brief survey of deep reinforcement learning. _arXiv
preprint arXiv:1708.05866_, 2017. * Aubret et al. \\[2019\\] A. Aubret, L.
Matignon, and S. Hassas. A survey on intrinsic motivation in reinforcement
learning, 2019. * Bai et al. \\[2023\\] H. Bai, R. Cheng, and Y. Jin.
Evolutionary reinforcement learning: A survey. _Intelligent Computing_,
2:0025, 2023. doi:
[10.34133/icomputing.0025](https://arxiv.org/html/2402.01874v1/10.34133/icomputing.0025).
URL
[https://spj.science.org/doi/abs/10.34133/icomputing.0025](https://spj.science.org/doi/abs/10.34133/icomputing.0025).
* Bai et al. \\[2022a\\] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N.
DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan, N. Joseph, S. Kadavath,
J. Kernion, T. Conerly, S. El-Showk, N. Elhage, Z. Hatfield-Dodds, D.
Hernandez, T. Hume, S. Johnston, S. Kravec, L. Lovitt, N. Nanda, C. Olsson, D.
Amodei, T. Brown, J. Clark, S. McCandlish, C. Olah, B. Mann, and J. Kaplan.
Training a helpful and harmless assistant with reinforcement learning from
human feedback, 2022a. * Bai et al. \\[2022b\\] Y. Bai, S. Kadavath, S. Kundu,
A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C.
McKinnon, C. Chen, C. Olsson, C. Olah, D. Hernandez, D. Drain, D. Ganguli, D.
Li, E. Tran-Johnson, E. Perez, J. Kerr, J. Mueller, J. Ladish, J. Landau, K.
Ndousse, K. Lukosuite, L. Lovitt, M. Sellitto, N. Elhage, N. Schiefer, N.
Mercado, N. DasSarma, R. Lasenby, R. Larson, S. Ringer, S. Johnston, S.
Kravec, S. E. Showk, S. Fort, T. Lanham, T. Telleen-Lawton, T. Conerly, T.
Henighan, T. Hume, S. R. Bowman, Z. Hatfield-Dodds, B. Mann, D. Amodei, N.
Joseph, S. McCandlish, T. Brown, and J. Kaplan. Constitutional ai:
Harmlessness from ai feedback, 2022b. * Bard et al. \\[2020\\] N. Bard, J. N.
Foerster, S. Chandar, N. Burch, M. Lanctot, H. F. Song, E. Parisotto, V.
Dumoulin, S. Moitra, E. Hughes, I. Dunning, S. Mourad, H. Larochelle, M. G.
Bellemare, and M. Bowling. The hanabi challenge: A new frontier for ai
research. _Artificial Intelligence_, 280:103216, 2020. ISSN 0004-3702. doi:
[https://doi.org/10.1016/j.artint.2019.103216](https://doi.org/10.1016/j.artint.2019.103216).
URL
[https://www.sciencedirect.com/science/article/pii/S0004370219300116](https://www.sciencedirect.com/science/article/pii/S0004370219300116).
* Beck et al. \\[2023\\] J. Beck, R. Vuorio, E. Zheran Liu, Z. Xiong, L.
Zintgraf, C. Finn, and S. Whiteson. A Survey of Meta-Reinforcement Learning.
_arXiv e-prints_, art. arXiv:2301.08028, Jan. 2023. doi:
[10.48550/arXiv.2301.08028](https://arxiv.org/html/2402.01874v1/10.48550/arXiv.2301.08028).
* Bellman \\[1957\\] R. Bellman. A markovian decision process. _Indiana Univ.
Math. J._, 6:679‚Äì684, 1957. ISSN 0022-2518. * Bengio et al. \\[2009\\] Y.
Bengio, J. Louradour, R. Collobert, and J. Weston. Curriculum learning. In
_Proceedings of the 26th Annual International Conference on Machine Learning_,
ICML ‚Äô09, page 41‚Äì48, New York, NY, USA, 2009. Association for Computing
Machinery. ISBN 9781605585161. doi:
[10.1145/1553374.1553380](https://arxiv.org/html/2402.01874v1/10.1145/1553374.1553380).
URL
[https://doi.org/10.1145/1553374.1553380](https://doi.org/10.1145/1553374.1553380).
* BitCraze \\[2023\\] BitCraze. Crazyflie, 2023. URL
[https://www.bitcraze.io/products/crazyflie-2-1/](https://www.bitcraze.io/products/crazyflie-2-1/).
* Brockman et al. \\[2016\\] G. Brockman, V. Cheung, L. Pettersson, J.
Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym, 2016. * Brown et
al. \\[2020a\\] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P.
Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A.
Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J.
Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J.
Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei.
Language models are few-shot learners, 2020a. * Brown et al. \\[2020b\\] T. B.
Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan,
P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T.
Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M.
Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S.
McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-
shot learners, 2020b. * Cambria and White \\[2014\\] E. Cambria and B. White.
Jumping nlp curves: A review of natural language processing research \\[review
article\\]. _IEEE Computational Intelligence Magazine_, 9(2):48‚Äì57, 2014. doi:
[10.1109/MCI.2014.2307227](https://arxiv.org/html/2402.01874v1/10.1109/MCI.2014.2307227).
* Cao et al. \\[2023a\\] B. Cao, H. Lin, X. Han, and L. Sun. The life cycle of
knowledge in big language models: A survey, 2023a. * Cao et al. \\[2023b\\] Y.
Cao, L. Yao, J. McAuley, and Q. Z. Sheng. Reinforcement learning for
generative ai: A survey. _arXiv preprint arXiv:2308.14328_, 2023b. * Cao et
al. \\[2023c\\] Z. Cao, R. A. Ramachandra, and K. Yu. Temporal video-language
alignment network for reward shaping in reinforcement learning, 2023c. * Carta
et al. \\[2023\\] T. Carta, C. Romac, T. Wolf, S. Lamprier, O. Sigaud, and
P.-Y. Oudeyer. Grounding large language models in interactive environments
with online reinforcement learning, 2023. * Chang et al. \\[2023\\] Y. Chang,
X. Wang, J. Wang, Y. Wu, K. Zhu, H. Chen, L. Yang, X. Yi, C. Wang, Y. Wang, et
al. A survey on evaluation of large language models. _arXiv preprint
arXiv:2307.03109_, 2023. * Chen et al. \\[2021a\\] L. Chen, K. Lu, A.
Rajeswaran, K. Lee, A. Grover, M. Laskin, P. Abbeel, A. Srinivas, and I.
Mordatch. Decision transformer: Reinforcement learning via sequence modeling,
2021a. * Chen et al. \\[2020\\] M. Chen, A. Radford, R. Child, J. Wu, H. Jun,
D. Luan, and I. Sutskever. Generative pretraining from pixels. In _Proceedings
of the 37th International Conference on Machine Learning_, ICML‚Äô20. JMLR.org,
2020. * Chen et al. \\[2021b\\] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d.
O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al.
Evaluating large language models trained on code. _arXiv preprint
arXiv:2107.03374_, 2021b. * Chen et al. \\[2019\\] T. Chen, A. Murali, and A.
Gupta. Hardware conditioned policies for multi-robot transfer learning, 2019.
* Chentanez et al. \\[2004\\] N. Chentanez, A. Barto, and S. Singh.
Intrinsically motivated reinforcement learning. In L. Saul, Y. Weiss, and L.
Bottou, editors, _Advances in Neural Information Processing Systems_, volume
17\\. MIT Press, 2004. URL
[https://proceedings.neurips.cc/paper\\_files/paper/2004/file/4be5a36cbaca8ab9d2066debfe4e65c1-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2004/file/4be5a36cbaca8ab9d2066debfe4e65c1-Paper.pdf).
* Chevalier-Boisvert et al. \\[2019\\] M. Chevalier-Boisvert, D. Bahdanau, S.
Lahlou, L. Willems, C. Saharia, T. H. Nguyen, and Y. Bengio. Babyai: A
platform to study the sample efficiency of grounded language learning, 2019. *
Choi et al. \\[2022\\] K. Choi, C. Cundy, S. Srivastava, and S. Ermon.
Lmpriors: Pre-trained language models as task-specific priors, 2022. *
Chowdhary and Chowdhary \\[2020\\] K. Chowdhary and K. Chowdhary. Natural
language processing. _Fundamentals of artificial intelligence_, pages 603‚Äì649,
2020. * Chowdhery et al. \\[2022\\] A. Chowdhery, S. Narang, J. Devlin, M.
Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann,
P. Schuh, K. Shi, S. Tsvyashchenko, J. Maynez, A. Rao, P. Barnes, Y. Tay, N.
Shazeer, V. Prabhakaran, E. Reif, N. Du, B. Hutchinson, R. Pope, J. Bradbury,
J. Austin, M. Isard, G. Gur-Ari, P. Yin, T. Duke, A. Levskaya, S. Ghemawat, S.
Dev, H. Michalewski, X. Garcia, V. Misra, K. Robinson, L. Fedus, D. Zhou, D.
Ippolito, D. Luan, H. Lim, B. Zoph, A. Spiridonov, R. Sepassi, D. Dohan, S.
Agrawal, M. Omernick, A. M. Dai, T. S. Pillai, M. Pellat, A. Lewkowycz, E.
Moreira, R. Child, O. Polozov, K. Lee, Z. Zhou, X. Wang, B. Saeta, M. Diaz, O.
Firat, M. Catasta, J. Wei, K. Meier-Hellstern, D. Eck, J. Dean, S. Petrov, and
N. Fiedel. Palm: Scaling language modeling with pathways, 2022. * Cobbe et al.
\\[2021\\] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M.
Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training
verifiers to solve math word problems, 2021. * Dasgupta et al. \\[2023\\] I.
Dasgupta, C. Kaeser-Chen, K. Marino, A. Ahuja, S. Babayan, F. Hill, and R.
Fergus. Collaborating with language models for embodied reasoning, 2023. *
\\[35\\] G. DeepMind. pycolab. URL [https://github.com/google-
deepmind/pycolab](https://github.com/google-deepmind/pycolab). * Deng et al.
\\[2022\\] M. Deng, J. Wang, C.-P. Hsieh, Y. Wang, H. Guo, T. Shu, M. Song, E.
P. Xing, and Z. Hu. Rlprompt: Optimizing discrete text prompts with
reinforcement learning, 2022. * Devlin et al. \\[2019\\] J. Devlin, M.-W.
Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding, 2019. * Driess et al. \\[2023\\] D.
Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J.
Tompson, Q. Vuong, T. Yu, et al. Palm-e: An embodied multimodal language
model. _arXiv preprint arXiv:2303.03378_, 2023. * Du et al. \\[2023a\\] M. Du,
F. He, N. Zou, D. Tao, and X. Hu. Shortcut learning of large language models
in natural language understanding, 2023a. * Du and Ding \\[2021\\] W. Du and
S. Ding. A survey on multi-agent deep reinforcement learning: from the
perspective of challenges and applications. _Artificial Intelligence Review_,
54:3215‚Äì3238, 2021. * Du et al. \\[2023b\\] Y. Du, O. Watkins, Z. Wang, C.
Colas, T. Darrell, P. Abbeel, A. Gupta, and J. Andreas. Guiding pretraining in
reinforcement learning with large language models, 2023b. * Eschmann
\\[2021\\] J. Eschmann. _Reward Function Design in Reinforcement Learning_,
pages 25‚Äì33. Springer International Publishing, Cham, 2021. ISBN
978-3-030-41188-6. doi:
[10.1007/978-3-030-41188-6\\_3](https://arxiv.org/html/2402.01874v1/10.1007/978-3-030-41188-6_3).
URL
[https://doi.org/10.1007/978-3-030-41188-6\\_3](https://doi.org/10.1007/978-3-030-41188-6_3).
* Fan et al. \\[2023\\] A. Fan, B. Gokkaya, M. Harman, M. Lyubarskiy, S.
Sengupta, S. Yoo, and J. M. Zhang. Large language models for software
engineering: Survey and open problems, 2023. * Fran√ßois-Lavet et al.
\\[2018\\] V. Fran√ßois-Lavet, P. Henderson, R. Islam, M. G. Bellemare, J.
Pineau, et al. An introduction to deep reinforcement learning. _Foundations
and Trends¬Æ in Machine Learning_, 11(3-4):219‚Äì354, 2018. * Fujimoto and Gu
\\[2021\\] S. Fujimoto and S. S. Gu. A minimalist approach to offline
reinforcement learning, 2021. * Furuta et al. \\[2022\\] H. Furuta, Y. Matsuo,
and S. S. Gu. Generalized decision transformer for offline hindsight
information matching, 2022. * Gallegos et al. \\[2023\\] I. O. Gallegos, R. A.
Rossi, J. Barrow, M. M. Tanjim, S. Kim, F. Dernoncourt, T. Yu, R. Zhang, and
N. K. Ahmed. Bias and fairness in large language models: A survey, 2023. *
Garaffa et al. \\[2023\\] L. C. Garaffa, M. Basso, A. A. Konzen, and E. P. de
Freitas. Reinforcement learning for mobile robotics exploration: A survey.
_IEEE Transactions on Neural Networks and Learning Systems_, 34(8):3796‚Äì3810,
2023. doi:
[10.1109/TNNLS.2021.3124466](https://arxiv.org/html/2402.01874v1/10.1109/TNNLS.2021.3124466).
* Ghalandari et al. \\[2022\\] D. G. Ghalandari, C. Hokamp, and G. Ifrim.
Efficient unsupervised sentence compression by fine-tuning transformers with
reinforcement learning, 2022. * Goyal et al. \\[2019\\] P. Goyal, S. Niekum,
and R. J. Mooney. Using natural language for reward shaping in reinforcement
learning, 2019. * Gronauer and Diepold \\[2022\\] S. Gronauer and K. Diepold.
Multi-agent deep reinforcement learning: a survey. _Artificial Intelligence
Review_, pages 1‚Äì49, 2022. * Gu et al. \\[2023\\] J. Gu, F. Xiang, X. Li, Z.
Ling, X. Liu, T. Mu, Y. Tang, S. Tao, X. Wei, Y. Yao, X. Yuan, P. Xie, Z.
Huang, R. Chen, and H. Su. Maniskill2: A unified benchmark for generalizable
manipulation skills, 2023. * Guo et al. \\[2023\\] Z. Guo, R. Jin, C. Liu, Y.
Huang, D. Shi, L. Yu, Y. Liu, J. Li, B. Xiong, D. Xiong, et al. Evaluating
large language models: A comprehensive survey. _arXiv preprint
arXiv:2310.19736_, 2023. * Haarnoja et al. \\[2018\\] T. Haarnoja, A. Zhou, P.
Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy deep
reinforcement learning with a stochastic actor, 2018. * Haddadin et al.
\\[2022\\] S. Haddadin, S. Parusel, L. Johannsmeier, S. Golz, S. Gabl, F.
Walch, M. Sabaghian, C. J√§hne, L. Hausperger, and S. Haddadin. The franka
emika robot: A reference platform for robotics research and education. _IEEE
Robotics & Automation Magazine_, 29(2):46‚Äì64, 2022. doi:
[10.1109/MRA.2021.3138382](https://arxiv.org/html/2402.01874v1/10.1109/MRA.2021.3138382).
* Hu and Sadigh \\[2023\\] H. Hu and D. Sadigh. Language instructed
reinforcement learning for human-ai coordination, 2023. * Hu et al. \\[2023\\]
J. Hu, L. Tao, J. Yang, and C. Zhou. Aligning language models with offline
reinforcement learning from human feedback. _arXiv preprint arXiv:2308.12050_,
2023. * Huang and Chang \\[2022\\] J. Huang and K. C.-C. Chang. Towards
reasoning in large language models: A survey. _arXiv preprint
arXiv:2212.10403_, 2022. * Huang et al. \\[2023\\] S. Huang, L. Dong, W. Wang,
Y. Hao, S. Singhal, S. Ma, T. Lv, L. Cui, O. K. Mohammed, Q. Liu, et al.
Language is not all you need: Aligning perception with language models. _arXiv
preprint arXiv:2302.14045_, 2023. * Huang et al. \\[2022\\] W. Huang, F. Xia,
T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y.
Chebotar, P. Sermanet, N. Brown, T. Jackson, L. Luu, S. Levine, K. Hausman,
and B. Ichter. Inner monologue: Embodied reasoning through planning with
language models, 2022. * Janner et al. \\[2021\\] M. Janner, Q. Li, and S.
Levine. Reinforcement learning as one big sequence modeling problem. _CoRR_,
abs/2106.02039, 2021. URL
[https://arxiv.org/abs/2106.02039](https://arxiv.org/abs/2106.02039). * Jiang
et al. \\[2023\\] Y. Jiang, Q. Gao, G. Thattai, and G. Sukhatme. Language-
informed transfer learning for embodied household activities, 2023. *
Kaelbling et al. \\[1996\\] L. P. Kaelbling, M. L. Littman, and A. W. Moore.
Reinforcement learning: A survey. _Journal of artificial intelligence
research_, 4:237‚Äì285, 1996. * Kant et al. \\[2022\\] Y. Kant, A. Ramachandran,
S. Yenamandra, I. Gilitschenski, D. Batra, A. Szot, and H. Agrawal. Housekeep:
Tidying virtual households using commonsense reasoning. In S. Avidan, G.
Brostow, M. Ciss√©, G. M. Farinella, and T. Hassner, editors, _Computer Vision
‚Äì ECCV 2022_, pages 355‚Äì373, Cham, 2022. Springer Nature Switzerland. ISBN
978-3-031-19842-7. * Kim and Lee \\[2023\\] J. Kim and B. Lee. Ai-augmented
surveys: Leveraging large language models for opinion prediction in nationally
representative surveys, 2023. * Kumar et al. \\[2020\\] A. Kumar, A. Zhou, G.
Tucker, and S. Levine. Conservative q-learning for offline reinforcement
learning, 2020. * Kwon et al. \\[2023\\] M. Kwon, S. M. Xie, K. Bullard, and
D. Sadigh. Reward design with language models, 2023. * Levine et al.
\\[2020\\] S. Levine, A. Kumar, G. Tucker, and J. Fu. Offline reinforcement
learning: Tutorial, review, and perspectives on open problems. _arXiv preprint
arXiv:2005.01643_, 2020. * Lewis et al. \\[2017\\] M. Lewis, D. Yarats, Y. N.
Dauphin, D. Parikh, and D. Batra. Deal or no deal? end-to-end learning for
negotiation dialogues, 2017. * Li et al. \\[2023a\\] L. Li, Y. Zhang, D. Liu,
and L. Chen. Large language models for generative recommendation: A survey and
visionary discussions, 2023a. * Li et al. \\[2023b\\] Y. Li, F. Wei, J. Zhao,
C. Zhang, and H. Zhang. Rain: Your language models can align themselves
without finetuning. _arXiv preprint arXiv:2309.07124_, 2023b. * Lin et al.
\\[2023\\] J. Lin, X. Dai, Y. Xi, W. Liu, B. Chen, X. Li, C. Zhu, H. Guo, Y.
Yu, R. Tang, and W. Zhang. How can recommender systems benefit from large
language models: A survey, 2023. * Liu et al. \\[2023a\\] X. Liu, H. Yu, H.
Zhang, Y. Xu, X. Lei, H. Lai, Y. Gu, H. Ding, K. Men, K. Yang, et al.
Agentbench: Evaluating llms as agents. _arXiv preprint arXiv:2308.03688_,
2023a. * Liu et al. \\[2023b\\] Y. Liu, T. Han, S. Ma, J. Zhang, Y. Yang, J.
Tian, H. He, A. Li, M. He, Z. Liu, Z. Wu, L. Zhao, D. Zhu, X. Li, N. Qiang, D.
Shen, T. Liu, and B. Ge. Summary of ChatGPT-related research and perspective
towards the future of large language models. _Meta-Radiology_, 1(2):100017,
sep 2023b. doi:
[10.1016/j.metrad.2023.100017](https://arxiv.org/html/2402.01874v1/10.1016/j.metrad.2023.100017).
URL
[https://doi.org/10.1016%2Fj.metrad.2023.100017](https://doi.org/10.1016%2Fj.metrad.2023.100017).
* Ma et al. \\[2023\\] Y. J. Ma, W. Liang, G. Wang, D.-A. Huang, O. Bastani,
D. Jayaraman, Y. Zhu, L. Fan, and A. Anandkumar. Eureka: Human-level reward
design via coding large language models. _arXiv preprint arXiv:2310.12931_,
2023. * Mazyavkina et al. \\[2021\\] N. Mazyavkina, S. Sviridov, S. Ivanov,
and E. Burnaev. Reinforcement learning for combinatorial optimization: A
survey. _Computers & Operations Research_, 134:105400, 2021. ISSN 0305-0548.
doi:
[https://doi.org/10.1016/j.cor.2021.105400](https://doi.org/10.1016/j.cor.2021.105400).
URL
[https://www.sciencedirect.com/science/article/pii/S0305054821001660](https://www.sciencedirect.com/science/article/pii/S0305054821001660).
* Merity et al. \\[2016\\] S. Merity, C. Xiong, J. Bradbury, and R. Socher.
Pointer sentinel mixture models, 2016. * Mialon et al. \\[2023\\] G. Mialon,
R. Dess√¨, M. Lomeli, C. Nalmpantis, R. Pasunuru, R. Raileanu, B. Rozi√®re, T.
Schick, J. Dwivedi-Yu, A. Celikyilmaz, E. Grave, Y. LeCun, and T. Scialom.
Augmented language models: a survey, 2023. * Min et al. \\[2021\\] B. Min, H.
Ross, E. Sulem, A. P. B. Veyseh, T. H. Nguyen, O. Sainz, E. Agirre, I. Heinz,
and D. Roth. Recent advances in natural language processing via large pre-
trained language models: A survey, 2021. * Mnih et al. \\[2013\\] V. Mnih, K.
Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M.
Riedmiller. Playing atari with deep reinforcement learning, 2013. * Mnih et
al. \\[2015\\] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M.
G. Bellemare, A. Graves, M. A. Riedmiller, A. K. Fidjeland, G. Ostrovski, S.
Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D.
Wierstra, S. Legg, and D. Hassabis. Human-level control through deep
reinforcement learning. _Nature_, 518:529‚Äì533, 2015. URL
[https://api.semanticscholar.org/CorpusID:205242740](https://api.semanticscholar.org/CorpusID:205242740).
* Nastase et al. \\[2015\\] V. Nastase, R. Mihalcea, and D. R. Radev. A survey
of graphs in natural language processing. _Natural Language Engineering_,
21(5):665‚Äì698, 2015. * Ni et al. \\[2021\\] J. Ni, G. H. √Åbrego, N. Constant,
J. Ma, K. B. Hall, D. Cer, and Y. Yang. Sentence-t5: Scalable sentence
encoders from pre-trained text-to-text models, 2021. * Nie et al. \\[2023\\]
M. Nie, D. Chen, and D. Wang. Reinforcement learning on graphs: A survey.
_IEEE Transactions on Emerging Topics in Computational Intelligence_, 2023. *
OpenAI \\[2023a\\] OpenAI. Chatgpt, 2023a. URL
[https://chat.openai.com/chat](https://chat.openai.com/chat). * OpenAI
\\[2023b\\] OpenAI. Gpt-3.5, 2023b. URL
[https://platform.openai.com/docs/models/gpt-3-5](https://platform.openai.com/docs/models/gpt-3-5).
* OpenAI \\[2023c\\] OpenAI. Gpt-4 technical report, 2023c. * Oshikawa et al.
\\[2018\\] R. Oshikawa, J. Qian, and W. Y. Wang. A survey on natural language
processing for fake news detection. _arXiv preprint arXiv:1811.00770_, 2018. *
Otter et al. \\[2020\\] D. W. Otter, J. R. Medina, and J. K. Kalita. A survey
of the usages of deep learning for natural language processing. _IEEE
transactions on neural networks and learning systems_, 32(2):604‚Äì624, 2020. *
Ouyang et al. \\[2022\\] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L.
Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman,
J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P.
Christiano, J. Leike, and R. Lowe. Training language models to follow
instructions with human feedback, 2022. * Padakandla \\[2021\\] S. Padakandla.
A survey of reinforcement learning algorithms for dynamically varying
environments. _ACM Comput. Surv._, 54(6), jul 2021. ISSN 0360-0300. doi:
[10.1145/3459991](https://arxiv.org/html/2402.01874v1/10.1145/3459991). URL
[https://doi.org/10.1145/3459991](https://doi.org/10.1145/3459991). * Pan et
al. \\[2023\\] S. Pan, L. Luo, Y. Wang, C. Chen, J. Wang, and X. Wu. Unifying
large language models and knowledge graphs: A roadmap, 2023. * Patel et al.
\\[2021\\] A. Patel, S. Bhattamishra, and N. Goyal. Are nlp models really able
to solve simple math word problems?, 2021. * Pateria et al. \\[2021\\] S.
Pateria, B. Subagdja, A.-h. Tan, and C. Quek. Hierarchical reinforcement
learning: A comprehensive survey. _ACM Comput. Surv._, 54(5), jun 2021. ISSN
0360-0300. doi:
[10.1145/3453160](https://arxiv.org/html/2402.01874v1/10.1145/3453160). URL
[https://doi.org/10.1145/3453160](https://doi.org/10.1145/3453160). * Peng et
al. \\[2019\\] X. B. Peng, A. Kumar, G. Zhang, and S. Levine. Advantage-
weighted regression: Simple and scalable off-policy reinforcement learning,
2019. * Perez et al. \\[2022\\] E. Perez, S. Huang, H. F. Song, T. Cai, R.
Ring, J. Aslanides, A. Glaese, N. McAleese, and G. Irving. Red teaming
language models with language models. _CoRR_, abs/2202.03286, 2022. URL
[https://arxiv.org/abs/2202.03286](https://arxiv.org/abs/2202.03286). * Peters
and Schaal \\[2007\\] J. Peters and S. Schaal. Reinforcement learning by
reward-weighted regression for operational space control. In _Proceedings of
the 24th international conference on Machine learning_, pages 745‚Äì750, 2007. *
Prudencio et al. \\[2023\\] R. F. Prudencio, M. R. Maximo, and E. L.
Colombini. A survey on offline reinforcement learning: Taxonomy, review, and
open problems. _IEEE Transactions on Neural Networks and Learning Systems_,
2023. * Pryzant et al. \\[2023\\] R. Pryzant, D. Iter, J. Li, Y. T. Lee, C.
Zhu, and M. Zeng. Automatic prompt optimization with "gradient descent" and
beam search, 2023. * Qiu et al. \\[2020\\] X. Qiu, T. Sun, Y. Xu, Y. Shao, N.
Dai, and X. Huang. Pre-trained models for natural language processing: A
survey. _Science China Technological Sciences_, 63(10):1872‚Äì1897, 2020. *
Quartey et al. \\[2023\\] B. Quartey, A. Shah, and G. Konidaris. Exploiting
contextual structure to generate useful auxiliary tasks, 2023. * Radford et
al. \\[2019\\] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I.
Sutskever. Language models are unsupervised multitask learners. 2019\\. URL
[https://api.semanticscholar.org/CorpusID:160025533](https://api.semanticscholar.org/CorpusID:160025533).
* Radford et al. \\[2021\\] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G.
Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and
I. Sutskever. Learning transferable visual models from natural language
supervision, 2021. * Rae et al. \\[2022\\] J. W. Rae, S. Borgeaud, T. Cai, K.
Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson, R. Ring, S. Young,
E. Rutherford, T. Hennigan, J. Menick, A. Cassirer, R. Powell, G. van den
Driessche, L. A. Hendricks, M. Rauh, P.-S. Huang, A. Glaese, J. Welbl, S.
Dathathri, S. Huang, J. Uesato, J. Mellor, I. Higgins, A. Creswell, N.
McAleese, A. Wu, E. Elsen, S. Jayakumar, E. Buchatskaya, D. Budden, E.
Sutherland, K. Simonyan, M. Paganini, L. Sifre, L. Martens, X. L. Li, A.
Kuncoro, A. Nematzadeh, E. Gribovskaya, D. Donato, A. Lazaridou, A. Mensch,
J.-B. Lespiau, M. Tsimpoukelli, N. Grigorev, D. Fritz, T. Sottiaux, M.
Pajarskas, T. Pohlen, Z. Gong, D. Toyama, C. de Masson d‚ÄôAutume, Y. Li, T.
Terzi, V. Mikulik, I. Babuschkin, A. Clark, D. de Las Casas, A. Guy, C. Jones,
J. Bradbury, M. Johnson, B. Hechtman, L. Weidinger, I. Gabriel, W. Isaac, E.
Lockhart, S. Osindero, L. Rimell, C. Dyer, O. Vinyals, K. Ayoub, J. Stanway,
L. Bennett, D. Hassabis, K. Kavukcuoglu, and G. Irving. Scaling language
models: Methods, analysis & insights from training gopher, 2022. * Ramamurthy
et al. \\[2023\\] R. Ramamurthy, P. Ammanabrolu, K. Brantley, J. Hessel, R.
Sifa, C. Bauckhage, H. Hajishirzi, and Y. Choi. Is reinforcement learning
(not) for natural language processing: Benchmarks, baselines, and building
blocks for natural language policy optimization, 2023. * Reid et al.
\\[2022\\] M. Reid, Y. Yamada, and S. S. Gu. Can wikipedia help offline
reinforcement learning?, 2022. * Richardson et al. \\[2023\\] C. Richardson,
A. Sundar, and L. Heck. Syndicom: Improving conversational commonsense with
error-injection and natural language feedback. _arXiv preprint
arXiv:2309.10015_, 2023. * Roy and Roth \\[2016\\] S. Roy and D. Roth. Solving
general arithmetic word problems, 2016. * Sanh et al. \\[2020\\] V. Sanh, L.
Debut, J. Chaumond, and T. Wolf. Distilbert, a distilled version of bert:
smaller, faster, cheaper and lighter, 2020. * Schulman et al. \\[2017\\] J.
Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy
optimization algorithms, 2017. * Shen et al. \\[2023\\] T. Shen, R. Jin, Y.
Huang, C. Liu, W. Dong, Z. Guo, X. Wu, Y. Liu, and D. Xiong. Large language
model alignment: A survey. _arXiv preprint arXiv:2309.15025_, 2023. * Solaiman
and Dennison \\[2021\\] I. Solaiman and C. Dennison. Process for adapting
language models to society (palms) with values-targeted datasets. _Advances in
Neural Information Processing Systems_, 34:5861‚Äì5873, 2021. * Song et al.
\\[2023\\] J. Song, Z. Zhou, J. Liu, C. Fang, Z. Shu, and L. Ma. Self-refined
large language model as automated reward function designer for deep
reinforcement learning in robotics. _arXiv preprint arXiv:2309.06687_, 2023. *
Stiennon et al. \\[2020\\] N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe,
C. Voss, A. Radford, D. Amodei, and P. F. Christiano. Learning to summarize
with human feedback. _Advances in Neural Information Processing Systems_,
33:3008‚Äì3021, 2020. * Sun \\[2023a\\] H. Sun. Offline prompt evaluation and
optimization with inverse reinforcement learning. _arXiv preprint
arXiv:2309.06553_, 2023a. * Sun \\[2023b\\] H. Sun. Reinforcement learning in
the era of llms: What is essential? what is needed? an rl perspective on rlhf,
prompting, and beyond, 2023b. * Sutton and Barto \\[1998\\] R. Sutton and A.
Barto. Reinforcement learning: An introduction. _IEEE Transactions on Neural
Networks_, 9(5):1054‚Äì1054, 1998. doi:
[10.1109/TNN.1998.712192](https://arxiv.org/html/2402.01874v1/10.1109/TNN.1998.712192).
* Sutton and Barto \\[2018\\] R. S. Sutton and A. G. Barto. _Reinforcement
Learning: An Introduction_. A Bradford Book, Cambridge, MA, USA, 2018. ISBN
0262039249. * Thoppilan et al. \\[2022\\] R. Thoppilan, D. D. Freitas, J.
Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y.
Du, Y. Li, H. Lee, H. S. Zheng, A. Ghafouri, M. Menegali, Y. Huang, M. Krikun,
D. Lepikhin, J. Qin, D. Chen, Y. Xu, Z. Chen, A. Roberts, M. Bosma, V. Zhao,
Y. Zhou, C.-C. Chang, I. Krivokon, W. Rusch, M. Pickett, P. Srinivasan, L.
Man, K. Meier-Hellstern, M. R. Morris, T. Doshi, R. D. Santos, T. Duke, J.
Soraker, B. Zevenbergen, V. Prabhakaran, M. Diaz, B. Hutchinson, K. Olson, A.
Molina, E. Hoffman-John, J. Lee, L. Aroyo, R. Rajakumar, A. Butryna, M. Lamm,
V. Kuzmina, J. Fenton, A. Cohen, R. Bernstein, R. Kurzweil, B. Aguera-Arcas,
C. Cui, M. Croak, E. Chi, and Q. Le. Lamda: Language models for dialog
applications, 2022. * Torfi et al. \\[2020\\] A. Torfi, R. A. Shirvani, Y.
Keneshloo, N. Tavaf, and E. A. Fox. Natural language processing advancements
by deep learning: A survey. _arXiv preprint arXiv:2003.01200_, 2020. * Toro
Icarte et al. \\[2018\\] R. Toro Icarte, T. Q. Klassen, R. Valenzano, and S.
A. McIlraith. Teaching multiple tasks to an rl agent using ltl. In
_Proceedings of the 17th International Conference on Autonomous Agents and
MultiAgent Systems_, AAMAS ‚Äô18, page 452‚Äì461, Richland, SC, 2018.
International Foundation for Autonomous Agents and Multiagent Systems. * Van
Hasselt et al. \\[2015\\] H. Van Hasselt, A. Guez, and D. Silver. Deep
reinforcement learning with double q-learning. _Proceedings of the AAAI
Conference on Artificial Intelligence_, 30, 09 2015. doi:
[10.1609/aaai.v30i1.10295](https://arxiv.org/html/2402.01874v1/10.1609/aaai.v30i1.10295).
* Vaswani et al. \\[2017\\] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,
L. Jones, A. N. Gomez, ≈Å. Kaiser, and I. Polosukhin. Attention is all you
need. _Advances in neural information processing systems_, 30, 2017. * Wang et
al. \\[2023a\\] J. Wang, Y. Huang, C. Chen, Z. Liu, S. Wang, and Q. Wang.
Software testing with large language model: Survey, landscape, and vision,
2023a. * Wang et al. \\[2023b\\] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang,
J. Zhang, Z. Chen, J. Tang, X. Chen, Y. Lin, et al. A survey on large language
model based autonomous agents. _arXiv preprint arXiv:2308.11432_, 2023b. *
Wang et al. \\[2023c\\] S. Wang, Y. Zhu, H. Liu, Z. Zheng, C. Chen, and J. Li.
Knowledge editing for large language models: A survey, 2023c. * Wang et al.
\\[2023d\\] X. Wang, G. Chen, G. Qian, P. Gao, X.-Y. Wei, Y. Wang, Y. Tian,
and W. Gao. Large-scale multi-modal pre-trained models: A comprehensive
survey, 2023d. * Wang et al. \\[2023e\\] Y. Wang, W. Zhong, L. Li, F. Mi, X.
Zeng, W. Huang, L. Shang, X. Jiang, and Q. Liu. Aligning large language models
with human: A survey, 2023e. * Wang et al. \\[2016\\] Z. Wang, T. Schaul, M.
Hessel, H. Hasselt, M. Lanctot, and N. Freitas. Dueling network architectures
for deep reinforcement learning. In M. F. Balcan and K. Q. Weinberger,
editors, _Proceedings of The 33rd International Conference on Machine
Learning_, volume 48 of _Proceedings of Machine Learning Research_, pages
1995‚Äì2003, New York, New York, USA, 20‚Äì22 Jun 2016. PMLR. URL
[https://proceedings.mlr.press/v48/wangf16.html](https://proceedings.mlr.press/v48/wangf16.html).
* Wei et al. \\[2022\\] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S.
Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler, E. H. Chi, T. Hashimoto,
O. Vinyals, P. Liang, J. Dean, and W. Fedus. Emergent abilities of large
language models, 2022. * Wei et al. \\[2023\\] J. Wei, X. Wang, D. Schuurmans,
M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, and D. Zhou. Chain-of-thought
prompting elicits reasoning in large language models, 2023. * Weiss \\[1960\\]
G. Weiss. Dynamic programming and markov processes. ronald a. howard.
technology press and wiley, new york, 1960. viii + 136 pp. illus. $5.75.
_Science_, 132(3428):667‚Äì667, 1960. doi:
[10.1126/science.132.3428.667.a](https://arxiv.org/html/2402.01874v1/10.1126/science.132.3428.667.a).
URL
[https://www.science.org/doi/abs/10.1126/science.132.3428.667.a](https://www.science.org/doi/abs/10.1126/science.132.3428.667.a).
* Wu et al. \\[2022\\] H. Wu, M. Wang, J. Wu, F. Francis, Y.-H. Chang, A.
Shavick, H. Dong, M. T. Poon, N. Fitzpatrick, A. P. Levine, et al. A survey on
clinical natural language processing in the united kingdom from 2007 to 2022.
_NPJ digital medicine_, 5(1):186, 2022. * Wu et al. \\[2023a\\] L. Wu, Z.
Zheng, Z. Qiu, H. Wang, H. Gu, T. Shen, C. Qin, C. Zhu, H. Zhu, Q. Liu, H.
Xiong, and E. Chen. A survey on large language models for recommendation,
2023a. * Wu et al. \\[2019\\] Y. Wu, G. Tucker, and O. Nachum. Behavior
regularized offline reinforcement learning, 2019. * Wu et al. \\[2023b\\] Y.
Wu, S. Prabhumoye, S. Y. Min, Y. Bisk, R. Salakhutdinov, A. Azaria, T.
Mitchell, and Y. Li. Spring: Gpt-4 out-performs rl algorithms by studying
papers and reasoning, 2023b. * Xi et al. \\[2023\\] Z. Xi, W. Chen, X. Guo, W.
He, Y. Ding, B. Hong, M. Zhang, J. Wang, S. Jin, E. Zhou, et al. The rise and
potential of large language model based agents: A survey. _arXiv preprint
arXiv:2309.07864_, 2023. * Xie et al. \\[2023\\] T. Xie, S. Zhao, C. H. Wu, Y.
Liu, Q. Luo, V. Zhong, Y. Yang, and T. Yu. Text2reward: Automated dense reward
function generation for reinforcement learning. _arXiv preprint
arXiv:2309.11489_, 2023. * Yang et al. \\[2023\\] J. Yang, H. Jin, R. Tang, X.
Han, Q. Feng, H. Jiang, B. Yin, and X. Hu. Harnessing the power of llms in
practice: A survey on chatgpt and beyond, 2023. * Yu et al. \\[2021a\\] C. Yu,
J. Liu, S. Nemati, and G. Yin. Reinforcement learning in healthcare: A survey.
_ACM Comput. Surv._, 55(1), nov 2021a. ISSN 0360-0300. doi:
[10.1145/3477600](https://arxiv.org/html/2402.01874v1/10.1145/3477600). URL
[https://doi.org/10.1145/3477600](https://doi.org/10.1145/3477600). * Yu et
al. \\[2021b\\] T. Yu, D. Quillen, Z. He, R. Julian, A. Narayan, H. Shively,
A. Bellathur, K. Hausman, C. Finn, and S. Levine. Meta-world: A benchmark and
evaluation for multi-task and meta reinforcement learning, 2021b. * Yuan et
al. \\[2023\\] H. Yuan, C. Zhang, H. Wang, F. Xie, P. Cai, H. Dong, and Z. Lu.
Plan4mc: Skill reinforcement learning and planning for open-world minecraft
tasks, 2023. * Zeng et al. \\[2015\\] Z. Zeng, H. Shi, Y. Wu, Z. Hong, et al.
Survey of natural language processing techniques in bioinformatics.
_Computational and mathematical methods in medicine_, 2015, 2015. * Zhang et
al. \\[2023\\] S. Zhang, L. Dong, X. Li, S. Zhang, X. Sun, S. Wang, J. Li, R.
Hu, T. Zhang, F. Wu, and G. Wang. Instruction tuning for large language
models: A survey, 2023. * Zhang et al. \\[2022\\] T. Zhang, X. Wang, D. Zhou,
D. Schuurmans, and J. E. Gonzalez. Tempera: Test-time prompting via
reinforcement learning, 2022. * Zhang and Lu \\[2023\\] W. Zhang and Z. Lu.
Rladapter: Bridging large language models to reinforcement learning in open
worlds, 2023. * Zhao et al. \\[2023a\\] H. Zhao, H. Chen, F. Yang, N. Liu, H.
Deng, H. Cai, S. Wang, D. Yin, and M. Du. Explainability for large language
models: A survey, 2023a. * Zhao et al. \\[2023b\\] W. X. Zhao, K. Zhou, J. Li,
T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong, et al. A survey
of large language models. _arXiv preprint arXiv:2303.18223_, 2023b. * Zhou et
al. \\[2023a\\] C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y. Mao, X. Ma, A.
Efrat, P. Yu, L. Yu, et al. Lima: Less is more for alignment. _arXiv preprint
arXiv:2305.11206_, 2023a. * Zhou et al. \\[2023b\\] Y. Zhou, A. I. Muresanu,
Z. Han, K. Paster, S. Pitis, H. Chan, and J. Ba. Large language models are
human-level prompt engineers, 2023b. * Zhu et al. \\[2023\\] Y. Zhu, H. Yuan,
S. Wang, J. Liu, W. Liu, C. Deng, Z. Dou, and J.-R. Wen. Large language models
for information retrieval: A survey, 2023.


