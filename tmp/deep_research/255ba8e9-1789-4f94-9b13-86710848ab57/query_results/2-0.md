Query: recent advancements in reward design for reinforcement learning from human feedback in large language models
Extracted page content:
 Title: URL Source: https://arxiv.org/pdf/2412.10400v1 Markdown Content: # Reinforcement Learning Enhanced LLMs: A Survey Shuhe Wang ♠, Shengyu Zhang ♣, Jie Zhang ⋆, Runyi Hu ▲, Xiaoya Li ♦ Tianwei Zhang ▲, Jiwei Li ♣, Fei Wu ♣, Guoyin Wang, Eduard Hovy ♠ Abstract This paper surveys research in the rapidly grow-ing field of enhancing large language models (LLMs) with reinforcement learning (RL), a technique that enables LLMs to improve their performance by receiving feedback in the form of rewards based on the quality of their outputs, allowing them to generate more accurate, co-herent, and contextually appropriate responses. In this work, we make a systematic review of the most up-to-date state of knowledge on RL-enhanced LLMs, attempting to consolidate and analyze the rapidly growing research in this field, helping researchers understand the cur-rent challenges and advancements. Specifically, we (1) detail the basics of RL; (2) introduce popular RL-enhanced LLMs; (3) review re-searches on two widely-used reward model-based RL techniques: Reinforcement Learning from Human Feedback (RLHF) and Reinforce-ment Learning from AI Feedback (RLAIF); and (4) explore Direct Preference Optimization (DPO), a set of methods that bypass the reward model to directly use human preference data for aligning LLM outputs with human expecta-tions. We will also point out current challenges and deficiencies of existing methods and sug-gest some avenues for further improvements. 1 Introduction Large language models (Jiang et al., 2023; Ope-nAI, 2023; Dubey et al., 2024) are sophisticated language models pre-trained on extensive text data, allowing them to produce coherent and fluent re-sponses to diverse inputs. However, the interaction capabilities of these pre-trained LLMs can be in-consistent, sometimes leading to responses that, > ♠The University of Melbourne, ♣Zhejiang University, > ⋆CFAR and IHPC, A*STAR, Singapore, ▲Nanyang Techno-logical University, ♦University of Washington Email: shuhewang@student.unimelb.edu.au Project page of this work can be found at: https://github.com/ShuheWang1998/ Reinforcement-Learning-Enhanced-LLMs-A-Survey > * The latest update was on Dec. 3, 2024 (Version 1). while technically correct, may be harmful, biased, misleading, or irrelevant to users’ needs. There-fore, it is crucial to align the outputs of pre-trained LLMs with human preferences before they can be effectively applied to various natural language tasks (Wang et al., 2023b; Wan et al., 2023; Sun et al., 2023c,b; Giray, 2023; Zhang, 2023; Long, 2023; Sun, 2023; Gao et al., 2023; Paranjape et al., 2023; Sun et al., 2023a; Diao et al., 2023; Wang et al., 2023a; Zhang et al., 2023b; Sun et al., 2023d; Liu et al., 2024d; Yao et al., 2024; Liu et al., 2024c; Lee et al., 2024; Kambhampati, 2024; Wang et al., 2024c). Previously, a widely adopted approach for align-ing the outputs of pre-trained LLMs with human preferences has been supervised fine-tuning (SFT) (Hu et al., 2021; Mishra et al., 2021; Wang et al., 2022; Du et al., 2022; Dettmers et al., 2023; Taori et al., 2023; Zhang et al., 2023a; Chiang et al., 2023; Xu et al., 2023; Peng et al., 2023; Mukherjee et al., 2023; Li et al., 2023; Ding et al., 2023; Luo et al., 2023; Wang et al., 2024d; Zhou et al., 2024). This method further trains LLMs on (Instruction, Answer) pairs, where "Instruction" represents the human prompt given to the model, and "Answer" is the target output that follows the instruction. SFT helps guide LLMs to produce responses that adhere to specific characteristics or domain knowledge, making it possible for humans to interactive with LLMs. Despite its effectiveness, SFT has limita-tions: during training, the model is constrained to learn specific answers we provide, with metrics like perplexity (PPL) penalizing synonym use. On one hand, this can hinder the LLM’s ability to gen-eralize, as tasks like writing and summarization have multiple valid phrasings. On the other hand, it may cause poor performance in aligning with human preferences, as no direct human feedback is incorporated into the training process. To alleviate the above issues, reinforcement learning (RL) is adopted in aligning the outputs > arXiv:2412.10400v1 [cs.CL] 5 Dec 2024 of LLMs with human preferences, which can be decomposed into three steps: (1) First, before fine-tuning, a reward model (or reward function) is trained to approximate human preferences and score different LLM outputs; (2) Then, during each fine-tuning iteration, given a single instruction, the LLM generates multiple responses, each of which is scored by the trained reward model; (3) Finally, policy optimization, an RL optimization technique, updates the LLM’s weights to improve predictions based on these preference scores. Fine-tuing LLMs with RL tackles the aforementioned issues simul-taneously. In one line, rather than being restricted to learning a specific answer, RL adjusts the LLM based on various preference scores, rewarding any valid, well-phrased responses. In the other line, the reward model is designed to approximate hu-man preferences, enabling direct training on human preferences and fostering the LLM’s capacity for impressive creativity. In this paper, we organize the most up-to-date state of knowledge on reinforcement learning (RL) in large language models (LLMs), attempting to consolidate and analyze the rapidly growing re-search in this field, helping researchers understand the current landscape, challenges, and advance-ments. Specifically, • Section 2 presents the basics of reinforcement learning (RL) along with key terminologies, and outlines how the RL pipeline is adapted for LLMs. • Section 3 introduces popular and powerful LLMs enhanced by reinforcement learning. • Section 4 outlines the process of reinforce-ment learning from human feedback (RLHF), a training method that integrates reinforce-ment learning with human feedback to align LLMs with human values, preferences, and expectations. • Section 5 reviews research on reinforcement learning from AI feedback (RLAIF), which presents a promising alternative or comple-ment to RLHF by utilizing AI systems to pro-vide feedback on the outputs of the LLM be-ing trained, offering advantages in scalability, consistency, and cost-effectiveness. • Section 6 provides an analysis of the chal-lenges associated with RLHF and RLAIF. • Section 7 discusses research on direct prefer-ence optimization (DPO), a series of methods that bypasses the reward model and directly utilizes human preference data to align LLM outputs with human expectations. • Section 8 summarizes the current challenges and discusses opportunities for further im-provement.. 2 Basics: Reinforcement Learning for LLMs In this section, we first detail the basics of reinforce-ment learning (RL) along with key terminologies, and then outline how the RL pipeline is adapted for LLMs. 2.1 Basics of Reinforcement Learning Reinforcement Learning (RL) is a key approach in machine learning, focusing on how an agent engages with its environment to maximize cumu-lative rewards. Unlike supervised learning, which depends on labeled data, and unsupervised learn-ing, which uncovers patterns in unlabeled data, RL emphasizes learning through direct feedback via trial and error. Below, we sequentially describe basic definitions and general pipeline of RL. 2.1.1 Basic Definitions Here, we use the training example in Figure 1 to il-lustrate the full process of RL. In this example, our goal is to train a robot to move from the bottom-left corner of a square to the top-right corner. Addition-ally, each grid cell has a reward score, and we aim to maximize the robot’s total score. Before delving into the training process, we first introduce some relevant terms: • Agent: An agent is the entity we train to make correct decisions. In this example, our goal is to train the robot to make movement decisions, so the robot is the agent. • Environment: The environment is the ex-ternal system that the agent interacts with. For our example, as the trained robot (agent) moves within the grid, the grid serves as the environment. • State: The state represents the agent’s posi-tion at each time t. For instance, at the be-ginning, at time t0, the robot (agent) starts at the bottom-left corner, so the state at time t0Figure 1: An example of the full process of RL. Training Objective: The goal is to train a robot to navigate from the bottom-left corner of a square to the top-right corner. Each grid cell is assigned a reward score, and the objective is to maximize the robot’s overall score. General Pipeline of RL: The agent begins in an initial state s0, and at each time step t, it selects an action at based on its current state st. In response, the environment transitions to a new state st+1 , and the agent receives a reward rt. is the bottom-left corner, represented by the coordinates (0 , 0) .• Action(s): Actions represent the possible choices available to the agent within the envi-ronment at each time t. For example, at the start, at time t0, the robot (agent) can choose to move right or up, making these two actions available to the agent at t0.• Reward(s): Rewards are the signals or feed-back provided by the environment to the agent based on the action it takes at each time t. For instance, at time t0, the robot (agent) would receive a reward of +5 points for moving right, or a penalty of -1 point for moving up. • Policy: A policy is a set of decision-making strategies that helps the agent choose an action at each time t. In practice, at time t0, the pol-icy represents a probability distribution that directs the robot (agent) to move right or up in order to maximize its cumulative rewards. 2.1.2 General Pipeline of RL We have defined key terminologies used in RL, and in this section, we will continue to detail the general pipeline of RL. As illustrated in Figure 1, the general reinforce-ment learning (RL) pipeline can be represented as a Markov Decision Process (MDP). Formally, the agent begins in an initial state s0, and at each time step t, it selects an action at based on its current state st. In response, the environment transitions to a new state st+1 , and the agent receives a reward rt. This cycle continues, with the agent’s objec-tive being to maximize the cumulative rewards it accumulates over time. Mapping into the specific example in Figure 1, at the initial time t0, the robot starts at the bottom-left corner, denoted by the position (state) s0. As time progresses, at each time step t, the robot chooses an action at (either moving up or moving right). This action causes the robot to transition from its current position st to a new position st+1 , while earning a reward tt. This cycle of movement and reward col-lection continues until the robot reaches the desired position (state) at the top-right corner, achieving the goal of maximum cumulative rewards. 2.2 RL for LLMs We have outlined the general framework of RL above; now we will delve into the process of fine-tuning LLMs using RL. This approach aims to align LLMs with desired behaviors, enhance their per-formance, and ensure that their outputs are both effective and dependable. In reinforcement learning (RL), there are six key components:agent, environment, state, action, re-ward, and policy. To apply RL for fine-tuning large language models (LLMs), the first step is to map these components to the LLM framework. LLMs are highly proficient at next-token predic-tion, where they take a sequence of tokens as input and predict the next token based on the given con-text. From an RL perspective, we can view the LLM itself as the policy. The current textual se-quence represents the state, and based on this state, the LLM generates an action—the next token. This action updates the state, creating a new state that Figure 2: The framework of RL for LLMs proposed by Ouyang et al. (2022). incorporates the newly added token. After gen-erating a complete textual sequence, a reward is determined by assessing the quality of the LLM’s output using a pre-trained reward model. Figure 2 illustrates the specific RL framework for LLMs as proposed by (Ouyang et al., 2022). Ouyang et al. (2022) starts with an instruction-tuned model trained through supervised learning, enabling it to generate structured responses to hu-man instructions. Then, Ouyang et al. (2022) ap-plies the following two steps: Step 1: Collect comparison data, and train a reward model. Ouyang et al. (2022) collects a dataset of comparisons between outputs of the instruction-tuned model, where labelers indicate which output they prefer for a given input. Then, the collected dataset is used to train a reward model (RM) to predict the human-preferred output. Step 2: Optimize a policy against the reward model using PPO. Ouyang et al. (2022) leverages the output of the RM as a scalar reward, and fine-tunes the instruction-tuned model to optimize this reward using the PPO algorithm (Schulman et al., 2017). 3 Popular LLMs Enhanced by RL Recent popular LLMs with strong capabilities al-most all leverage reinforcement learning (RL) to further enhance their performance during the post-training process. The RL methods adopted by these models can be typically divided into two main lines: 1. Traditional RL approaches, such as Reinforce-ment Learning from Human Feedback (RLHF) and Reinforcement Learning from AI Feedback (RLAIF). These methods require training a reward model and involve a complex and often unstable process, using algorithms like Proximal Policy Op-timization (PPO) (Schulman et al., 2017) to opti-mize the policy model. Models like InstructGPT (Ouyang et al., 2022), GPT-4 (OpenAI, 2023), and Claude 3 (Anthropic, 2024) follow this approach. 2. Simplified approaches, such as Direct Prefer-ence Optimization (DPO) (Rafailov et al., 2024) and Reward-aware Preference Optimization (RPO) (Adler et al., 2024). These methods discard the reward model, offering a stable, performant, and computationally efficient solution. Models like Llama 3 (Dubey et al., 2024), Qwen 2 (Yang et al., 2024a), and Nemotron-4 340B (Adler et al., 2024) follow this approach. In this section, we provide a detailed description of each model, starting with a brief overview of these RL enhanced LLMs and followed by an explanation of how RL is applied in their post-training process. An overview of these RL Enhanced LLMs is shown in Tab 1. RL Enhanced LLMs Organization # Params RL Methods > Instruct-GPT (Ouyang et al., 2022) 1.3B, 6B, 175B RLHF, PPO GPT-4 (OpenAI, 2023) -RLHF, PPO, RBRM Gemini (Team et al., 2023) -RLHF InternLM2 (Cai et al., 2024) 1.8B, 7B, 20B RLHF, PPO Claude 3 (Anthropic, 2024) -RLAIF Reka (Team et al., 2024c) 7B, 21B RLHF, PPO Zephyr (HuggingFaceH4, 2024) 141B-A39B ORPO Phi-3 (Abdin et al., 2024) 3.8B, 7B, 14B DPO DeepSeek-V2 (Liu et al., 2024a) 236B-A21B GRPO ChatGLM (GLM et al., 2024) 6B, 9B ChatGLM-RLHF Nemotron-4 340B (Adler et al., 2024) 340B DPO, RPO Llama 3 (Dubey et al., 2024) 8B, 70B, 405B DPO Qwen2 (Yang et al., 2024a) (0.5-72)B, 57B-A14B DPO Gemma2 (Team et al., 2024b) 2B, 9B, 27B RLHF Starling-7B (Zhu et al., 2024) 7B RLAIF, PPO Athene-70B (Nexusflow, 2024) 70B RLHF Hermes 3 (Teknium et al., 2024) 8B, 70B, 405B DPO o1 (OpenAI, 2024b) -RL through CoT Table 1: An overview of RL Enhanced LLMs. The format ‘141B-A39B’ refers to MoE models with 141B total and 39B active parameters. 3.1 InstructGPT InstructGPT (Ouyang et al., 2022) is a series of language models fine-tuned from GPT-3 (Brown et al., 2020) by OpenAI, using human feedback to better align with human intent. The series in-cludes models in three sizes: 1.3 B, 6 B, and 175 B parameters. The model is first fine-tuned using supervised learning with prompts collected from the OpenAI API or written by labelers and corre-sponding labeler demonstrations, then further re-fined using reinforcement learning from human feedback (RLHF). Human evaluations reveal that InstructGPT outputs are preferred over GPT-3. No-tably, the 1.3B parameter InstructGPT model is favored over the 175B GPT-3, despite having 100 times fewer parameters. Additionally, InstructGPT demonstrates improved truthfulness and reduced toxic outputs, with minimal performance trade-offs on public NLP datasets. Before applying reinforcement learning (RL), the authors train a 6B reward model (RM) initial-ized from the supervised fine-tuned (SFT) model, with the final unembedding layer removed. This RM is trained using comparison data ranked by la-belers. During the RL phase, they fine-tune the SFT model to optimize the scalar reward output from the RM using the PPO algorithm (Schulman et al., 2017). To address performance regressions on pub-lic NLP datasets, they experiment with mixing pre-training gradients with PPO gradients, resulting in models known as PPO-ptx. 3.2 GPT-4 GPT-4 (OpenAI, 2023), developed by OpenAI, is a large multimodal model that can process both image and text inputs to produce text outputs. It excels at understanding and generating natural lan-guage, particularly in complex and nuanced scenar-ios. Evaluations show that GPT-4 performs excep-tionally well on a range of human-designed exams, often surpassing the majority of human test takers. Additionally, it outperforms earlier large language models and most state-of-the-art systems, which frequently rely on benchmark-specific training or hand-engineered solutions. GPT-4 leverages RLHF methods, as outlined in InstructGPT (Ouyang et al., 2022) which we have describe in Sec 3.1, in the post-training align-ment stage. To steer the models more effectively towards appropriate refusals at a finer level, the authors further use a zero-shot GPT-4 classifier as the rule-based reward model (RBRM). This RBRM provides an additional reward signal to the GPT-4 policy model during PPO fine-tuning on a subset of training prompts. The RBRM takes a prompt (optional), the policy model’s output, and a human-written rubric (e.g., a set of rules in multiple-choice style) as input, then classifies the output accord-ing to the rubric. Through this approach, GPT-4 is rewarded for refusing harmful content and for appropriately responding to known-safe prompts. 3.3 Gemini Gemini (Team et al., 2023) represents a family of advanced multimodal models developed by Google, distinguished by their impressive capabili-ties. The initial version, Gemini 1.0, comes in three sizes—Ultra, Pro, and Nano—ranging from large to small in terms of performance. Each size is tai-lored to address specific computational constraints and application needs. Notably, Gemini Ultra, the most powerful variant, achieves state-of-the-art re-sults in 30 out of 32 benchmarks and is the first model to attain human expert-level performance on MMLU (Hendrycks et al., 2020), while setting new records across all 20 multimodal benchmarks. Gemini implements a post-training process that utilizes an optimized feedback loop, collecting human-AI interactions to drive continuous improve-ment in key performance areas. During the post-training’s RLHF phase, an iterative approach is adopted wherein reinforcement learning (RL) in-crementally enhances the reward model (RM). Con-currently, the RM undergoes continuous refinement through systematic evaluation and data collection. This dynamic interplay promotes ongoing advance-ment in both RL and RM, leading to progressively improved performance over time. 3.4 InternLM2 InternLM2 (Cai et al., 2024) is an open-source se-ries of large language models developed by Shang-hai AI Laboratory, available in three sizes: 1.8B, 7B, and 20B. The model demonstrates superior performance across six dimensions and 30 bench-marks, including long-context modeling and open-ended subjective evaluations, thanks to innovative pre-training and optimization techniques. To further enhance alignment, InternLM2 em-ploys a novel strategy called Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) with the use of PPO. This approach addresses two key challenges. The first is prefer-ence conflict, where it is difficult to satisfy two preferences, such as helpfulness and harmlessness, simultaneously. The second challenge is reward hacking, which becomes more problematic as the model’s scale increases and its policy becomes more powerful. COOL RLHF introduces a Condi-tional Reward mechanism that reconciles diverse preferences by allowing a single reward model to dynamically adjust its focus based on specific con-ditional prompts, effectively integrating multiple preferences. Additionally, COOL RLHF incorpo-rates a multi-round Online RLHF strategy with two distinct pathways: a Fast Path for immediate, tar-geted improvements and a Slow Path for long-term, comprehensive refinement of the reward model. This approach enables the model to quickly adapt to new human feedback while reducing the risk of reward hacking. 3.5 Claude 3 Claude 3 (Anthropic, 2024) is a family of large mul-timodal models developed by Anthropic, which demonstrates strong performance across bench-mark evaluations. It comprises three models with varying abilities and speeds: the largest, Claude 3 Opus; the mid-sized, Claude 3 Sonnet; and the smallest, Claude 3 Haiku. The Claude 3 models show strong benchmark performance, setting new standards in reasoning, math, and coding. Claude 3 Opus achieves state-of-the-art results on evalu-ations such as GPQA (Rein et al., 2023), MMLU (Hendrycks et al., 2020), and MMMU (Yue et al., 2024). Claude 3 Haiku matches or surpasses Claude 2 in most text tasks, while Sonnet and Opus perform significantly better. The authors use a technique called Constitutional AI (Bai et al., 2022) to align Claude 3 with human values during reinforcement learning (RL). In the RL stage, Constitutional AI follows a process sim-ilar to RLHF, but instead of human preferences for harmlessness, it uses AI feedback, known as RLAIF. Specifically, it distills language model in-terpretations of a set of rules and principles into a hybrid human/AI preference model (PM), using hu-man labels for helpfulness and AI labels for harm-lessness. Afterwards, they fine-tune the supervised learning model using RL with this PM, resulting in a policy trained by RLAIF. 3.6 Zephyr 141B-A39B Zephyr 141B-A39B (HuggingFaceH4, 2024) is the newest addition to the Zephyr (Tunstall et al., 2023) series of language models, developed through a col-laboration between Argilla, KAIST, and Hugging Face. This model is a Mixture of Experts (MoE) with a total of 141 billion parameters, 39 billion of which are active, fine-tuned from Mixtral-8x22B-v0.1 (Mistral AI, 2024). Zephyr 141B-A39B employs a novel alignment algorithm known as Odds Ratio Preference Opti-mization (ORPO) (Hong et al., 2024). ORPO is a straightforward, unified alignment approach that discourages the model from adopting undesired generation styles during supervised fine-tuning. Notably, ORPO does not require an SFT warm-up phase, a reward model, or a reference model, making it highly resource-efficient. The method works by adding an odds ratio-based penalty to the standard SFT negative log-likelihood loss, en-abling the model to distinguish between preferred and non-preferred response styles. 3.7 DeepSeek-V2 DeepSeek-V2 (Liu et al., 2024a), developed by DeepSeek-AI, is a powerful Mixture-of-Experts (MoE) language model designed for economical training and efficient inference. It features innova-tive architectures such as Multi-head Latent Atten-tion (MLA) and DeepSeekMoE. With 236 billion total parameters, of which 21 billion are activated per token, it supports a context length of up to 128K tokens. The model is pre-trained on a high-quality, multi-source corpus of 8.1 trillion tokens. Evalua-tions show that DeepSeek-V2, along with its chat versions, maintains top-tier performance among open-source models, despite having only 21 billion activated parameters. DeepSeek-V2 is optimized using Group Relative Policy Optimization (GRPO) (Shao et al., 2024) during the RL phase to reduce training costs. Un-like traditional RL methods that use a critic model of similar size to the policy model, which increases training expenses, GRPO foregoes the critic model and estimates the baseline from scores computed on a group of outputs for the same question. Addition-ally, a two-stage RL training strategy is employed: the first stage focuses on reasoning alignment, and the second on human preference alignment, as the authors find these stages exhibit distinct character-istics. 3.8 ChatGLM ChatGLM (GLM et al., 2024), developed by Zhipu AI, represents an evolving series of large language models. The latest version in this series is GLM-4, which includes variants such as GLM-4, GLM-4-Air, and GLM-4-9B. These models are pre-trained on a dataset of over 10 trillion tokens, predom-inantly in Chinese and English, and are subse-quently post-trained through a combination of su-pervised fine-tuning (SFT) and RLHF to achieve advanced alignment quality. Evaluation results in-dicate that GLM-4 rivals or even surpasses GPT-4 (OpenAI, 2023) on general benchmarks like MMLU, and demonstrates superior performance in Chinese-specific alignments as measured by Align-Bench (Liu et al., 2023b). The reinforcement learning phase involves the ChatGLM-RLHF (Hou et al., 2024) pipeline, which enhances alignment with human preferences. This pipeline comprises three primary components: gathering human preference data, training a re-ward model, and optimizing policy models. To support large-scale training, ChatGLM-RLHF in-cludes methods to reduce reward variance for stable training, leverages model parallelism with fused gradient descent, and applies regularization con-straints to prevent catastrophic forgetting in large language models. Experimental results confirm that ChatGLM-RLHF yields substantial improve-ments in alignment-focused tasks compared to the supervised fine-tuned version of ChatGLM. 3.9 Nemotron-4 340B Nemotron-4 340B (Adler et al., 2024) is a fam-ily of models released by NVIDIA, consisting of Nemotron-4-340B-Base, Nemotron-4-340B-Instruct, and Nemotron-4-340B-Reward. The Nemotron-4-340B-Base model is trained on 9 tril-lion tokens from a high-quality dataset. In the alignment process to develop Nemotron-4-340B-Instruct, over 98% of the data used is synthetically generated by the model. Evaluations demonstrate that these models perform competitively with open-access models across a broad range of evaluation benchmarks. During the preference fine-tuning phase, both DPO (Rafailov et al., 2024) and a new alignment algorithm, Reward-aware Preference Optimization (RPO), are employed to improve the model through multiple iterations. RPO addresses a limitation in DPO, where the quality difference between se-lected and rejected responses is not considered, leading to overfitting and the forgetting of valuable responses. RPO uses an implicit reward from the policy network to approximate this gap, enabling the model to better learn from and retain superior feedback. 3.10 Llama 3 Llama 3 (Dubey et al., 2024), developed by Meta, is a collection of open-source foundational lan-guage models available in sizes of 8 billion, 70 billion, and 405 billion parameters. It is trained on a significantly larger corpus consisting of approx-imately 15 trillion multilingual tokens, a notable increase compared to the 1.8 trillion tokens used for Llama 2 (Touvron et al., 2023). Extensive empiri-cal evaluations demonstrate that Llama 3 achieves performance comparable to leading models, such as GPT-4 (OpenAI, 2023), across a diverse range of tasks. The post-training process for aligning Llama 3 with human feedback involves six rounds of iter-ative refinement. Each round includes supervised fine-tuning (SFT) followed by DPO, with the fi-nal model being an average of the outputs from all rounds. For each round, a reward model (RM) is trained on newly collected preference annotation data, targeting a wide range of capabilities built upon the pre-trained checkpoint. After SFT, DPO is applied to further optimize the SFT models, us-ing recent preference data batches obtained from the best-performing models of previous rounds. To enhance the stability of DPO training, two key ad-justments are implemented: masking out format-ting tokens in the DPO loss and introducing reg-ularization via an NLL (negative log-likelihood) loss. 3.11 Qwen2 Qwen2 (Yang et al., 2024a), developed by Alibaba, is a series of large language models ranging from 0.5 billion to 72 billion parameters in dense con-figurations, as well as a Mixture-of-Experts vari-ant with 57 billion parameters, of which 14 bil-lion are activated per token. It is pre-trained on a high-quality, large-scale dataset containing over 7 trillion tokens, covering a wide array of domains and languages. Extensive evaluations show that Qwen2 outperforms most prior open-weight mod-els, including its predecessor Qwen1.5, and deliv-ers competitive results across a range of bench-marks, including language understanding, genera-tion, multilingual proficiency, coding, mathematics, and reasoning. The preference fine-tuning process for Qwen2 consists of two main stages: offline and online learning. In the offline stage, Qwen2 is opti-mized using DPO, which aims to maximize the likelihood difference between two responses to the same prompt, based on a pre-compiled preference dataset. In the online stage, the model improves continuously in real-time by utilizing preference pairs selected by the reward model from multiple responses generated by the current policy model. Additionally, the Online Merging Optimizer (Lu et al., 2024) is employed to minimize alignment costs. 3.12 Gemma 2 Gemma 2 (Team et al., 2024b), developed by Google, is the latest addition to the Gemma fam-ily of lightweight, state-of-the-art open models, with sizes ranging from 2 billion to 27 billion pa-rameters. The model incorporates several well-established modifications to the Transformer archi-tecture, including interleaving local-global atten-tions (Beltagy et al., 2020) and group-query atten-tion (Ainslie et al., 2023). Experiments demon-strate that these models deliver the best perfor-mance for their size and even provide competitive alternatives to models 2-3 times larger. Similar to Gemma 1.1 (Team et al., 2024a), dur-ing the post-training RLHF phase, the authors use a high-capacity model as an automatic rater to tune hyperparameters and mitigate reward hacking (Amodei et al., 2016; Skalse et al., 2022). However, unlike Gemma 1.1, they employ a reward model that is an order of magnitude larger than the policy model. This reward model is specifically designed to focus on conversational capabilities, with an em-phasis on multi-turn interactions. 3.13 Starling-7B Starling-7B (Zhu et al., 2024) is a strong 7-billion-parameter chat model developed by UC Berkeley, focused on alignment with human preferences for helpfulness and harmlessness. It is fine-tuned from Openchat-3.5 (Wang et al., 2024a) using RLAIF on a high-quality preference dataset called Nectar, which comprises 3.8 million pairwise comparisons generated by prompting GPT-4 to rank responses. As a result, the model’s score on MT-Bench im-proves from 7.81 to 8.09, its score on AlpacaEval increases from 88.51% to 91.99%, and its human evaluation ELO on Chatbot Arena (Chiang et al., 2024) rises from 1072 to 1087. The authors introduce several improvements to the PPO algorithm during the RLAIF process to en-hance training stability and robustness. First, they introduce a constant positive reward for length con-trol to prevent excessive verbosity. This adjustment helps address the issue where a highly negative re-ward from the reward model during the early stages can cause the policy model to become overly ver-bose after only a few gradient updates. Second, they pretrain the critic model to reduce early per-formance drops due to a randomly initialized critic. Third, they conduct full parameter tuning on both the actor and critic models, as opposed to tuning only the top four layers, to maximize performance improvements during the reinforcement learning stage. 3.14 o1 OpenAI’s o1 (OpenAI, 2024b) is a newly devel-oped large language model optimized for complex reasoning, utilizing reinforcement learning for its training. Before producing responses, o1 engages in an extensive internal thought process, enabling it to excel across various reasoning tasks. The model significantly surpasses GPT-4o (OpenAI, 2024a) in many challenging tasks: ranks in the 89th percentile on Codeforces for competitive program-ming, places among the top 500 participants in the AIME for mathematics, and surpasses PhD-level accuracy in scientific benchmarks such as GPQA. The training of o1 involves a large-scale rein-forcement learning algorithm that emphasizes pro-ductive thinking through a detailed chain of thought (CoT) (Wei et al., 2023), implemented with high data efficiency. To preserve the model’s unfiltered reasoning ability, no policy compliance or user pref-erence training is applied to its internal thought pro-cesses, which also provides a unique opportunity to understand the model’s raw thought process. This approach allows o1 to refine its strategies, correct errors, and deconstruct complex problems during training. Notably, the model’s performance im-proves with increased training compute and with more extensive test-time computation. 3.15 Others Reka Core, Flash, and Edge: Team et al. (2024c) are powerful multimodal language models developed from scratch by Reka. Reka Edge and Reka Flash are dense models with 7B and 21B pa-rameters, respectively, outperforming many larger models and offering exceptional performance for their compute class. The flagship model, Reka Core, competes with leading models like GPT-4v, Gemini, and Claude 3 in both automated and blind human evaluations. During post-training, follow-ing supervised fine-tuning, Reka models undergo multiple rounds of RLHF using PPO to enhance alignment further. Phi-3: Abdin et al. (2024) is a series of language models introduced by Microsoft, comprising phi-3-mini, phi-3-small, and phi-3-medium. Remarkably, the smallest model, phi-3-mini, is trained on 3.3 trillion tokens yet contains only 3.8 billion parame-ters, making it compact enough for deployment on a mobile device. Despite its relatively small size, phi-3-mini demonstrates performance comparable to larger models like Mixtral 8x7B and GPT-3.5, achieving 69% on MMLU and a score of 8.38 on MT-bench in both academic benchmarks and in-ternal testing. During post-training, the authors employ DPO to guide phi-3 away from undesired behavior by treating those outputs as “rejected” re-sponses. Athene-70B: Nexusflow (2024) is a powerful chat model fine-tuned from Llama-3-70B (Dubey et al., 2024), developed by Nexusflow. It achieves an impressive Arena-Hard-Auto score of 77.8%, placing it close to leading proprietary models like GPT-4o (79.2%) and Claude-3.5-Sonnet (79.3%). This marks a significant leap from its predecessor, Llama-3-70B-Instruct, which scored 46.6%. This progress is attributed to Nexusflow’s targeted post-training approach, which enhances the model’s per-formance. Specifically, Nexusflow curates high-quality preference data based on internal bench-mark evaluations covering instruction following, coding, creative writing, and multilingual tasks. This data is then used for targeted RLHF, result-ing in substantial performance gains over Llama-3-70B-Instruct. Hermes 3: Teknium et al. (2024) is a series of neutrally-aligned generalist instruction and tool-use models with advanced reasoning and creative capabilities, developed by Nous Research. It is Original Data Distribution (378K) Filtered Data Distribution (80K) > 25.9% > (98000) > 25.9% > (98000) > 25.9% > (98000) > 13.2% > (50000) > 4.9% (18548) > 1.9% (7221) 2.3% (8504) > 36.2% > (29682) > 33.9% > (27785) > 8.8% > (7221) > 10.4% > (8504) > 8.2% > (6709) > 2.5% (2030) Figure 1 |The composition chart of the Skywork-Reward preference data selections before and after applying data selection and filtering operations. used in our data mixture (section 3.1), the data selection and filtering techniques employed to optimize its composition (section 3.2), and the training objective that guides the reward model’s learning process (section 3.3). Our methodology aims to enhance the effectiveness of reward modeling while maintaining transparency and accessibility by focusing on solely publicly available preference data. We visualize the composition chart of the Skywork-Reward preference data selections in fig. 1. > 3.1. Dataset Mixture > Existing research (Dong et al., 2024; Jiang et al., 2023; Touvron et al., 2023) frequently leverages a mixture of preference datasets from multiple sources to train reward models. These datasets typically contain between several hundred thousand to over a million samples. For instance, Llama 2 (Touvron et al., 2023) employs approximately 1.5 million publicly available preference data points, augmented with 1.4 million internally generated samples, for reward model training. A substantial portion of the public data originates from StackExchange, with the remainder capturing attributes such as helpfulness, harmlessness, and general human preferences. In a similar vein, Dong et al. (2024) assemble a more diverse dataset by aggregating samples from eight distinct sources, producing a collection of around 700K preference pairs. Notably, approximately 90% of the responses in this dataset are generated by various LLMs, with more than half of the annotations sourced from GPT-3.5 and GPT-4. This growing reliance on LLM-generated data underscores the increasing trend toward using automated systems for large-scale preference labeling in reward model development. We present the statistics of the Skywork Reward Preference data collections in table 1. > A lightweight yet high-quality data composition Our objective is to construct a more lightweight preference data collection that not only reduces the overall data requirements but also targets impor-tant abilities and domains that RLHF seeks to optimize, such as math and code. Additionally, we > focus exclusively on publicly available data to ensure transparency, reproducibility, and to enable broader adoption of our methodologies without reliance on proprietary or internal datasets. This strategy has resulted in the creation of the following dataset mixture, which we introduce below with a brief overview of each included dataset. •HelpSteer2 (Wang et al., 2024e) is a compact preference dataset comprising only 10K 4 Figure 3: The composition of the Skywork-Reward. The figure is copied from Liu et al. (2024b). finetuned from Llama 3.1 (Dubey et al., 2024) in 8B, 70B, and 405B variants and the largest model, Hermes 3 405B, sets the state-of-the-art perfor-mance among open-weight models across several public benchmarks. Hermes is trained on diverse synthetic reasoning tasks and creative applications such as role playing and writing. It is designed to precisely and neutrally follow system and instruc-tion prompts, unlike many commercial models that may decline instructions for moral reasons. To fur-ther align Hermes, the authors leverage DPO and train a LoRA (Hu et al., 2021) adapter instead of fine-tuning the entire model, significantly reduc-ing GPU memory usage for both the reference and trained models. 4 RLHF: Reinforcement Learning from Human Feedback Reinforcement learning from human feedback (RLHF) is a training approach that combines re-inforcement learning (RL) with human feedback to align LLMs with human values, preferences, and expectations. RLHF consists of two main compo-nents: (1) Collecting Human Feedback to Train Reward Model , where human evaluators provide feedback on the LLM’s outputs by scoring or rank-ing responses based on factors such as quality and relevance. This feedback is then used to train a re-ward model that predicts the quality of the outputs and serves as the reward function in the RL process; and (2) Preference Optimization Using Human Feedback , where the trained reward model guides the optimization of the LLM’s outputs to maximize predicted rewards, aligning the LLM’s behavior with human preferences. Below, we will illustrate these two components via recent research studies. 4.1 Collecting Human Feedback to Train Reward Model Skywork-Reward (Liu et al., 2024b). Skywork-Reward is a carefully designed dataset contain-ing 80,000 high-quality preference pairs, curated through effective data selection and filtering strate-gies. As shown in Figure 3, the original dataset, with 378,000 preference pairs, is significantly re-fined into a compact, high-quality dataset of 80,000 pairs. Despite being significantly smaller than existing datasets, it achieves exceptional quality through rigorous cleaning, consistency checks, model-based scoring to filter out low-quality sam-ples, and manual reviews. Covering a diverse range of tasks such as instruction following, code generation, and multilingual handling, Skywork-Reward serves as the foundation for models like Skywork-Reward-Gemma-27B, which excel on benchmarks 1. By enabling language models to better understand human preferences, Skywork-Reward helps LLMs become more accurate and useful in real-world applications. TÜLU-V2-mix (Ivison et al., 2023). TÜLU-V2-mix is designed to enhance instruction-following capabilities in large language models, offering a diverse dataset that improves the model’s general-ization and execution abilities across multi-domain tasks. It covers a wide range of tasks, including question answering, code generation, translation, and multi-turn conversations, with a strong em-phasis on multilingual adaptability and handling complex real-world scenarios. Skywork-Reward, on the other hand, is designed to align models with human preferences using preference pairs, helping models learn to generate user-preferred responses, such as fluent and coherent text. While TÜLU-V2-mix excels in generalization across a wide range of tasks, Skywork-Reward specializes in optimizing user-centric outputs. Together, they address com-plementary goals for advancing language model capabilities. 4.2 Preference Optimization Using Human Feedback Once the reward model is trained, it is used to guide the fine-tuning of the original LLM through reinforcement learning. The main objective is to improve the LLM’s behavior based on the predicted rewards, making it more likely to generate outputs > 1 https://huggingface.co/spaces/allenai/reward-bench that align with human preferences. Recent research (Ouyang et al., 2022; Yuan et al., 2023; Dong et al., 2024; Ahmadian et al., 2024) has shown that this process can be broken down into two key steps: (1) Rewarding: In this step, the LLM generates multiple outputs in response to a given instruction. Each output is then passed through the trained re-ward model, which assigns a scalar score that ap-proximates human preferences. (2) Policy Optimization: In this step, the LLM is fine-tuned by adjusting its parameters to maximize the predicted reward, using the Proximal Policy Op-timization (PPO) (Schulman et al., 2017) or Trust Region Policy Optimization (TRPO) (Schulman, 2015) algorithm. These two steps—rewarding and policy optimiza-tion—can be iterated, meaning that the process of generating outputs, rewarding them with the trained reward model, and fine-tuning the LLM to maxi-mize rewards can be repeated multiple times. With each iteration, the LLM’s performance improves as it refines its behavior to better align with human preferences. This iterative cycle allows the LLM to continuously adapt and optimize its responses, ultimately leading to more effective and aligned outputs. 5 RLAIF: Reinforcement Learning from AI Feedback Reinforcement learning from AI feedback (RLAIF) serves as a promising alternative or supplement to RLHF that leverages AI systems—often more pow-erful or specialized LLMs (e.g., GPT-4 (OpenAI, 2024a))—to provide feedback on the outputs of the LLM being trained. This approach provides benefits such as scalability, consistency, and cost efficiency while minimizing reliance on human evaluators. Below, we explore several methods for substituting human feedback with AI feedback in reinforcement learning, highlighting approaches: (1) Distilling AI Feedback to Train Reward Model, (2) Prompting LLMs As a Reward Function, and (3) Self-Rewarding. 5.1 Distilling AI Feedback to Train Reward Model Beyond manually collected data, distilling datasets from pre-trained LLMs presents an efficient alterna-tive. By leveraging the outputs of powerful LLMs like GPT-4, researchers can build a bridge between manual curation and autonomous evaluation. UltraFeedback (Cui et al., 2023). UltraFeed-back is a large-scale AI feedback dataset aimed at improving the performance and alignment of large language models (LLMs). It includes over 1 million high-quality GPT-4 feedback annotations across 250,000 user-assistant interactions, focusing on key dimensions like instruction adherence, accu-racy, honesty, and usefulness. The dataset was cre-ated by collecting 60,000 diverse instructions, gen-erating responses using 17 different models, and leveraging GPT-4 for detailed critiques and scor-ing, wherein chain-of-thought reasoning is used to reduce bias. Magpie. Xu et al. (2024b) introduce a self-synthesis method that leverages the autoregressive nature of aligned LLMs. By utilizing predefined templates as prompts, the model autonomously gen-erates user queries and corresponding responses, eliminating the need for manual intervention or initial seed questions. Specifically, as shown in Fig-ure 4, aligned LLMs (e.g., Llama-3-Instruct model) is employed to synthesize 4 million instruction-response pairs, subsequently filtering the dataset to retain 300,000 high-quality pairs. These pairs were then used to fine-tune the Llama-3-8B-Base model. Remarkably, the fine-tuned model achieved per-formance comparable to the official Llama-3-8B-Instruct model, which had undergone training on 10 million examples through supervised fine-tuning and reinforcement learning with human feedback. Besides, models fine-tuned with Magpie excelled on alignment benchmarks such as AlpacaEval, sur-passing models trained on other open datasets and preference optimization methods. HelpSteer2 (Wang et al., 2024e). HelpSteer2 is an efficient, open-source preference dataset com-prising approximately 10,000 comparison samples, designed to train high-performance reward mod-els. The dataset is built using responses gener-ated by various models (including GPT-3.5, Claude, and others) and features multi-dimensional anno-tations such as fluency, relevance, creativity, and safety. Preference pairs are crafted based on human or automated evaluations, enabling fine-grained alignment for reward models. Through rigorous data cleaning and optimization, HelpSteer2 deliv-ers high-quality annotations in a compact format. It is released under the CC-BY-4.0 license, fostering the accessibility. WildChat > OpenHermes > Tulu V2 Mix > UltraFeedback > ShareGPT > Magpie -Air > Magpie -Pro > Llama -3-Instruct > 5% > 10 % > 25.08 > 22.66 > 9.94 9.91 9.73 > 10.90 > 18.36 > 22.92 > 15% > 20% > 25% > 30% > Step 1<|start_header_id|>user > <|end_header_id|> > LLM > <|start_header_id|>user > <|end_header_id|> > What materials should I > use to build a nest? > <|start_header_id|> > assistant<|end_header_id|> > Building a nest! That’s a > wonderful project! …… > Instruction > Response > Instruction: What materials > should I use to build a nest? > Response: Building a nest! > That’s a wonderful project! > …… > What materials should I > use to build a nest? > Step 2 > SFT Only > SFT + DPO > SFT + RLHF > Filters SFT AlpacaEval 2 > (Length Control ) > MAGPIE > Evol Instruct > 14.62 > Length Control W in Rate > “” Figure 1: This figure illustrates the process of self-synthesizing instruction data from aligned LLMs (e.g., Llama-3-8B-Instruct) to create a high-quality instruction dataset. In Step 1, we input only the pre-query template into the aligned LLM and generate an instruction along with its response using auto-regressive generation. In Step 2, we use a combination of a post-query template and another pre-query template to wrap the instruction from Step 1, prompting the LLM to generate the query for the second turn. This completes the construction of the instruction dataset. M AGPIE efficiently generates diverse and high-quality instruction data. Our experimental results show that M AGPIE outperforms other public datasets for aligning Llama-3-8B-base. [14 , 26 , 64 , 65 , 66 ], which is both time-consuming and labor-intensive [ 37 ]. In contrast, the second type of method uses LLMs to produce synthetic instructions [ 16 , 31 , 46 , 47 , 53 , 55 , 58 , 59 ]. Although these methods reduce human effort, its success heavily depends on prompt engineering and the careful selection of initial seed questions. The diversity of synthetic data tends to decrease as the dataset size grows. Despite ongoing efforts, the scalable creation of high-quality and diverse instruction datasets continues to be a challenging problem. Is it possible to synthesize high-quality instructions at scale by directly extracting data from advanced aligned LLMs themselves? A typical input to an aligned LLM contains three key components: the pre-query template, the query, and the post-query template. For instance, an input to Llama-2-chat could be “ [INST] Hi! [/INST] ”, where [INST] is the pre-query template and [/INST] is the post-query template. These templates are predefined by the creators of the aligned LLMs to ensure the correct prompting of the models. We observe that when we only input the pre-query template to aligned LLMs such as Llama-3-Instruct, they self-synthesize a user query due to their auto-regressive nature. Our preliminary experiments indicate that these random user queries are of high quality and great diversity, suggesting that the abilities learned during the alignment process are effectively utilized. Based on these findings, we developed a self-synthesis method to construct high-quality instruction datasets at scale, named M AGPIE (as illustrated in Figure 1). Unlike existing methods, our approach does not rely on prompt engineering or seed questions. Instead, it directly constructs instruction data by prompting aligned LLMs with a pre-query template for sampling instructions. We applied this method to the Llama-3-8B-Instruct and Llama-3-70B-Instruct models, creating two instruction datasets: M AGPIE -Air and M AGPIE -Pro, respectively. Our M AGPIE -Air and M AGPIE -Pro datasets were created using 206 and 614 GPU hours, respectively, without requiring any human intervention or API access to production LLMs like GPT-4. Addi-tionally, we generated two multi-turn instruction datasets, M AGPIE -Air-MT and M AGPIE -Pro-MT, which contain sequences of multi-turn instructions and responses. The statistics and advantages of our instruction datasets compared to existing ones are summarized in Table 1. We perform a comprehensive analysis of the generated data, allowing practitioners to filter and select data instances from these datasets for fine-tuning according to their particular needs. To compare M AGPIE data with other public instruction datasets (e.g., ShareGPT [ 10 ], WildChat [ 64 ], Evol Instruct [ 58 ], UltraChat [ 16 ], OpenHermes [ 49 ], Tulu V2 Mix [ 24 ]) and various preference tuning strategies with UltraFeedback [ 13 ], we fine-tune the Llama-3-8B-Base model with each dataset and assess the performance of the resultant models on LLM alignment benchmarks such as AlpacaEval 2 [ 33 ], Arena-Hard [ 32 ], and WildBench [ 34 ]. Our results show that models fine-tuned with M AGPIE achieve superior performance, even surpassing the official Llama-3-8B-Instruct model on AlpacaEval, which was fine-tuned with over 10 million data points for supervised fine-tuning (SFT) and follow-up feedback learning. Not only does M AGPIE excel in SFT alone compared to prior public datasets that incorporate both SFT and preference optimization (e.g., direct preference Figure 4: Magpie self-synthesizes data from aligned LLMs. The figure is borrowed from Xu et al. (2024b). OffsetBias (Park et al., 2024). OffsetBias is a meticulously designed dataset aimed at mitigat-ing biases in reward models, constructed using re-sponses generated by diverse models, including GPT-3.5, GPT-4, Claude, and open-source models like Llama 2. As shown in Figure 5, OffsetBias systematically addresses six identified bias types, namely, content, style, informativeness, safety, cre-ativity, and length. Based on this, comparison samples are generated through attribute-controlled prompts and multi-model outputs. These samples are annotated with multi-dimensional scores and preference labels to highlight or neutralize biases, enabling fine-grained alignment. OffsetBias serves as a robust resource for improving the fairness and reliability of reward models, with its data openly accessible for research and development. 5.2 Prompting LLMs As a Reward Function As reward model training becomes more sophis-ticated, a natural progression is to employ LLMs themselves as evaluators in the loop of reinforce-ment learning. Exploring with LLMs (ELLM) Rewards (Du et al., 2023). ELLM is a method that integrates LLMs with reinforcement learning (RL) to enhance exploration during the pretraining phase. Figure 6 showcases the overall pipeline: the agent’s cur-rent state is transformed into a natural language description, which is input into the LLM. The LLM then generates exploration goals based on this state description, such as specific actions or target loca-tions. The RL agent attempts to achieve these goals, and rewards are provided by the environment upon goal completion. This approach improves explo-ration efficiency by guiding the agent toward areas of the state space that are likely to be valuable, without requiring pre-designed rewards. ELLM is particularly useful in sparse-reward environments. Compared to traditional methods, ELLM signifi-cantly improves exploration efficiency, covering more common-sense behaviors and providing bet-ter initialization for downstream tasks. Reward Design with Language Models (RDLM). Kwon et al. (2023) leverage a LLM like GPT-3 to simplify reward function design in reinforce-ment learning by allowing users to define desired behaviors through natural language descriptions. Specifically, users provide a task description or a few examples, and the LLM generates reward signals by evaluating the agent’s behavior against these criteria. Instead of producing reward code, RDLM outputs direct reward values that the RL agent uses for policy optimization. This method is ideal for tasks where user goals are clear but manually designing a reward function is complex. While ELLM focuses on guiding exploration dur-ing pretraining by generating meaningful goals, RDLM emphasizes task-specific reward generation to streamline complex reward design and achieve better agent alignment with human intent. Eureka (Ma et al., 2023). Eureka is an algorithm that leverages LLMs to automatically generate and optimize reward function code for reinforcement learning tasks. In Figure 7, first, a coding LLM like GPT-4 is used to generate initial reward func-tion code based on task descriptions. This code is then iteratively refined using evolutionary strate-gies, where candidate reward functions are evalu-ated based on how well they guide the RL agent toward task success. The process evolves the re-ward functions to improve their quality and effec-tiveness. Eureka is particularly effective in tasks requiring complex or highly specific reward defini-Figure 5: Identified bias types and examples in OffsetBias. The figure is borrowed from Park et al. (2024). forcement Learning with Large Language Models Zihan Wang 2 C´ edric Colas 3 4 Trevor Darrell 1 Pieter Abbeel 1 Abhishek Gupta 2 Jacob Andreas 3 ically haped d explo-reward-sitions, in large ovelty is scribe a ge from method, rewards an anguage agent’s anguage s toward ul behav-p. We nvironment , showing overage training nce on 1. Cut down the tree. 2. Craft a pickaxe. 3. Eat cow. 4. Sleep. . . .k. Build a wood house. > You see trees, cows, grass, table, and bushes. You have wood in your inventory. You feel hungry, thirsty, and sleepy. > LLM > Prompt: > What should you do next? Figure 1: ELLM uses a pretrained large language model (LLM) to suggest plausibly useful goals in a task-agnostic way. Building on LLM capabilities such as context-sensitivity and common-sense, ELLM trains RL agents to pursue goals that are likely meaningful without requiring direct human intervention. Prompt is illustrative; see full prompt and goal format in Appendix D. complex tasks in practice, RL agents may therefore need to learn some behaviors in the absence of externally-defined rewards. What should they learn? Figure 6: The figure is copied from (Du et al., 2023). tions, such as advanced robotic skills. Its focus on reward code optimization makes it suitable for sce-narios where precise reward shaping is critical. By utilizing LLMs’ ability to generate and refine code, Eureka evolves reward functions that effectively guide RL agents. Experiments demonstrate that Eureka outperforms human-designed rewards in 83% of tested tasks, with an average performance improvement of 52%, showcasing its potential for advanced skill learning, such as robotics tasks, in challenging scenarios. Text2Reward (Xie et al., 2023). Text2Reward is a framework that leverages large language models to automatically generate dense and interpretable reward function code from natural language task de-scriptions, enabling efficient reward shaping across Published as a conference paper at ICLR 2024 > Figure 2: EUREKA takes unmodified environment source code and language task description as context to zero-shot generate executable reward functions from a coding LLM. Then, it iterates between reward sampling, GPU-accelerated reward evaluation, and reward reflection to progressively improve its reward outputs. domain expertise to construct task prompts or learn only simple skills, leaving a substantial gap in achieving human-level dexterity (Yu et al., 2023; Brohan et al., 2023). On the other hand, reinforcement learning (RL) has achieved impressive results in dexter-ity (Andrychowicz et al., 2020; Handa et al., 2023) as well as many other domains-if the human designers can carefully construct reward functions that accurately codify and provide learning signals for the desired behavior; likewise, many real-world RL tasks admit sparse rewards that are difficult for learning, necessitating reward shaping that provides incremental learning signals. Despite their fundamental importance, reward functions are known to be notoriously difficult to design in prac-tice (Russell & Norvig, 1995; Sutton & Barto, 2018); a recent survey conducted finds 92% of polled reinforcement learning researchers and practitioners report manual trial-and-error reward design and 89% indicate that their designed rewards are sub-optimal (Booth et al., 2023) and lead to unintended behavior (Hadfield-Menell et al., 2017). Given the paramount importance of reward design, we ask whether it is possible to develop a universal reward programming algorithm using state-of-the-art coding LLMs, such as GPT-4. Their remarkable abilities in code writing, zero-shot generation, and in-context learning have previously enabled effective programmatic agents (Shinn et al., 2023; Wang et al., 2023a). Ideally, this reward design algorithm should achieve human-level reward generation capabilities that scale to a broad spectrum of tasks, including dexterity, automate the tedious trial-and-error procedure without human supervision, and yet be compatible with human oversight to assure safety and alignment. We introduce Evolution-driven Universal RE ward Kit for Agent ( EUREKA ), a novel reward design algorithm powered by coding LLMs with the following contributions: 1. Achieves human-level performance on reward design across a diverse suite of 29 open-sourced RL environments that include 10 distinct robot morphologies, including quadruped, quadcopter, biped, manipulator, as well as several dexterous hands; see Fig. 1. Without any task-specific prompting or reward templates, EUREKA autonomously generates rewards that outperform expert human rewards on 83% of the tasks and realizes an average normalized improvement of 52% .2. Solves dexterous manipulation tasks that were previously not feasible by manual reward engineering . We consider pen spinning, in which a five-finger hand needs to rapidly rotate a pen in pre-defined spinning configurations for as many cycles as possible. Combining EUREKA with curriculum learning, we demonstrate for the first time rapid pen spinning maneuvers on a simulated anthropomorphic Shadow Hand (see Figure 1 bottom). 3. Enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF) that can generate more performant and human-aligned reward functions 2Figure 7: The overall pipeline of Eureka. The figure is borrowed from Ma et al. (2023). > Figure 8: An overview of Text2Reward. The figure is copied from Xie et al. (2023). diverse RL tasks. As shown in Figure 8, the pro-cess starts with users providing a task description in natural language, which is input into an LLM to generate executable reward code. This code often includes task-specific logic and may integrate exter-nal libraries for complex functionalities. The gen-erated reward function is then used in RL to guide the agent’s behavior. Additionally, Text2Reward supports iterative refinement of the reward code through human feedback, enabling further opti-mization. This method excels at providing flex-ible, interpretable rewards across diverse RL tasks, particularly in robotics and manipulation. Unlike Eureka, evolving and optimizing reward function code through LLMs and evolutionary algorithms, Text2Reward emphasizes creating human-readable reward code that integrates external libraries and supports iterative refinement via human feedback. While both methods aim to automate reward design, Eureka excels in optimizing complex reward logic for advanced skills, whereas Text2Reward prior-itizes flexibility, interpretability, and adaptability for a broad range of tasks. RLAIF. Lee et al. (2023) replace human feed-back in RL with AI-generated feedback by lever-aging LLMs. The process begins with generating candidate outputs for a given task, such as text summarization or dialogue generation. These out-puts are paired and fed into an LLM, which eval-uates them and provides preferences (e.g., select-ing the better output) or assigns scores based on task-specific criteria. This feedback is then used Generative Verifiers: Reward Modeling as Next-Token Prediction “Let’s verify step by step.” GenRM > Finetuned Verifier > Problem > Solution > “Is the answer correct (Yes/No)?” Yes > No > Other tokens GenRM-CoT > Finetuned Verifier Problem > Solution > Token Probability > Verification CoT 1 … > Verification CoT N > No > Yes > Yes > 0.4 > 0.2 > 0.9 > 0.8 > Average > r > r Figure 3 | An illustration of generative verifiers , namely GenRM and GenRM -CoT. Given a question and a candidate solution, GenRM directly finetunes an LLM to answer the question ‘Is the answer correct (Yes/No)?’ via SFT on the next-token response corresponding to either ‘Yes’ or ‘No’. During inference, the verifier score is obtained by extracting the probability of the ‘Yes’ token (4). In comparison, GenRM -CoT finetunes a LLM to produce verification chain-of-thought (CoT) rationale before yielding the final Yes/No token. At test-time, we sample multiple CoT rationales and use majority voting to compute the average probability of ‘Yes’, enabling GenRM-CoT to utilize additional inference-compute for better verification. Our results show that GenRM outperforms discriminative RMs, LLM-as-a-Judge, and self-consistency on algorithmic string manipulation and math reasoning tasks (Figure 1). Best-of-N performance further improves with GenRM -CoT that uses majority-voting, nearly matching performance with oracle verifier on algorithmic tasks. On GSM8K, when using a Gemma2-9B GenRM -CoT verifier on solutions from Gemini 1.0 Pro, we observe an improvement from 73% → 93 .4% in terms of the number of problems solved, surpassing GPT-4 and Gemini 1.5 Pro. Furthermore, GenRM-CoT trained on grade-school math problems exhibit easy-to-hard generalization, solving 17% more high-school competition problems in MATH500 (Lightman et al., 2023) with Best-of-32. Moreover, we find that generative verifiers scale more favorably than discriminative verifiers as we increase model capacity, and outperform LLM-as-a-Judge as we scale inference-time compute with majority voting. Overall, generative verifiers hold significant potential for improving the reasoning capabilities of LLMs. 2. Preliminaries An autoregressive language model generates an output sequence y = ( 𝑦 1, 𝑦 2, . . . , 𝑦 𝑇 ) given a input context x (e.g., math problem) by predicting tokens one at a time, based on the previously generated tokens. Assuming that the language model is parameterized by 𝜃 , the conditional probability distribution of generating a sequence y given context x is 𝑝 𝜃 (y | x) = > 𝑇 Ö > 𝑡 =1 𝑝 𝜃 ( 𝑦 𝑡 | x, 𝑦 <𝑡 ) (1) with the convention 𝑦 <1 = ∅ and y<𝑡 = ( 𝑦 1, 𝑦 2, . . . , 𝑦 𝑡 −1). For ease of notation, we define 𝑝 𝜃 ( 𝑦 𝑡 | x) := 𝑝 𝜃 ( 𝑦 𝑡 | y<𝑡 , x). For a vocabulary size 𝑀 , the probability of predicting the 𝑡 -th token 𝑦 𝑡 , 𝑝 𝜃 ( 𝑦 𝑡 | x),is determined using a softmax with temperature 𝛾 on logit scores 𝑧 of all the tokens: 𝑝 𝜃 ( 𝑦 𝑡 | x) = > exp (𝑧 𝑡 /𝛾 ) > Í𝑀 𝑖 =1exp (𝑧 𝑖 /𝛾 ) , where 𝑧 𝑡 = logit 𝜃 ( 𝑦 𝑡 | x, y<𝑡 ). Higher values of temperature 𝛾 introduce more randomness, while setting temperature 𝜏 = 0 makes the output deterministic, which corresponds to greedy decoding. > 3 Figure 9: Illustration of GenRM. The figure is copied from Zhang et al. (2024a). RLAIF vs. RLHF s - - - - - - - > \- - - - - - - - > \- - - - - - - - > SFT > Model > General > Purpose > LLM > Prompt to Reward > 1-10 > RL model > Reinforcement > Learning > Reward Figure 4: In direct-RLAIF (d-RLAIF), the off-the-shelf LLM is directly used to provide rewards during RL, circum-venting the issue of RM “staleness” and the time consuming process of RM training. 2.2. Reinforcement Learning from AI Feedback 2.2.1. C ANONICAL RLAIF We describe our adaptation of the canonical RLAIF setup below. Unless otherwise mentioned, RLAIF is carried out using this method. A reward model (RM) is trained on the LLM-generated preference labels following the methodology in Appendix A.2. Since our approach produces soft labels (e.g. [0 .6, 0.4] ), we train the RM with a cross-entropy loss on the softmax of the scores generated by the RM. The softmax converts the RM scores into a probability distribution. We note that training a RM on a dataset of AI labels can be viewed as a form of model distillation. Finally, we conduct reinforcement learning to train the RLAIF policy model, using the RM to assign rewards to model responses, as described in Appendix A.3. 2.2.2. D IRECT -RLAIF ( D-RLAIF) One issue with RLAIF is that the reward model may become “stale” as the policy is trained. In the typical setup, the RM is trained on generations sampled from the initial policy. As the policy is trained, the generated trajectories become increasingly out-of-distribution from the dataset the RM was trained on, leading to suboptimal performance (Bai et al., 2022a). One solution is to conduct iterative RLAIF, where a new RM is periodically trained on the latest policy, though this is a time consuming process. We introduce direct-RLAIF (d-RLAIF) - a simple alternative to canonical RLAIF that directly uses LLM feedback as the reward signal in RL. D-RLAIF addresses the RM staleness issue, as the off-the-shelf LLM directly scores generated the prompt instructs the LLM on how to rate a generation. Then, the likelihood of each score token between 1 and 10 is computed, the likelihoods are normalized to a probability distribution, a weighted score is calculated as s(y|x) = ∑10 > i=1 iP (i|y, x ), and finally the score is again normalized to the range [−1, 1] . Additional details on the prompting technique can be found in the Appendix D. RL is then conducted in a similar manner to canonical RLAIF, where the direct score is used as reward instead of a RM score. 2.3. Evaluation We evaluate our results with three metrics - AI Labeler Alignment , Win Rate , and Harmless Rate . AI Labeler Alignment measures the accuracy of AI-labeled preferences with respect to human preferences. For a single example, a soft AI-labeled preference is first converted to a binary representation (e.g. [0 .6, 0.4] → [1 , 0] ). Then, a score of 1 is assigned if the label agrees with the human preference and 0 otherwise. The alignment accuracy zacc can be expressed as follows: zacc = 1 D > D ∑ > i=1 1[arg max > j P AI i,j = pHi ], where D is the size of the preference dataset, P AI ∈ RD×2 is the matrix of soft AI preferences, and pH ∈ RD is the corresponding vector of human preferences, containing ele-ments 0 or 1 to denote whether the first or second response is preferred, respectively. Win Rate evaluates the end-to-end quality of two policies by measuring how often one policy is preferred by human annotators over another. Given an input and two generations, human annotators select their preferred generation. The percentage of instances where policy A is preferred over policy B is referred to as the “win rate of A vs. B” . A 50% win rate indicates that A and B are equally preferred. Harmless Rate measures the percentage of responses that are considered harmless by human evaluators. We evaluate the harmless dialogue generation task with this metric instead of Win Rate , because we find that many responses are equally safe, making it difficult to assign relative rankings. Figure 10: The figure is borrowed from Lee et al. (2023). to train a reward model that predicts the quality of outputs and guides the RL agent. In its stream-lined variant, d-RLAIF (see Figure 10), the LLM directly provides scores as reward signals, bypass-ing the need for a reward model. The RL policy is optimized using these rewards, typically with al-gorithms like Proximal Policy Optimization (PPO). This approach enables automated, scalable, and high-quality feedback generation, effectively align-ing RL agent behavior with task objectives while reducing reliance on human annotations. GenRM. Zhang et al. (2024a) re-define verifica-tion by treating it as a text generation task, lever-aging large language models to produce valida-tion outputs and reasoning chains, such as "yes" or "no" with explanations. As shown in Figure 9, this approach integrates verification into the gen-erative capabilities of LLMs, enabling them to as-sess and explain candidate answers in a transparent and interpretable manner. By framing verification as next-token prediction, GenRM eliminates re-liance on traditional discriminative models and en-hances reasoning accuracy. Experimental results demonstrate its ability to outperform conventional methods, showcasing its potential in tasks requir-ing logical reasoning, interpretability, and scalable performance. 5.3 Self-Rewarding The self-rewarding mechanism enables the LLM to autonomously assess and refine its own perfor-mance, addressing the cost, scalability, and adapt-ability limitations of existing RL methods. Self-Refined LLM. Song et al. (2023) leverage LLMs to automatically generate reward functions for deep reinforcement learning (DRL) tasks and introduces a self-optimization mechanism to iter-atively refine these functions. The process begins with the LLM generating an initial reward function based on natural language task descriptions. The reward function is then applied to RL training, and the agent’s performance is evaluated. Feedback from this evaluation is fed back into the LLM, en-abling it to dynamically adjust and improve the reward function in a closed-loop manner. Com-pared to Eureka and Text2Reward, this approach eliminates the need for external optimization algo-rithms or manual intervention. Self-Rewarding Language Models (SRLM). Yuan et al. (2024) introduce a novel approach where LLMs act as both the generator and eval-uator to create a self-contained learning system. As shown in Figure 11, the model begins by generat-ing new prompts (instructions) and multiple candi-date responses derived from existing data, thereby creating a diverse and comprehensive set of train-ing samples. Subsequently, the model evaluates these candidate responses using a structured scor-ing mechanism to determine their quality. The eval-uation framework encompasses multiple dimen-sions, including relevance, coverage, usefulness, clarity, and professionalism, assigning a score to Generate responses Generate rewards Preference pairs DPO training select Generated new prompts Self-Instruction creation Instruction following training Next iteration model Seed model > (for t=1) Figure 1: Self-Rewarding Language Models. Our self-alignment method consists of two steps: (i) Self-Instruction creation : newly created prompts are used to generate candidate responses from model Mt, which also predicts its own rewards via LLM-as-a-Judge prompting. (ii) Instruction following training : preference pairs are selected from the generated data, which are used for training via DPO, resulting in model Mt+1 . This whole procedure can then be iterated resulting in both improved instruction following and reward modeling ability. Starting from a seed model, in each iteration there is a process of Self-Instruction creation whereby candidate responses are generated by the model for newly created prompts, and are then assigned rewards by that same model. The latter is implemented via LLM-as-a-Judge prompting, which can also be seen as an instruction following task. A preference dataset is built from the generated data, and the next iteration of the model is trained via DPO, see Figure 1. In our experiments, we start with a Llama 2 70B [Touvron et al., 2023] seed model fine-tuned on Open Assistant [Köpf et al., 2023], and then perform the above training scheme. We find that not only does the instruction following performance improve from Self-Rewarding LLM alignment compared to the baseline seed model, but importantly the reward modeling ability, which is no longer fixed, improves as well. This means that the model during iterative training is able, at a given iteration, to provide a higher quality preference dataset to itself than in the previous iteration. While this effect likely saturates in real-world settings, it provides the intriguing possibility of obtaining reward models (and hence LLMs) that are superior to ones that could have been trained from the original human-authored seed data alone. ## 2 Self-Rewarding Language Models Our approach first assumes access to a base pretrained language model, and a small amount of human-annotated seed data. We then build a model that aims to possess two skills simultaneously: 1. Instruction following : given a prompt that describes a user request, the ability to generate a high quality, helpful (and harmless) response. 2. Self-Instruction creation : the ability to generate and evaluate new instruction-following examples to add to its own training set. These skills are used so that the model can perform self-alignment, i.e., they are the components used to iteratively train itself using AI Feedback (AIF). Self-instruction creation consists of generating candidate responses and then the model itself judging their quality, i.e., it acts as its own reward model, replacing the need for an external one. This is implemented via the LLM-as-a-Judge mechanism [Zheng et al., 2023b], i.e., by formulating the evaluation of responses as an instruction following task. This self-created AIF preference data is used as a training set. Our overall self-alignment procedure is an iterative one, which proceeds by building a series of such models, with the aim that each improves over the last. Importantly, because the model can both improve its generation ability, and act as its own reward model through the same generation mechanism, this means the reward model itself can improve through these iterations, deviating from standard practices where the reward model is fixed [Ouyang et al., Figure 11: The overview of SRLM. The figure is copied from Yuan et al. (2024). Review the user’s question and the corresponding response using the additive 5-point scoring system described below. Points are accumulated based on the satisfaction of each criterion: - Add 1 point if the response is relevant and provides some information related to the user’s inquiry, even if it is incomplete or contains some irrelevant content. - Add another point if the response addresses a substantial portion of the user’s question, but does not completely resolve the query or provide a direct answer. - Award a third point if the response answers the basic elements of the user’s question in a useful way, regardless of whether it seems to have been written by an AI Assistant or if it has elements typically found in blogs or search results. - Grant a fourth point if the response is clearly written from an AI Assistant’s perspective, addressing the user’s question directly and comprehensively, and is well-organized and helpful, even if there is slight room for improvement in clarity, conciseness or focus. - Bestow a fifth point for a response that is impeccably tailored to the user’s question by an AI Assistant, without extraneous information, reflecting expert knowledge, and demonstrating a high-quality, engaging, and insightful answer. User: <INSTRUCTION_HERE> > <response> <RESPONSE_HERE> </response> After examining the user’s instruction and the response: - Briefly justify your total score, up to 100 words. - Conclude with the score using the format: “Score: <total points>” Remember to assess from the AI Assistant perspective, utilizing web search knowledge as necessary. To evaluate the response in alignment with this additive scoring model, we’ll systematically attribute points based on the outlined criteria. > Figure 2: LLM-as-a-Judge prompt for our LLM to act as a reward model and provide self-rewards for its own model generations. The model is initially trained with seed training data of how to perform well at this task, and then improves at this task further through our self-rewarding training procedure. Section 2.2), following Xu et al. [2023], discarding the pair if their scores are the same. These pairs can be used for training with a preference tuning algorithm. We use DPO [Rafailov et al., 2023]. > 2.4 Overall Self-Alignment Algorithm Iterative Training Our overall procedure trains a series of models M1, . . . , M Twhere each successive model tuses augmented training data created by the t−1th model. We thus define AIFT( Mt) to mean AI Feedback Training data created using model Mt. > Model Sequence We define the models, and the training data they use as follows: > M0: Base pretrained LLM with no fine-tuning. > M1: Initialized with M0, then fine-tuned on the IFT+EFT seed data using SFT. > M2: Initialized with M1, then trained with AIFT( M1) data using DPO. > M3: Initialized with M2, then trained with AIFT( M2) data using DPO. This iterative training resembles the procedure used in Pairwise Cringe Optimization and specifically is termed Iterative DPO, introduced in Xu et al. [2023]; however, an external fixed reward model was used in that work. 4 Figure 12: Prompt for LLM as a judge. The figure is borrowed from Yuan et al. (2024). each response based on these criteria. Utilizing these scores, preference pairs are constructed, con-sisting of a preferred response and a dispreferred response. These pairs are used for Direct Prefer-ence Optimization (DPO), improving its ability to generate high-quality responses. Through iterative refinement, the model progressively enhances its performance. Figure 12 provides a detailed expla-nation of the prompts used by the model to evaluate candidate responses. Experimental results demon-strate that fine-tuning Llama 2 70B using SRLM over three iterations outperforms several state-of-the-art models, including GPT-4 and Claude 2, on benchmarks like AlpacaEval 2.0, showcasing its effectiveness in improving instruction-following and general task performance. Generative Judge via Self-generated Con-trastive Judgments (Con-J). Ye et al. (2024) propose a self-rewarding mechanism with self-generated contrastive judgments, allowing LLMs to evaluate and refine their outputs by providing detailed, natural language rationales. As shown in Figure 13, unlike traditional scalar reward models that output a single numerical score, the Genera-tive Judge compares candidate outputs and gener-ates positive and negative evaluations with accom-panying explanations in natural language. This enables the model to assess why one output is preferable to another, providing interpretability and aligning its decisions with nuanced human prefer-ences. The framework is also trained using DPO on human-labeled preference data, where the LLM is prompted to produce contrastive rationales for paired outputs. These self-generated evaluations serve as both the reward signal and the basis for iterative refinement, enabling the model to improve its alignment with task objectives autonomously. In experiments, the Generative Judge achieved per-formance comparable to scalar reward models in aligning outputs with human preferences but ex-celled in interpretability and robustness to dataset biases. By leveraging contrastive judgments, the model demonstrated enhanced adaptability to tasks requiring multi-faceted reasoning and improved its capacity for transparent decision-making. 6 Analysis of RLHF/RLAIF While RLHF and RLAIF are effective methods for aligning LLMs with desired behaviors, there are still challenges that require careful analysis. These include addressing out-of-distribution issues between the trained reward models and the aligned LLMs, ensuring the interpretability of the model for humans, and maintaining safety and evaluation benchmarks to train robust reward models. In this section, we discuss recent works that tackle these challenges and provide strategies for overcoming them. Figure 13: Illustration of a scalar reward model and the proposed Con-J. The figure is copied from Ye et al. (2024). Response > Helpfulness > Correctness > Coherence > Complexity > Verbosity > Token Embedding > Decoder Layer > Regression Layer … > Decoder Layer ArmoRM Tokens > ✓ > 0.8 × > 0.6 × > 0× > 0× > -0.2 × Score ✅ ⬜ ⬜ ❌ ✅ Prompt ✓ > Gating Layer Figure 1: Architecture of our reward model. It consists of an LLM backbone, a regression layer for multi-objective reward modeling, and a gating layer that outputs coefficients to scalarize the reward objectives into a scalar score. This framework has achieved tremendous success in the post-training of ChatGPT [Ouyang et al., 2022] and Claude [Bai et al., 2022]. These ideas also extend to other approaches, such as rejection sampling fine-tuning [Dong et al., 2023; Gulcehre et al., 2023] and iterative direct preference learning [Xiong et al., 2023; Guo et al., 2024; Xie et al., 2024]. In these approaches, the intermediate policy is typically iteratively deployed to collect new responses, uses the reward model to label the responses, and fine-tunes the model on the newly collected preference data. In all of these RLHF frameworks, the capacity of the reward model is crucial as it directly affects the quality of the aligned LLMs. The most popular reward modeling approach is based on the maximum likelihood estimation (MLE) of the Bradley-Terry (BT) model [Bradley and Terry, 1952]. Despite its widespread use, the BT model is rather limited in the capacity of capturing the complicated human preference [Munos et al., 2023; Swamy et al., 2024; Ye et al., 2024]. In addition to the capacity issue, common RMs, like the BT model, are typically black-box models that output scores or preferences without providing human-interpretable explanations, making it subject to the widely observed phenomenon of reward hacking [Skalse et al., 2022; Singhal et al., 2023; Chen et al., 2024], where the aligned LLMs generate high-reward responses (rated by the RM) that do not align with actual human preferences [Gao et al., 2023; Lin et al., 2023; Coste et al., 2023]. A notable example of this is the verbosity bias, where aligned LLMs produce longer-than-necessary responses because the RM favors length, regardless of quality [Singhal et al., 2023; Wang et al., 2024a; Chen et al., 2024]. In this work, we aim to enhance reward models by making them more interpretable [Molnar, 2020] and steerable [Wong et al., 2021]. Using the aforementioned verbosity bias as an example, suppose the RM’s output is decomposable, meaning that it assigns a high score to a response due to two factors: 40% for its helpfulness and 60% for its length. In this case, we can see that the RM may suffer from the verbosity bias. Furthermore, if the RM is steerable, we could adjust its decision-making process to base its scoring 100% on helpfulness. This would be regardless of response length, Figure 14: Overview of ArmoRM. The figure is borrowed from Wang et al. (2024b). 6.1 Out of Distribution (OOD) Out-of-distribution (OOD) issues present a signif-icant challenge in reward modeling, particularly when the reward model and the large language model (LLM) are trained independently. This sep-aration can lead to inconsistencies in the knowl-edge and decision-making frameworks of the two models, potentially causing the reward model to encounter unfamiliar scenarios or fail to generalize effectively. Addressing OOD challenges is critical for ensuring that reward models (RMs) perform reliably across diverse inputs. Lou et al. (2024) point out that RMs often strug-gle when encountering OOD inputs, exhibiting a dangerous tendency toward overconfidence. This overconfidence stems from the models’ reliance on training data distributions, which may not account for the variability of real-world environments. They emphasized that traditional RMs lack mechanisms to quantify and act on uncertainty. By introducing uncertainty quantification, the proposed approach enables RMs to distinguish between "known" and "unknown" regions in the data space, ensuring more cautious and robust decision-making. More-over, the integration of contrastive learning and reg-ularization techniques further enhances the RM’s ability to handle OOD scenarios. Yang et al. (2024b) find that reward models fail-ing to generalize preferences when input texts con-tain novel combinations of known patterns or previ-ously unseen linguistic structures. To address this limitation, they proposed Generalizable Reward Model (GRM), which regularizes the hidden states of RMs during training, ensuring they preserve the underlying language understanding of LLMs. Ad-ditionally, a text-generation loss is introduced to maintain the balance between preference learning and the core generative capabilities of LLMs. The result is a reward model that is more adaptable to diverse inputs. 6.2 Human Interpretability Human interpretability is a crucial aspect of reward modeling, as it enables researchers and practition-ers to understand and trust the decisions made by the model. Reward models often produce discrete scores to evaluate LLM outputs, but the rationale behind these scores is not always transparent. En-hancing interpretability is vital for ensuring that the alignment process is comprehensible and reliable, particularly in sensitive applications where human preferences play a central role. ArmoRM. Wang et al. (2024b) argue that cur-rent reward models often conflate different objec-tives, making it difficult to discern which aspects of the input data influence their scoring. To ad-dress this, they proposed the ArmoRM (Absolute Rating Multi-Objective Reward Model). As illus-trated in Figure 14, the model processes a context and multiple candidate responses, evaluating them across interpretable dimensions such as honesty, safety, verbosity, and relevance. Each dimension is assessed by a dedicated sub-model that gener-ates individual scores. These scores are then dy-namically weighted by a gating network, which adapts to the context and produces a final reward score used as feedback for reinforcement learn-ing. This mixture-of-experts approach effectively separates the objectives, allowing the scores to be more clearly attributed to specific input features or goals, thus improving both interpretability and transparency. Quantile Reward Models (QRM). Dorka (2024) observe that traditional reward models typi-cally produce a single point estimate for rewards, which limits their ability to capture the diversity and complexity of human preferences. In contrast, they proposed QRM, which leverages quantile re-gression to estimate the full distribution of rewards, allowing for a richer representation of human feed-back. Figure 15 illustrates the architecture of the QRM: the LLM backbone processes the prompt and response, producing two types of embeddings: one for the gating network (prompt embedding) and another for the quantile regression layers (prompt-response embedding). The quantile regression lay-ers estimate the reward distribution for various attributes, such as helpfulness and harmlessness. Meanwhile, the gating network assigns weights to these attribute distributions. These weighted dis-tributions are then combined to produce the final reward distribution. This approach is particularly effective in handling noisy labels and conflicting preferences, as it models such uncertainties as dis-tinct modes within the reward distribution. By es-timating a complete distribution, QRMs enhance interpretability in decision-making, such as focus-ing on lower quantiles for risk-averse tasks or upper quantiles for exploration. General Preference Representation Model (GPM). Zhang et al. (2024c) emphasize the im-portance of structured preference representations in improving interpretability. The proposed pref-erence representation learning method enhances interpretability by embedding human preferences into a latent space, which provides a structured and transparent way to model complex relationships. Instead of relying on traditional point-based scor-ing systems, this approach maps preferences into a continuous space, where each dimension repre-sents a specific attribute or characteristic, such as relevance or coherence. This allows for clear ex-planations of why certain responses are preferred based on their positions within the space. For ex-ample, a response might rank higher due to its con-ciseness, and this preference can be directly traced to its alignment with the "conciseness" dimension in the latent space. Unlike traditional methods, which struggle with intransitive or cyclic prefer-ences, preference embeddings naturally capture these nuanced relationships. By visualizing or inter-preting how responses relate to one another across multiple dimensions, the method avoids forcing a linear ranking and instead reflects the true com-plexity of human feedback. Additionally, the latent representations adapt dynamically to different con-texts, making it possible to explain preferences based on the specific attributes relevant to the sit-uation. For instance, a humorous response might be preferred in one scenario, while informative-ness could dominate in another, and the model can attribute the preference to these varying factors. 6.3 Safety Safety is a critical consideration for reward models, especially when dealing with potentially harmful or biased inputs. As reward models guide the op-timization of LLMs, their handling of sensitive or Figure 15: Illustration of QRM. The figure is copied from Dorka (2024). > Figure 16: The overall framework of Quark. The figure is copied from Lu et al. (2022). adversarial content plays a significant role in en-suring that the outputs generated by LLMs align with ethical and safety standards. This subsection explores the challenges and recent advancements in enhancing the safety of reward models. Safe RLHF (Dai et al., 2023). When aligning LLMs with human values, Safe RLHF emphasizes both helpfulness and harmlessness. Safe RLHF uses a structured method to balance these two ob-jectives by decoupling human preference annota-tions into two distinct objectives: a reward model for helpfulness and a cost model for harmlessness. This decoupling is achieved by independently an-notating helpfulness and harmlessness on collected response data, with each response evaluated sepa-rately for these aspects. In the Safe RL phase, the method seeks to maxi-mize expected rewards (helpfulness) while enforc-ing cost constraints (harmlessness) through a La-grangian approach, where an unconstrained objec-tive can be formulated as: min > θ max > λ≥0 [−JR(θ) + λ · JC (θ)] , (1) where JR(θ) is the reward objective, JC (θ) is the cost objective, and λ is dynamically adjusted as a Lagrange multiplier to balance helpfulness and harmlessness adaptively during training. The method iteratively updates both model parameters θ and the Lagrange multiplier λ, with each round of Safe RLHF training adjusting λ to reflect recent feedback on the safety constraints. Quantized Reward Konditioning (Quark). Lu et al. (2022) provide a framework Quark for ad-dressing harmful content by equipping reward mod-els with mechanisms to identify and unlearn unsafe outputs. The "unlearning" aspect of the Quark al-gorithm is reflected in its ability to adjust the gener-ative tendencies of a language model through rein-forcement learning, gradually "forgetting" undesir-able traits such as toxicity, repetition, or negative sentiment. The algorithm evaluates generated sam-ples using a reward function, marking low-quantile samples as traits that the model needs to suppress, and progressively weakens these tendencies during the fine-tuning process through conditional gen-eration. Additionally, the reinforcement learning Generate Responses to “Red Teaming” Prompts Eliciting Harmful Samples > Generate Responses > to “Red Teaming” > Prompts Eliciting Harmful Samples > RLAIF > Training > with PM + SL-CAI Models > Constitutional AI Feedback > for Self-Improvement > Helpful RLHF Model > Generate Responses > to “Red Teaming” > Prompts Eliciting Harmful > Samples > Generate Responses > to “Red Teaming” > Prompts Eliciting Pairs of Samples > Finetuned > Preference > Model (PM) > Finetuned > SL-CAI > Model > Final > RL-CAI > Model > Response > Critique > Revision Figure 1 We show the basic steps of our Constitutional AI (CAI) process, which consists of both a super-vised learning (SL) stage, consisting of the steps at the top, and a Reinforcement Learning (RL) stage, shown as the sequence of steps at the bottom of the figure. Both the critiques and the AI feedback are steered by a small set of principles drawn from a ‘constitution’. The supervised stage significantly improves the initial model, and gives some control over the initial behavior at the start of the RL phase, addressing potential exploration problems. The RL stage significantly improves performance and reliability. 1 Introduction We would like to train AI systems that remain helpful, honest, and harmless, even as some AI capabilities reach or exceed human-level performance. This suggests that we will need to develop techniques that do not rely on humans to supervise all aspects of AI behavior, and that can be used to automatically test and enhance robustness to harmful behaviors. We also aim to develop methods that encode desirable AI behavior in a simple and transparent form, and that make it easier to understand and evaluate AI decision making. In this paper we develop a method we refer to as Constitutional AI (CAI), depicted in Figure 1, and use it to train a non-evasive and relatively harmless AI assistant, without any human feedback labels for harms .The method therefore improves upon, and partially replaces reinforcement learning from human feedback [Christiano et al., 2017]. The new assistant ‘RL-CAI’ is preferred by crowdworkers over those trained with previously collected [Bai et al., 2022, Ganguli et al., 2022] human feedback labels for harmfulness. We chose the term ‘constitutional’ because we are able to train less harmful systems entirely through the specification of a short list of principles or instructions, i.e. a constitution. But we are also employing this terminology to emphasize that when developing and deploying a general AI system, we cannot avoid choosing some set of principles to govern it, even if they remain hidden or implicit. Our motivations for developing this technique were: (1) to study simple possibilities for using AI systems to help supervise other AIs, and thus scale supervision , (2) to improve on our prior work training a harmless AI assistant by eliminating evasive responses , reducing tension 1 [Bai et al., 2022, Glaese et al., 2022] between helpfulness and harmlessness and encouraging the AI to explain its objections to harmful requests, (3) to make the principles governing AI behavior, and their implementation, more transparent, and (4) to reduce iteration time by obviating the need to collect new human feedback labels when altering the objective. Let us discuss these motivations in more detail. 1.1 Motivations Scaling Supervision We use the term ‘Scaling Supervision’ for techniques that leverage AI to help humans to more efficiently supervise AI, making it possible to train systems to behave in desirable ways (e.g. to be helpful, honest, and > 1That is, helpfulness tends to increase harmfulness, since models are willing to obey pernicious requests, and con-versely models trained to be harmless tend to be more evasive and generally less helpful. By harmfulness we in-clude both a variety of forms of harm to the user and responses that help the user to achieve harmful aims. See [Bai et al., 2022, Ganguli et al., 2022] for more discussion of our operational definitions of helpful and harmless. 2Figure 17: The overview of Constitutional AI (CAI) process. The figure is borrowed from Bai et al. (2022). objective incorporates both the attenuation of low-quantile tendencies and the enhancement of high-reward objectives, reducing the model’s reliance on undesirable traits. By leveraging reward quantiles to guide the process, Quark effectively "unlearns" existing biases in the model, ultimately enabling the generation of high-quality text that aligns with desired goals. Constitutional AI. Bai et al. (2022) introduce a novel approach to guiding AI behavior through pre-defined principles, referred to as a "constitution," enabling the training of harmless and transparent AI assistants without relying heavily on human-labeled data. The central idea is that AI can self-assess and refine its outputs based on these princi-ples, ensuring safety and alignment with desired goals. The process involves two key phases: a su-pervised learning phase and a reinforcement learn-ing phase. During the supervised phase, the model generates initial responses, critiques them based on constitutional principles, and refines its outputs, which are then used to fine-tune the model. In the reinforcement learning phase, the model generates multiple responses to prompts, which are evaluated by a preference model trained to align with the con-stitutional guidelines. These evaluations serve as a reward signal to optimize the model further. Figure 17 illustrates this dual-phase framework in detail. In the supervised learning phase, the model progressively learns to identify and rectify undesirable traits in its responses through self-feedback. In the reinforcement learning phase, a preference model evaluates the generated re-sponses, strengthening the model’s ability to pro-duce outputs that align with constitutional princi-ples while maintaining transparency. This frame-work ensures the AI remains non-evasive, engag-ing directly with sensitive or harmful prompts by explaining why they are problematic rather than avoiding them. By leveraging minimal manual oversight and applying clear rules, this approach presents an innovative way to reduce harmful out-puts while enhancing transparency and precise be-havioral control in AI systems. BeaverTails (Ji et al., 2024). BeaverTails is a large-scale, high-quality question-answer dataset designed to enhance the safety and utility of large language models (LLMs). As displayed in Fig-ure 18, this dataset uniquely separates annotations of "helpfulness" and "harmlessness" for question-answer pairs, providing distinct perspectives on these crucial attributes. It comprises safety meta-labels for 333,963 Q&A pairs and 361,903 pairs of expert comparison data for both helpfulness and harmlessness metrics The dataset spans diverse real-world scenarios, including everyday inquiries, professional domains, ethical challenges, and cross-cultural contexts, enabling researchers to refine LLM behavior more effectively. Unlike existing datasets, BeaverTails provides significant advan-tages in terms of scale and annotation granularity, aiming to become a core resource for exploring LLM safety and alignment within the community. Rule-Based Rewards (RBR) (Mu et al., 2024). RBR is a method designed to make LLMs safer and more helpful by relying on explicit, detailed rules rather than general guidelines. These rules, such as "Refusals should include an apology but not sound judgmental," are broken into simple binary propo-Figure 2: Two-Stage Annotation Process. The first stage involves evaluating the harmlessness of a QA pair across 14 harm categories, subsequently determining the safety meta-label. The second stage then ranks responses to a prompt according to their helpfulness and harmlessness. • Within this dataset, 44.64% were assigned the safe meta-label, while the remaining 55.36% were categorized under the unsafe meta-label. • We acquired 361,903 pairs of human-preference annotations separately for the helpfulness and harmlessness metrics of the responses. The inter-crowdworker agreement rate: safety meta-label = 81.68%, helpfulness preference = 62.39% and harmlessness = 60.91%. Additionally, we solicited crowdworkers to assign confidence scores to their annotations, applicable to both the classification and response-ranking tasks. The confidence spectrum extended from “very uncertain” and “uncertain” to “certain” and “very certain”, corresponding to a scale of 0 to 3. 3.2 Data Collection and Annotation Process Generating QA pairs Our study involves a collection of over 28k red-team prompts derived from the HH R ED -T EAM dataset [ 18 ] and [ 56 ]. Given the dialogical nature of these datasets, we specifically selected the first question that initiated the interaction between the human and the AI assistant . These questions were meticulously crafted by Ganguli et al. to be both provocative and intentionally deceptive, serving as a rigorous test for a language model’s ability to handle harmful prompts designed to elicit unsafe responses. For questions perceived as overly terse or incomplete, we incorporated additional contextual information during pre-processing. The average word count (using the regex /\b\w+\b/ ) for each prompt is 13.61. We then prompt the Alpaca-7B model [ 27 ] to generate multiple unique responses per question across the set of 7.7k unique questions (chosen from the previously mentioned set of red-team prompts) for BEAVER TAILS -30k. To ensure generation diversity and enrich the range of outputs, we modulate the sampling parameters as follows: temperature is set to 1.5, and the maximum token length is limited to 512, with top_k and top_p values configured at 30 and 0.95, respectively. We measure the average word count (using the regex /\b\w+\b/ ) and observe a mean of 61.38 words per response across the resultant 30k responses. Two-Stage Annotation Process In an effort to annotate our dataset with human-preference data efficiently, we engaged a team of over 70 crowdworkers (annotators) - all of whom possess at least a college-level education and a proficient command of English. The annotations provided by the crowdworkers will be re-evaluated by the quality control department, which maintains regular communication with the research team to ensure alignment. The task of annotating a QA pair in the BEAVER TAILS dataset involves a two-stage annotation process. During the first stage, the QA pair is annotated through a multi-classification process involving 14 harm categories (see Sec. 3.3), leading to the assignment of a corresponding safety meta-label. To facilitate the QA-moderation task during LLMs deployment (see Sec. 4.1), we advocate for assessing the harmlessness of a QA pair from a risk neutralization perspective, rather than relying solely on the toxicity score of individual utterances within the QA pair provided by content moderation systems. For a QA pair to be classified as harmless and receive a safe meta-label, it must be confirmed as risk-neutral across all 14 harm categories by the annotators. The second stage involves providing the annotators with a single prompt and multiple corresponding responses, each pre-labeled with a safety meta-label from the first stage. The annotators are then Figure 18: Annotation process of BeaverTails. The figure is copied from Ji et al. (2024). Figure 1: The RBR is combined with the helpful-only RM score during RL training. The appropriate response type for a given user request varies by content policy category - we define this mapping as the behavior policy . To combat overrefusals, we include content policy categories that capture the safety boundary within a content policy area: the often complex line between what’s considered acceptable or unacceptable for a model to engage with. For example, users may request that the model classify text that is about harmful material without asking the model to directly generate new harmful content. In these cases, the behavior policy may require the model to comply. ## 4 Rule-Based Rewards for Safety In this section, we describe Rule-Based Rewards (RBRs), our proposed approach to building safety reward functions for RL training based on a content and behavior policy. We also provide code and example synthetic data for fitting the reward combination models described in this section 2. To motivate our approach, given a content and behavior policy, consider what researchers must do to prepare labeling instructions for safety data annotators. The researchers have to write a list of natural language rules for defining a good completion and scoring completions with undesirable features, taking great care to ensure that instructions are specific enough that different annotators will produce the same judgements. For example, consider collecting data that scores completions from 1-7. For a request that should be hard-refused, a simplified version of a rule in our example can be: "rank completions with a short apology and statement of inability highest at 7, deduct 1 point for each undesirable refusal quality (such as judgemental language) that is present, if the refusal contains disallowed content rank it lowest at 1". Researchers often also have to provide illustrative examples. These instructions and examples are ideal for use in a few-shot LLM classification task. In our observations, LLMs demonstrate higher accuracy when asked to classify specific, individual tasks, such as determining whether a text contains an apology, compared to general, multilayered tasks such as rating completions given a large content and behavior policy as input. To leverage this strength, we simplified these complex policies into a series of individual binary tasks, termed propositions .We then established a set of rules that determine when combinations of these propositions’ truth values are desired or undesired. This framework allows us to accurately rank completions using > Figure 19: The overview of rule-based rewards (RBR). The figure is copied from Mu et al. (2024). > sitions, like whether the response includes an apol-ogy or avoids judgmental language. A Grader LLM evaluates each response against these propositions and assigns probabilities, which are then combined with an existing helpful-only reward model (RM) to create a total reward. As shown in Figure 19, this combined reward function is used in reinforcement learning, ensuring that the model aligns with both safety policies and helpfulness goals without being overly cautious. Unlike RLHF or RLAIF, which relies on collecting/generating synthetic datasets to train a reward model, RBR directly integrates the rules into the learning process. RLAIF’s synthetic datasets , built from general guidelines, can lose de-tail or require extensive updates as policies evolve. In contrast, RBR provides fine-grained control by applying rules dynamically during training, mak-ing it more precise and adaptable. Experimental results show that RBR achieves superior perfor-mance, with an F1 score of 97.1 compared to 91.7 for a human-feedback baseline, effectively balanc-ing safety and usefulness in LLM outputs. > 6.4 Reward Model Evaluation RewardBench (Lambert et al., 2024). Reward-Bench is a comprehensive benchmark designed to evaluate reward models, which addresses the lack of targeted, standardized evaluation methodologies. It covers diverse domains, including chat, reason-ing, and safety, and introduces a novel prompt-choice-rejection triplet dataset structure (see Fig-ure 20). This structure enables precise assess-ment of a reward model’s ability to align with human preferences by recognizing and prioritiz-ing high-quality outputs. The benchmark includes challenging test cases, such as out-of-distribution queries and fine-grained differences, like factual inaccuracies or logical inconsistencies. It also pro-poses systematic evaluation metrics, such as rejec-tion propensity, which measures a model’s abil-ity to reject low-quality content. Empirical stud-ies within RewardBench analyze various reward models trained through methods like maximum likelihood estimation (MLE) and direct preference optimization (DPO). These studies reveal critical ## Figure 1: The scoring method of the R EWARD BENCH evaluation suite. Each prompt is accompanied by a chosen and rejected completion which are independently rated by a reward model. # 3 Background ## Reward Modeling The first step of training a reward model, and therefore doing RLHF, is col-lecting preference data from a group of human labelers. Individuals are presented with prompts , ## x, akin to a question or task, and asked to choose between a set of completions , yi, answering the request. The most common case is for only two completions to be shown with measurement of preference, such as win-loss-tie or a Likert scale indicating the magnitude of preference between completions (Bai et al., 2022a), though other methods for labeling exist, such as ranking in a batch of 4+ answers (Ouyang et al., 2022). The resulting data is transformed into a set of prompt-chosen-rejected trios, where the chosen completion is preferred over the rejected completion for training. Training a reward model involves training a classifier to predict the human preference probability, ## p∗, between two answers, as modeled by a Bradley-Terry model (Bradley and Terry, 1952): ## p∗(y1 ≻ yx ∣ x) = exp (r∗(x, y 1)) ## exp (r∗(x, y 1)) + exp (r∗(x, y 2)) . (1) Then, estimate the parameters of the RM by optimizing the maximum likelihood loss as follows: ## L(θ, D) = E(x,y chosen ,y rejected )∼D [log (1 + erθ (x,y rejected ) − rθ (x,y chosen ))] .For language models, the RM is often implemented by appending a linear layer to predict one logit or removing the final decoding layers and replacing them with a linear layer. At inference time, a trained reward model returns a scalar, such that P (y1 ≻ y2 ∣ x) ∝ er(x,y 1) (which intuitively is the probability that the completion would be a preferred response, but is trained indirectly via the pairwise loss). Thus, a win between completions y1 and y2 is achieved when r(x, y 1) > r(x, y 2). ## Direct Preference Optimization Direct Preference Optimization solves the RLHF problem with-out needing to learn a separate reward model. It achieves this by reparameterizing the preference-based reward function using only the policy models (Rafailov et al., 2023) The implicit reward used in DPO is a function of the policy model probabilities (i.e. the model being trained), π(y∣x), a regularization constant, β, the base model probabilities, πref (y∣x), and a partition function Z(x): ## r(x, y ) = β log π(y∣x) ## πref (y∣x) + β log Z(x). (2) Given two completions to a prompt, we compare the rewards r(x, y 1) and r(x, y 2) as follows, where the score is computed via the log ratios of π: log π(y1∣x) πref (y1∣x) > log π(y2∣x) πref (y2∣x) .Figure 20: The prompt-choice-rejection triplets of RewardBench. The figure is copied from Lambert et al. (2024). Does the response accurately use specific industry terminologies and jargons? Pairwise Ranking + > Both response attempt to convey the fundamental concept of containerization, but with varying degrees of clarity and technical details. Response A approaches the concept by likening containerization to “putting things in a box”, a metaphor that while easy to understand, lacks the precision and industry-specific [...] On the other hand, Response B more effectively employs technical jargon such > as “packaging”, “configuration files”, “libraries”, and “dependencies”. [...] It can be concluded that Response B is better than Response A. A B B Verbal Feedback Scoring Decision vs Does the response use simple language and explanation that are easy to understand for a beginner? Direct Assessment + > The response effectively uses simple and accessible language to explain containerization and Docker, which is great for beginners. The analogy of putting things in a box is particularly helpful as it visually illustrates the concept of [...] > However, the response could be improved by briefly mentioning why containerization is significant, such as its benefits in ensuring that software runs consistently across different computing environments. It loses a point for not fully addressing the significance of containerization in the broader context of software development, which could add valuable insight for the reader. Verbal Feedback Scoring Decision Response A Instruction > In the field of software development, what is the meaning and significance of “Containerization”? Also, explain the role of Docker in containerization. > Containerization, in software, is somewhat like putting things in a box. It means you take your software and all its parts and put it in a container. Docker is a tool that helps with this. It helps to put the software in containers and makes it easy to use them. [...] > Containerization in software development refers to the process of packaging up an application along with all its related configurations files, libraries, and dependencies required to run, into a standalone unit or a ‘container’. [...] Response B > Evaluation Criteria AFigure 2: Comparison of direct assessment and pairwise ranking. Both responses could be considered decent under the umbrella of ‘helpfulness’. However, the scoring decision might change based on a specific evaluation criterion. 3 Methodology We propose a new recipe for training a unified evaluator LM based on merging the weights of models trained for direct assessment and pairwise ranking. We begin with background on direct as-sessment and pairwise ranking for evaluator LMs (Section 3.1, 3.2), followed by the construction process of our training data (Section 3.3). Finally, we present our methods to train the state-of-the-art evaluator LM, Prometheus 2 models (Section 3.4). 3.1 Direct Assessment Direct assessment is mapping an instruction i and response r into a scalar value score s, such as fdirect : ( i, r ) 7 → s where s ∈ R. For the scor-ing range, we use a 1-5 Likert scale scoring. Prior works have identified several recipes to align the scores provided by evaluator LMs ( sLM )and the scores assigned by humans ( shuman ). For instance, Liu et al. (2023a) and Zheng et al. (2023) uations are flexible to specific needs rather than generic qualities. Specifically, e is represented as a score rubric including a description for the cri-teria itself and a set of descriptions for each score between the scoring range. This is expressed as: fdirect : ( i, r, a, e ) 7 → (vr, s ) where s ∈ { 1, 2, 3, 4, 5} (1) 3.2 Pairwise Ranking Pairwise ranking is mapping an instruction i and two pair of responses (rm, rn) into either i or j,such as fpair : ( i, r m, r n) 7 → s where s ∈ { m, n }.Similar to direct assessment, prior works have identified that integrating a reference answer a and verbal feedback vrm,r n into the evaluation pipeline is crucial (Zheng et al., 2023; Li et al., 2023b,a). In addition, to support granular assessment under custom criterion, we add the evaluation criteria e as input to the evaluator LM (Ye et al., 2023; Kim et al., 2023). To the best of our knowledge, we are the first to study such fine-grained evaluation in Figure 21: The dual-task framework of Prometheus 2. The figure is copied from Kim et al. (2024b). insights, including the models’ limitations in re-jecting problematic outputs, their susceptibility to training data distribution in reasoning tasks, and performance variability in instruction adherence. By making the dataset and codebase publicly avail-able, RewardBench not only provides reproducible tools for the research community but also sets a new standard for reward model evaluation. Prometheus 2 (Kim et al., 2024b) Prometheus 2 is an open-source evaluation model developed to address key challenges in assessing language models, such as lack of transparency, reliance on proprietary systems like GPT-4, and high evalu-ation costs. Its primary motivation is to provide a reliable and accessible alternative for evaluat-ing language model outputs across diverse tasks, including text generation, reasoning, and factual accuracy. Unlike traditional approaches that de-pend on closed-source evaluators, Prometheus 2 empowers the research community with a trans-parent and reproducible framework, enabling inde-pendent evaluations without sacrificing quality or consistency. The innovation of Prometheus 2 lies in its design as a dedicated evaluation model trained on high-quality datasets that include both direct scoring and pairwise ranking tasks (see Figure 21). This dual-task framework ensures the model can handle nuanced distinctions, such as subtle gram-matical errors or logical inconsistencies, which are critical for robust LM evaluations. Additionally, Prometheus 2 incorporates alignment techniques to closely mimic human preferences, achieving state-of-the-art performance in agreement with hu-man and proprietary evaluations. Its systematic approach enables the model to outperform existing open-source evaluators, providing accurate, consis-tent, and interpretable assessments. 7 Direct Preference Optimization (DPO) While effective, RLHF or RLAIF is often mired in complexity due to the challenges of reinforce-ment learning algorithms and the necessity of an accurately trained reward model. Recent research has turned towards Direct Preference Optimization (DPO), which bypasses the reward model by di-rectly using human preference data to fine-tune LLMs. DPO reframes the objective from reward maximization to preference optimization, and of-fers a straightforward and potentially more robust pathway for aligning LLM outputs with human ex-pectations. This section delves into the methodolo-gies underpinning DPO, exploring how approaches like SLiC-HF, β-DPO, sDPO, and others leverage preference data to enhance LLM alignment without the overhead of traditional RL frameworks. 7.1 SLiC-hf SLiC-HF (Zhao et al., 2023) leverages Sequence Likelihood Calibration to optimize LLMs based on human feedback without relying on reward-based reinforcement learning, using human pref-erence data in a simpler, contrastive setup. This is achieved by using a rank calibration loss to dis-tinguish between positive and negative sequences. Given an input sequence x, SLiC-HF pairs human-preferred sequences y+ (positive) with less pre-ferred sequences y− (negative) and encourages the model to assign higher likelihood to y+ over y−. The calibration loss function, Lcal (θ) =max(0 , β − log Pθ(y+|x) + log Pθ(y−|x)) , incor-porates a margin parameter β to ensure adequate separation between preferred and non-preferred se-quences. SLiC-HF employs two primary approaches: SLiC-HF-direct and SLiC-HF-sample-rank. SLiC-HF-direct uses raw human feedback data (without filtering or ranking) to calibrate the likelihood of sequences directly. This direct application min-imizes complexity but may suffer from out-of-distribution examples if the feedback data does not match model outputs. SLiC-HF-sample-rank, on the other hand, involves generating multiple candi-date sequences for a given input, then selecting the best one using a ranking or reward model. In this approach, the candidates are generated by sampling and ranking, often employing a pairwise ranking model that has been trained to predict human pref-erences. 7.2 DPO Similar to Slic-hf, DPO (Rafailov et al., 2024) by-passes the iterative sampling complexities of RLHF by utilizing a closed-form optimization with a sim-ple binary classification objective that models pref-erences directly. In contrast to RLHF, which typically trains a separate reward model, DPO implicitly optimizes for the desired preference function by adjusting the policy directly. This is achieved through a re-parameterization approach, where the model’s outputs approximate an optimal policy under the Bradley-Terry model—a commonly used statistical model for paired preference data. A key insight in DPO is using a closed-form expression to directly represent the optimal policy in terms of the learned preference probabilities. The derived policy for-mula avoids iterative policy updates and instead relies on a classification-style loss, which is com-puted by comparing the likelihood of preferred and dis-preferred responses. The binary cross-entropy loss between these likelihoods serves as the pri-mary optimization metric, ensuring that the model output aligns with human preferences in a stable manner. 7.3 β-DPO Although DPO has gained attention as a stream-lined alternative to RLHF, the static nature of DPO’s β parameter—a hyperparameter governing the balance between model preference alignment and retention of original model traits—limits its robustness across diverse data qualities. The β-DPO (Wu et al., 2024a) method introduces a dy-namic calibration mechanism for the β parameter by leveraging batch-level data quality assessments. A batch-specific β adjustment responds to the in-formativeness of the pairwise data in each batch. Specifically, β is adapted based on the mean re-ward discrepancy within each batch: for closely matched pairs (low gap), β is decreased to en-able more assertive updates, while for more dis-tinct pairs (high gap), β is increased to temper the updates, thus avoiding overfitting. To implement this, the β parameter for each batch is computed as βbatch = [1 + α(Ei∈batch [Mi] − M0)] β0, where Miis the individual reward discrepancy, M0 is a base-line threshold updated via a moving average, and α scales the discrepancy’s impact. Additionally, β-DPO incorporates a filtering mechanism guided by β, selecting the top 80% most informative sam-ples within each batch by estimating the reward discrepancy distribution. 7.4 sDPO Another problem of Traditional DPO is to use en-tire preference datasets in a single step, aligning models by comparing their outputs against a sin-gle reference model. In contrast, sDPO (Kim et al., 2024a) partitions these datasets and feeds them into the training process incrementally. This method al-lows each training step to use a more aligned model from the prior step as the reference, creating a pro-gressively refined alignment path. sDPO begins with a SFT base model that serves as the initial reference model. At each step, a por-tion of the preference data is used to align the tar-get model, and the aligned model from the previ-ous step becomes the reference model for the next. This iterative setup allows the reference model’s alignment quality to gradually improve, offering a progressively higher standard, or lower bound, for each subsequent alignment step. sDPO modi-fies the DPO loss by introducing an evolving lower bound through the increasingly aligned reference models. The objective of each step’s training is to maximize the preference score by differentiating the target model’s log probability ratios for chosen versus rejected responses relative to the reference model. This approach creates an internal progres-sion from easier to more challenging preference optimization, akin to curriculum learning. Addi-tionally, sDPO suggests an easy-to-hard partition-ing strategy for preference data, where early chunks consist of data on which the model performs well, helping stabilize early alignment and intensify dif-ficulty as the steps advance, thus reinforcing the alignment through a structured optimization path. 7.5 RSO RSO (Liu et al., 2023a) centers on the development of Statistical Rejection Sampling Optimization, de-signed to refine language model alignment with human preferences by addressing data distribution limitations inherent in SLiC and DPO. RSO begins by constructing a reward-ranking model based on a human preference dataset, which provides pair-wise comparisons of output quality. This reward-ranking model then guides the statistical rejection sampling process, allowing the system to generate response pairs that closely approximate an optimal target policy. Unlike SLiC, which samples pairs from a SFT policy, RSO selects candidate pairs through a controlled rejection sampling approach. This approach first samples from the SFT policy and then probabilistically accepts or rejects sam-ples based on how closely they match the desired distribution according to the reward-ranking model. The sampling mechanism emphasizes accuracy by progressively recalculating the acceptance criteria, thus continuously refining the sampled distribution toward the optimal policy. RSO then fits the model to these preference-labeled pairs using tailored loss functions, such as hinge or sigmoid-norm, to en-sure alignment without relying on explicit rein-forcement learning structures. 7.6 GPO GPO (Tang et al., 2024) aligns large models with human feedback by optimizing over offline datasets. The core methodology in GPO is creating a general-ized framework for offline preference optimization by using a family of convex functions to parameter-ize loss functions. Existing methods such as DPO and SLiC are claimed as specific instances of this general approach, depending on the convex func-tion chosen ( e.g. , logistic for DPO and hinge for SLiC). GPO further extends to variants by allowing flexibility in the convex function, defining a broad range of preference optimization strategies with distinct regularization strengths. GPO provides a Taylor expansion around ρθ = 0 to approximate and analyze the loss functions. This approxima-tion reveals that the GPO loss dynamically bal-ances preference optimization and regularization by adapting to the chosen convex function’s prop-erties. For instance, by choosing a function with a rapidly decaying tail, GPO enforces stronger reg-ularization, constraining the learned policy closer to the reference model. In contrast, slower decay-ing functions lead to more flexible policies with potentially greater divergence from the reference policy, which could increase model expressiveness but may require more careful tuning of the regular-ization coefficient, β. 7.7 DRO DRO (Richemond et al., 2024) aims to improve LLM alignment by using single-trajectory data rather than traditional, costly preference data. Cen-Figure 22: Overview of sDPO where preference datasets are divided to be used in multiple steps. The figure is borrowed from Kim et al. (2024a). > Figure 23: RSO fits a pairwise reward-ranking model from human preference data. The figure is borrowed from Liu et al. (2023a). > Figure 24: A simplified illustration of reward modeling and online iterative RLHF. The figure is borrowed from Dong et al. (2024). tral to the DRO framework is the construction of a single, quadratic objective function that approx-imates optimal policy and value functions in the single-trajectory setting. Here, the primary goal is to avoid pairwise preferences and instead use a direct feedback score (like a thumbs-up or thumbs-down). DRO begins by defining a regularized ob-jective function where the policy optimization is guided by a KL divergence term, maintaining con-sistency with a reference policy, and incorporates a reward signal for each single trajectory. The DRO loss function is crafted as a sum of squared residu-als between the observed reward and a computed expected value adjusted by the policy and reference terms. Additionally, DRO uses an iterative pro- > Figure 25: Illustration of our implementation of iterative direct preference learning. The figure is borrowed from Dong et al. (2024). cess where gradient updates are applied to both the policy and value function parameters to minimize empirical loss. This setup includes a regularization parameter to balance the policy updates against the reference model’s stability. 8 Analysis of DPO While the simplicity and efficiency of DPO make it an appealing choice, its practical implementa-tion reveals challenges and opportunities for im-provement. This section delves into the safety im-plications of DPO, particularly in how it handles harmful outputs, and explores DPO variants, which aim to optimize the trade-off between minimizing harmful content and maintaining generative diver-sity. We reveal studies that highlight the theoretical and practical considerations that define the effec-tiveness and limitations of DPO-based methods in achieving safe, reliable, and high-interpretability LLMs. 8.1 Safety D2O (Duan et al., 2024). D2O is designed to align LLMs with human values by training on neg-ative examples, such as harmful or ethically prob-lematic outputs. It optimizes a distribution-level Bradley-Terry preference model, which contrasts Figure 26: Safe RLHF pipeline compared to conventional RLHF method. The figure is borrowed from Dai et al. (2023). > Figure 27: Illustration of DPO and D 2O Comparison. The figure is borrowed from Duan et al. (2024). the model’s responses with the negative samples and encourages the model to reduce harmfulness without introducing harmful biases from positive responses. The optimization process in D 2O avoids catastrophic forgetting—a common problem when the model is forced to only minimize negative out-puts—which can lead to the model forgetting how to generate useful, informative content. This is achieved by progressively sampling self-generated responses during training and maximizing the dif-ference between these and the human-annotated negative samples, maintaining a balance between exploration and the minimization of harmful con-tent. D 2O demonstrates that it upper bounds the effectiveness of previous methods like Instance-level DPO. This means D 2O minimizes the nega-tive content while enhances the model’s ability to explore diverse responses, improving robustness and response quality without overfitting to negative samples. NPO (Zhang et al., 2024b). NPO builds on prin-ciples of preference optimization by utilizing only negative samples to refine unlearning in language models. NPO minimizes a loss function that se-lectively decreases model confidence on data des-ignated for unlearning. This loss function is de-rived from the DPO but focuses solely on discour-aging specific outputs instead of comparing both preferred and less preferred responses. In imple-mentation, the NPO loss adaptively weights each gradient step, reducing the influence of already unlearned samples by lowering their gradient con-tributions through a weight, which approaches zero as the model confidence on undesirable samples declines, slowing divergence and preventing catas-trophic collapse. 8.2 Variations of DPO DNO (Rosset et al., 2024). DNO operates through a batched on-policy structure, which al-lows iterative self-improvement of the model based Figure 28: Illustration of how SPO is applied where we are able to query the preference function online and where we are given a fixed dataset. The figure is borrowed from Swamy et al. (2024). on a Nash equilibrium concept. Each iteration in-volves the model learning a regression-based objec-tive, where it aims to maximize the likelihood of responses preferred over competing outputs in a se-quence of "self-play" rounds. Pairs of responses (or outputs) are generated from model outputs on spe-cific prompts, ranked by a preference function that estimates "win-rates." High-margin pairs—where one response is significantly preferred—are re-tained to focus training on clear improvements. To maintain stability and computational efficiency, DNO implements a filtering strategy, ensuring that only preference pairs with a high margin of prefer-ence are selected for training. SPPO (Wu et al., 2024b). SPPO reformulates language model optimization as a constant-sum two-player game, where the goal is to identify a Nash equilibrium policy through iterative updates. Each policy update in SPPO uses a multiplicative weight approach, a framework adapted from game theory, specifically designed to approximate Nash equilibria. The method proceeds by sampling re-sponses for a given prompt and using a preference model to assign win probabilities, indicating which responses are preferred. In each iteration, SPPO refines the policy by adjusting the probability dis-tribution over responses based on observed prefer-ences, ensuring responses with higher preference win rates are increasingly favored. The SPPO objective function optimizes over each response’s probability weight to approxi-mate an ideal Nash equilibrium. It avoids the di-rect computation of log-partition factors—used in traditional preference optimization methods like DPO—by approximating these factors with a con-stant, which could help reduce variance in policy updates. SPO (Swamy et al., 2024). SPO is rooted in the concept of the Minimax Winner from social choice theory, a solution concept that SPO employs to handle complex preference aggregation tasks. At the core, SPO frames RLHF as a two-player zero-sum game where, conventionally, two policies are pitted against each other in a "dueling" setup. How-ever, SPO simplifies this to a single-agent, self-play mechanism that approximates the Minimax Winner. To accomplish this, SPO uses a preference function that compares two trajectories and assigns a score based on the proportion of times one trajectory is preferred over the other. This score then serves as a reward signal, which the agent optimizes. By lever-aging the symmetry of the preference-based zero-sum game, the process converges robustly even without requiring explicit adversarial or competi-tive training. DPOP (Pal et al., 2024). DPOP is designed to address a failure mode of DPO when fine-tuning LLMs on preference data with low edit distances. It is found that DPO can unintentionally decrease the likelihood of preferred responses in such cases due to its focus on relative probabilities between preferred and dispreferred completions. To over-come this, DPOP augments the standard DPO loss with a corrective penalty term that ensures the log-likelihood of preferred completions does not fall below the reference model’s likelihood. The full DPOP loss function combines a standard DPO term with a regularization term that penalizes the re-duction in probability of the preferred completion. This modification forces the model to retain a high probability for preferred responses, mitigating the risk of performance degradation observed in DPO, especially when the edit distance between comple-tion pairs is small. TDPO (Zeng et al., 2024). TDPO refines the DPO framework by optimizing at the token level rather than the sentence level, addressing diver-gence efficiency and content diversity. TDPO for-mulates text generation as a Markov Decision Pro-cess, where each token is treated as an action within a sequence. TDPO introduces token-wise KL di-vergence constraints, employing forward KL di-vergence to regulate token-level generation while maintaining diversity. By extending the Bradley-Terry model to the token level, TDPO leverages the Regret Preference Model to compute preference probabilities for each token pair. The loss function incorporates both forward and reverse KL diver-Figure 29: Illustration of DPOP avoiding a failure mode of DPO. The figure is borrowed from Pal et al. (2024). gence terms, achieving a balance between align-ment with human preferences and generative diver-sity. Two variants, TDPO1 and TDPO2, differ in how they handle the KL divergence, with TDPO2 introducing a parameter α to fine-tune the diver-gence balance between preferred and dispreferred responses. 8.3 Human Interpretability ΨPO (Azar et al., 2024). ΨPO optimizes a pol-icy by maximizing a non-linear function of the preference probabilities, expressed as Ψ( p∗(y ≻ y′|x)) , where Ψ is a non-decreasing function, while maintaining proximity to a reference policy through KL-divergence regularization. By setting Ψ to the identity function, Identity-Preference Opti-mization (IPO) is proposed as a practical version of ΨPO that directly learns from preferences without needing a reward model and without relying on the Bradley-Terry assumption. IPO avoids overfit-ting by ensuring that policy optimization remains regularized towards the reference policy, even in the presence of deterministic or nearly determin-istic preferences. The method employs a simple yet effective empirical loss function, derived from root-finding problems, which can be optimized via gradient descent. Unpacking DPO and PPO (Ivison et al., 2024). Unpacking DPO and PPO investigate PPO and DPO, and finds that PPO’s online nature allows for dynamic adaptation and significant performance improvements in complex domains such as reason-ing and coding, where iterative feedback is essen-tial, whereas DPO is computationally more effi-cient but limited in its flexibility due to its reliance on static data. The comparative analysis suggests that preference quality, reward model size, and training algorithm choice significantly influence downstream performance, with PPO generally out-performing DPO in multi-task, generalist settings, but DPO showing strong results in tasks requiring less complex adaptation. Iterative Preference Learning from Human Feedback (Xiong et al., 2024). Iterative pref-erence learning from human feedback formulates RLHF as a reverse-KL regularized contextual ban-dit problem, where the objective is to maximize human feedback alignment while ensuring that the learned policy does not deviate too far from the pre-trained model, as captured by a KL divergence term. Theoretical analysis reveals that the reverse-KL constraint introduces a stochastic optimal policy, which addresses the challenge of balancing explo-ration with fidelity to the pretrained policy, a key issue in real-world alignment. In offline learning, pessimism is applied by conservatively estimating the reward, using uncertainty bounds derived from concentration inequalities, which guarantees sam-ple efficiency. The online iterative learning setting is based on batch hybrid learning, where human feedback is incorporated incrementally, and explo-ration is controlled via uncertainty-based explo-ration strategies. This study derives finite-sample theoretical guarantees for both offline and online settings, showing that the proposed methods, such as the iterative DPO with pessimistic reward estima-tion and multi-step rejection sampling, outperform existing approaches in terms of sample efficiency and alignment performance. Furthermore, the anal-ysis highlights the trade-off between exploration and exploitation, proving that strategic exploration during online learning enhances the model’s ability to generalize to out-of-distribution data, while also minimizing the KL divergence to the initial policy Insights into Alignment (Saeidi et al., 2024). In-sights into alignment reveal that DPO faces chal-lenges related to overfitting and inefficient learning, particularly in the absence of a regularization mech-anism. IPO addresses these by introducing a regu-larization term to smooth the preference function, effectively balancing the alignment with general-ization across tasks. KTO (Ethayarajh et al., 2024), inspired by prospect theory, eliminates the need for paired preferences by treating each response as either desirable or undesirable, simplifying the opti-mization process and reducing computational com-plexity. Lastly, CPO (Guo et al., 2024) improves DPO by removing the reference model during train-ing, reducing memory consumption and enabling larger-scale model fine-tuning with fewer resources, while still maintaining alignment through a combi-nation of maximum-likelihood and preference loss. Theoretically, these methods trade off the complex-ity of RL-based feedback for a more direct and efficient alignment process, though they require careful attention to regularization and preference sampling to prevent model bias or poor generaliza-tion, especially in diverse task domains. Is DPO Superior to PPO for LLM Alignment (Xu et al., 2024a)? Theoretical analysis (Xu et al., 2024a) reveals that DPO, by directly optimiz-ing policies based on preference pairs, sidesteps the need for an explicit reward model, instead fram-ing the reward as a log ratio of policy probabilities. However, this approach exposes DPO to signifi-cant risks of out-of-distribution bias, as it lacks the regularizing influence of a reward function, lead-ing to potentially skewed policy distributions when preference data does not cover the full model out-put space. In contrast, PPO mitigates such issues by leveraging a learned reward model, which in-troduces a KL divergence regularization term that constrains the model’s policy updates, preventing excessive divergence from the reference policy and ensuring better generalization across diverse input distributions. The study proves that PPO’s solu-tions are a proper subset of DPO’s, meaning any optimal solution under PPO can also be a solution under DPO, but DPO may produce biased solu-tions in cases where distribution shifts exist. More-over, PPO’s performance is significantly enhanced through key techniques like advantage normaliza-tion, large batch sizes, and exponential moving average updates for the reference model, which sta-bilize training and improve convergence, especially in complex tasks such as code generation. 9 Conclusion This paper surveys the most up-to-date state of knowledge on reinforcement learning enhanced LLMs, attempting to consolidate and analyze the rapidly growing research in this field. We make a systematic review of the literature, including the ba-sics of RL, popular RL-enhanced LLMs, studies on two reward model-based RL techniques—RLHF and RLAIF—and works focused on bypassing the reward model to directly align LLM outputs with human expectations through DPO. We hope this work will help researchers understand the current challenges and advancements, and motivate further endeavors to address the deficiencies of current RL-enhanced LLMs. References > Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. 2024. Phi-3 technical report: A highly ca-pable language model locally on your phone. arXiv preprint arXiv:2404.14219 .Bo Adler, Niket Agarwal, Ashwath Aithal, Dong H Anh, Pallab Bhattacharya, Annika Brundyn, Jared Casper, Bryan Catanzaro, Sharon Clay, Jonathan Cohen, et al. 2024. Nemotron-4 340b technical report. arXiv preprint arXiv:2406.11704 .Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker. 2024. Back to ba-sics: Revisiting reinforce style optimization for learn-ing from human feedback in llms. arXiv preprint arXiv:2402.14740 .Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. 2023. Gqa: Training generalized multi-query trans-former models from multi-head checkpoints. In Pro-ceedings of the 2023 Conference on Empirical Meth-ods in Natural Language Processing , pages 4895– 4901. Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. 2016. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565 .Anthropic. 2024. Claude 3 family. Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bi-lal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. 2024. A general theoret-ical paradigm to understand learning from human preferences. In International Conference on Arti-ficial Intelligence and Statistics , pages 4447–4455. PMLR. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073 .Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150 .Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et al. 2024. Internlm2 technical re-port. arXiv preprint arXiv:2403.17297 .Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023) , 2(3):6. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anasta-sios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E Gonzalez, et al. 2024. Chatbot arena: An open plat-form for evaluating llms by human preference. arXiv preprint arXiv:2403.04132 .Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. 2023. Ultrafeedback: Boosting lan-guage models with high-quality feedback. arXiv preprint arXiv:2310.01377 .Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, and Yaodong Yang. 2023. Safe rlhf: Safe reinforcement learning from human feedback. arXiv preprint arXiv:2310.12773 .Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314 .Shizhe Diao, Pengcheng Wang, Yong Lin, Rui Pan, Xiang Liu, and Tong Zhang. 2023. Active prompt-ing with chain-of-thought for large language models. arXiv preprint arXiv:2302.12246 .Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023. Enhancing chat language models by scaling high-quality instructional conver-sations. arXiv preprint arXiv:2305.14233 .Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, and Tong Zhang. 2024. Rlhf work-flow: From reward modeling to online rlhf, 2024. https://arxiv.org/abs/2405.07863. Nicolai Dorka. 2024. Quantile regression for dis-tributional reward models in rlhf. arXiv preprint arXiv:2409.10164 .Yuqing Du, Olivia Watkins, Zihan Wang, Cédric Co-las, Trevor Darrell, Pieter Abbeel, Abhishek Gupta, and Jacob Andreas. 2023. Guiding pretraining in reinforcement learning with large language models. In International Conference on Machine Learning ,pages 8657–8677. PMLR. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm: General language model pretraining with autoregres-sive blank infilling. pages 320–335. Shitong Duan, Xiaoyuan Yi, Peng Zhang, Tun Lu, Xing Xie, and Ning Gu. 2024. Negating negatives: Align-ment without human positive samples via distribu-tional dispreference optimization. arXiv preprint arXiv:2403.03419 .Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 .Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. 2024. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306 .Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. 2023. Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997 .Louie Giray. 2023. Prompt engineering with chatgpt: a guide for academic writers. Annals of biomedical engineering , 51(12):2629–2633. Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chen-hui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Han-lin Zhao, Hanyu Lai, et al. 2024. Chatglm: A family of large language models from glm-130b to glm-4 all tools. arXiv preprint arXiv:2406.12793 .Yiju Guo, Ganqu Cui, Lifan Yuan, Ning Ding, Zexu Sun, Bowen Sun, Huimin Chen, Ruobing Xie, Jie Zhou, Yankai Lin, et al. 2024. Controllable preference opti-mization: Toward controllable multi-objective align-ment. arXiv preprint arXiv:2402.19085 .Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language under-standing. arXiv preprint arXiv:2009.03300 .Jiwoo Hong, Noah Lee, and James Thorne. 2024. Reference-free monolithic preference optimization with odds ratio. arXiv preprint arXiv:2403.07691 .Zhenyu Hou, Yilin Niu, Zhengxiao Du, Xiaohan Zhang, Xiao Liu, Aohan Zeng, Qinkai Zheng, Minlie Huang, Hongning Wang, Jie Tang, et al. 2024. Chatglm-rlhf: Practices of aligning large language models with human feedback. arXiv preprint arXiv:2404.00934 .Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adap-tation of large language models. arXiv preprint arXiv:2106.09685 .HuggingFaceH4. 2024. Zephyr-orpo-141b-a35b-v0.1. https://huggingface.co/HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1. Hamish Ivison, Yizhong Wang, Jiacheng Liu, Zeqiu Wu, Valentina Pyatkin, Nathan Lambert, Noah A. Smith, Yejin Choi, and Hannaneh Hajishirzi. 2024. Unpack-ing dpo and ppo: Disentangling best practices for learning from preference feedback. arXiv preprint arXiv:2406.09279 .Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A Smith, Iz Belt-agy, et al. 2023. Camels in a changing climate: En-hancing lm adaptation with tulu 2. arXiv preprint arXiv:2311.10702 .Jiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, Chi Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. 2024. Beavertails: To-wards improved safety alignment of llm via a human-preference dataset. Advances in Neural Information Processing Systems , 36. Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825 .Subbarao Kambhampati. 2024. Can large language models reason and plan? Annals of the New York Academy of Sciences , 1534(1):15–18. Dahyun Kim, Yungi Kim, Wonho Song, Hyeonwoo Kim, Yunsu Kim, Sanghoon Kim, and Chanjun Park. 2024a. sdpo: Don’t use your data all at once. arXiv preprint arXiv:2403.19270 .Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. 2024b. Prometheus 2: An open source language model specialized in evaluating other language mod-els. arXiv preprint arXiv:2405.01535 .Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. 2023. Reward design with language models. arXiv preprint arXiv:2303.00001 .Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. 2024. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787 .Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Car-bune, and Abhinav Rastogi. 2023. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267 .Jooyoung Lee, Fan Yang, Thanh Tran, Qian Hu, Emre Barut, Kai-Wei Chang, and Chengwei Su. 2024. Can small language models help large language models reason better?: Lm-guided chain-of-thought. arXiv preprint arXiv:2404.03414 .Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, and Mike Lewis. 2023. Self-alignment with instruction back-translation. arXiv preprint arXiv:2308.06259 .Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et al. 2024a. Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434 .Chris Yuhao Liu, Liang Zeng, Jiacai Liu, Rui Yan, Ju-jie He, Chaojie Wang, Shuicheng Yan, Yang Liu, and Yahui Zhou. 2024b. Skywork-reward: Bag of tricks for reward modeling in llms. arXiv preprint arXiv:2410.18451 .Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Zhang, Jinmeng Rao, Steven Zheng, Daiyi Peng, Diyi Yang, Denny Zhou, et al. 2024c. Best practices and lessons learned on synthetic data. Shengcai Liu, Caishun Chen, Xinghua Qu, Ke Tang, and Yew-Soon Ong. 2024d. Large language models as evolutionary optimizers. pages 1–8. Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter J. Liu, and Jialu Liu. 2023a. Statistical rejection sampling improves preference optimization. arXiv preprint arXiv:2309.06657 .Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yi-fan Xu, Weng Lam Tam, et al. 2023b. Alignbench: Benchmarking chinese alignment of large language models. arXiv preprint arXiv:2311.18743 .Jieyi Long. 2023. Large language model guided tree-of-thought. arXiv preprint arXiv:2305.08291 .Xingzhou Lou, Dong Yan, Wei Shen, Yuzi Yan, Jian Xie, and Junge Zhang. 2024. Uncertainty-aware reward model: Teaching reward models to know what is unknown. arXiv preprint arXiv:2410.00847 .Keming Lu, Bowen Yu, Fei Huang, Yang Fan, Runji Lin, and Chang Zhou. 2024. Online merging optimizers for boosting rewards and mitigating tax in alignment. arXiv preprint arXiv:2405.17931 .Ximing Lu, Sean Welleck, Jack Hessel, Liwei Jiang, Lianhui Qin, Peter West, Prithviraj Ammanabrolu, and Yejin Choi. 2022. Quark: Controllable text generation with reinforced unlearning. Advances in neural information processing systems , 35:27591– 27609. Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xi-ubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. Wizardcoder: Empowering code large language models with evol-instruct. Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023. Eu-reka: Human-level reward design via coding large language models. arXiv preprint arXiv:2310.12931 .Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2021. Cross-task generaliza-tion via natural language crowdsourcing instructions. arXiv preprint arXiv:2104.08773 .Mistral AI. 2024. Mixtral-8x22b-v0.1. https://huggingface.co/mistralai/Mixtral-8x22B-v0.1. Tong Mu, Alec Helyar, Johannes Heidecke, Joshua Achiam, Andrea Vallone, Ian Kivlichan, Molly Lin, Alex Beutel, John Schulman, and Lilian Weng. 2024. Rule based rewards for language model safety. arXiv preprint arXiv:2411.01111 .Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawa-har, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. 2023. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707 .Nexusflow. 2024. Athene-llama3-70b: Advancing open-weight chat models. OpenAI. 2023. Gpt-4 technical report. OpenAI. 2024a. Hello, GPT-4o. https://openai.com/index/hello-gpt-4o/. OpenAI. 2024b. O-1: Optimization for lan-guage models with continuous integration. https://openai.com/o1/. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instruc-tions with human feedback. Advances in neural in-formation processing systems , 35:27730–27744. Arka Pal, Deep Karkhanis, Samuel Dooley, Man-ley Roberts, Siddartha Naidu, and Colin White. 2024. Smaug: Fixing failure modes of prefer-ence optimisation with dpo-positive. arXiv preprint arXiv:2402.13228 .Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and Marco Tulio Ribeiro. 2023. Art: Automatic multi-step reasoning and tool-use for large language mod-els. arXiv preprint arXiv:2303.09014 .Junsoo Park, Seungyeon Jwa, Meiying Ren, Daeyoung Kim, and Sanghyuk Choi. 2024. Offsetbias: Lever-aging debiased data for tuning evaluators. arXiv preprint arXiv:2407.06551 .Baolin Peng, Chunyuan Li, Pengcheng He, Michel Gal-ley, and Jianfeng Gao. 2023. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277 .Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-pher D Manning, Stefano Ermon, and Chelsea Finn. 2024. Direct preference optimization: Your language model is secretly a reward model. Advances in Neu-ral Information Processing Systems , 36. David Rein, Betty Li Hou, Asa Cooper Stickland, Jack-son Petty, Richard Yuanzhe Pang, Julien Dirani, Ju-lian Michael, and Samuel R Bowman. 2023. Gpqa: A graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022 .Pierre Harvey Richemond, Yunhao Tang, Daniel Guo, Daniele Calandriello, Mohammad Gheshlaghi Azar, Rafael Rafailov, Bernardo Avila Pires, et al. 2024. Offline regularised reinforcement learning for large language models alignment. arXiv preprint arXiv:2405.19107 .Corby Rosset, Ching-An Cheng, Arindam Mi-tra, Michael Santacroce, Ahmed Awadallah, and Tengyang Xie. 2024. Direct nash optimization: Teaching language models to self-improve with gen-eral preferences. arXiv preprint arXiv:2404.03715 .Amir Saeidi, Shivanshu Verma, and Chitta Baral. 2024. Insights into alignment: Evaluating dpo and its variants across multiple tasks, 2024. https://api.semanticscholar.org/CorpusID269303161. John Schulman. 2015. Trust region policy optimization. arXiv preprint arXiv:1502.05477 .John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proxi-mal policy optimization algorithms. arXiv preprint arXiv:1707.06347 .Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, et al. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300 .Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. 2022. Defining and characteriz-ing reward gaming. Advances in Neural Information Processing Systems , 35:9460–9471. Jiayang Song, Zhehua Zhou, Jiawei Liu, Chunrong Fang, Zhan Shu, and Lei Ma. 2023. Self-refined large language model as automated reward function designer for deep reinforcement learning in robotics. arXiv preprint arXiv:2309.06687 .Hao Sun. 2023. Reinforcement learning in the era of llms: What is essential? what is needed? an rl perspective on rlhf, prompting, and beyond. arXiv preprint arXiv:2310.06147 .Hao Sun, Alihan Hüyük, and Mihaela van der Schaar. 2023a. Query-dependent prompt evaluation and opti-mization with offline inverse rl. Xiaofei Sun, Linfeng Dong, Xiaoya Li, Zhen Wan, Shuhe Wang, Tianwei Zhang, Jiwei Li, Fei Cheng, Lingjuan Lyu, Fei Wu, and Guoyin Wang. 2023b. Pushing the limits of chatgpt on nlp tasks. Xiaofei Sun, Xiaoya Li, Jiwei Li, Fei Wu, Shang-wei Guo, Tianwei Zhang, and Guoyin Wang. 2023c. Text classification via large language models. arXiv preprint arXiv:2305.08377 .Xiaofei Sun, Xiaoya Li, Shengyu Zhang, Shuhe Wang, Fei Wu, Jiwei Li, Tianwei Zhang, and Guoyin Wang. 2023d. Sentiment analysis through llm negotiations. arXiv preprint arXiv:2311.01876 .Gokul Swamy, Christoph Dann, Rahul Kidambi, Zhi-wei Steven Wu, and Alekh Agarwal. 2024. A mini-maximalist approach to reinforcement learning from human feedback. arXiv preprint arXiv:2401.04056 .Yunhao Tang, Zhaohan Daniel Guo, Zeyu Zheng, Daniele Calandriello, Rémi Munos, Mark Row-land, Pierre Harvey Richemond, Michal Valko, Bernardo Ávila Pires, and Bilal Piot. 2024. General-ized preference optimization: A unified approach to offline alignment. arXiv preprint arXiv:2402.05749 .Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Alpaca: Astrong, replicable instruction-following model. Stan-ford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html ,3(6):7. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, et al. 2023. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 .Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. 2024a. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295 .Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupati-raju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. 2024b. Gemma 2: Improving open language models at a practical size. arXiv preprint arXiv:2408.00118 .Reka Team, Aitor Ormazabal, Che Zheng, Cyprien de Masson d’Autume, Dani Yogatama, Deyu Fu, Donovan Ong, Eric Chen, Eugenie Lamprecht, Hai Pham, et al. 2024c. Reka core, flash, and edge: A se-ries of powerful multimodal language models. arXiv preprint arXiv:2404.12387 .Ryan Teknium, Jeffrey Quesnelle, and Chen Guang. 2024. Hermes 3 technical report. arXiv preprint arXiv:2408.11857 .Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Ro-driguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and fine-tuned chat models. Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, et al. 2023. Zephyr: Di-rect distillation of lm alignment. arXiv preprint arXiv:2310.16944 .Zhen Wan, Fei Cheng, Zhuoyuan Mao, Qianying Liu, Haiyue Song, Jiwei Li, and Sadao Kurohashi. 2023. Gpt-re: In-context learning for relation ex-traction using large language models. arXiv preprint arXiv:2305.02105 .Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. 2024a. Openchat: Advanc-ing open-source language models with mixed-quality data. In The Twelfth International Conference on Learning Representations .Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. 2024b. Interpretable preferences via multi-objective reward modeling and mixture-of-experts. arXiv preprint arXiv:2406.12845 .Qineng Wang, Zihao Wang, Ying Su, Hanghang Tong, and Yangqiu Song. 2024c. Rethinking the bounds of llm reasoning: Are multi-agent discussions the key? arXiv preprint arXiv:2402.18272 .Shuhe Wang, Beiming Cao, Shengyu Zhang, Xiaoya Li, Jiwei Li, Fei Wu, Guoyin Wang, and Eduard Hovy. 2023a. Sim-gpt: Text similarity via gpt annotated data. arXiv preprint arXiv:2312.05603 .Shuhe Wang, Xiaofei Sun, Xiaoya Li, Rongbin Ouyang, Fei Wu, Tianwei Zhang, Jiwei Li, and Guoyin Wang. 2023b. Gpt-ner: Named entity recognition via large language models. arXiv preprint arXiv:2304.10428 .Shuhe Wang, Guoyin Wang, Yizhong Wang, Jiwei Li, Eduard Hovy, and Chen Guo. 2024d. Packing anal-ysis: Packing is more appropriate for large models or datasets in supervised fine-tuning. arXiv preprint arXiv:2410.08081 .Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-isa Liu, Noah A Smith, Daniel Khashabi, and Han-naneh Hajishirzi. 2022. Self-instruct: Aligning lan-guage models with self-generated instructions. arXiv preprint arXiv:2212.10560 .Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy J Zhang, Makesh Narsimhan Sreedhar, and Oleksii Kuchaiev. 2024e. Helpsteer2: Open-source dataset for train-ing top-performing reward models. arXiv preprint arXiv:2406.08673 .Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-thought prompting elic-its reasoning in large language models. Junkang Wu, Yuexiang Xie, Zhengyi Yang, Jiancan Wu, Jinyang Gao, Bolin Ding, Xiang Wang, and Xiangnan He. 2024a. β-dpo: Direct preference optimization with dynamic β. arXiv preprint arXiv:2407.08639 .Yue Wu, Zhiqing Sun, Huizhuo Yuan, Kaixuan Ji, Yim-ing Yang, and Quanquan Gu. 2024b. Self-play pref-erence optimization for language model alignment, 2024b. https://arxiv.org/abs/2405.00675. Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu, Qian Luo, Victor Zhong, Yanchao Yang, and Tao Yu. 2023. Text2reward: Automated dense reward function generation for reinforcement learning. arXiv preprint arXiv:2309.11489 .Wei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang, Han Zhong, Heng Ji, Nan Jiang, and Tong Zhang. 2024. Iterative preference learning from human feed-back: Bridging theory and practice for rlhf under kl-constraint. In Forty-first International Conference on Machine Learning .Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large lan-guage models to follow complex instructions. arXiv preprint arXiv:2304.12244 .Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu Mei, Guangju Wang, Chao Yu, and Yi Wu. 2024a. Is dpo superior to ppo for llm alignment? a comprehensive study. arXiv preprint arXiv:2404.10719 .Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yun-tian Deng, Radha Poovendran, Yejin Choi, and Bill Yuchen Lin. 2024b. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing. arXiv preprint arXiv:2406.08464 .An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. 2024a. Qwen2 technical report. arXiv preprint arXiv:2407.10671 .Rui Yang, Ruomeng Ding, Yong Lin, Huan Zhang, and Tong Zhang. 2024b. Regularizing hidden states en-ables learning generalizable reward model for llms. arXiv preprint arXiv:2406.10216 .Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2024. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems , 36. Ziyi Ye, Xiangsheng Li, Qiuchi Li, Qingyao Ai, Yu-jia Zhou, Wei Shen, Dong Yan, and Yiqun Liu. 2024. Beyond scalar reward model: Learning gen-erative judge from preference data. arXiv preprint arXiv:2410.03742 .Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. 2024. Self-rewarding language models. arXiv preprint arXiv:2401.10020 .Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. 2023. Rrhf: Rank responses to align language models with human feedback without tears. arXiv preprint arXiv:2304.05302 .Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. 2024. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Pro-ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9556–9567. Yongcheng Zeng, Guoqing Liu, Weiyu Ma, Ning Yang, Haifeng Zhang, and Jun Wang. 2024. Token-level direct preference optimization. arXiv preprint arXiv:2404.11999 .Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. 2024a. Generative verifiers: Reward modeling as next-token prediction. arXiv preprint arXiv:2408.15240 .Ruiqi Zhang, Licong Lin, Yu Bai, and Song Mei. 2024b. Negative preference optimization: From catastrophic collapse to effective unlearning. arXiv preprint arXiv:2404.05868 .Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tian-wei Zhang, Fei Wu, et al. 2023a. Instruction tuning for large language models: A survey. arXiv preprint arXiv:2308.10792 .Yifan Zhang. 2023. Meta prompting for agi systems. arXiv preprint arXiv:2311.11482 .Yifan Zhang, Ge Zhang, Yue Wu, Kangping Xu, and Quanquan Gu. 2024c. General preference modeling with preference representations for aligning language models. arXiv preprint arXiv:2410.02197 .Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. 2023b. Multi-modal chain-of-thought reasoning in language mod-els. arXiv preprint arXiv:2302.00923 .Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J. Liu. 2023. Slic-hf: Sequence likelihood calibration with human feed-back. arXiv preprint arXiv:2305.10425 .Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. 2024. Lima: Less is more for align-ment. Advances in Neural Information Processing Systems , 36. Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu, Karthik Ganesan, Wei-Lin Chiang, Jian Zhang, and Jiantao Jiao. 2024. Starling-7b: Improving helpful-ness and harmlessness with rlaif. In First Conference on Language Modeling .


